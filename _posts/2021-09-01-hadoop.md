---
layout: post
title: "Introduction to Hadoop"
description: "Notes from Hadoop Tutorial."
toc: true
comments: false
# image: 
hide: true
search_exclude: false
show_tags: true
# sticky_rank: 1
categories: [big-data]
permalink: /hadoop
---

# Introduction to Hadoop

## Big Data Overview
* **Big Data** is a collection of large datasets that cannot be processed using traditional computing techniques.
    * Black Box Data: information about flight crew conversation and aircraft performance
    * Social Media Data: information about profiles and interactions of users across the globe
    * Stock Exchange Data: information about 'buy' and 'sell' decisions made by investors
    * Power Grid Data: information consumed by a particular node with respect to a base station
    * Transport Data: information about vehicle model, capacity, distance, and availability
    * Search Engine Data: information to retrieve from multiple databases
* Big data includes huge volume, high velocity, and extensible variety of data in three forms:
    1. Structured Data, e.g. Relational data
    1. Semi-Structured Data, e.g. XML data
    1. Unstructured Data, e.g. Word, PDF, Text, Media Logs
* Benefits of Big Data include:
    * Using social media network information to inform marketing campaigns and strategies.
    * Using social media preferences and product perception to plan production strategies.
    * Using medical history data to provide better healthcare services.
* **Operational Big Data** refers to systems that provide operational capabilities for real-time, interactive workloads where data is primarily captured and stored.
    * NoSQL Big Data systems, like MongoDB, can run massive computations inexpensively and efficiently, as well as provide insights into patterns and trends based on real-time data.
* **Analytical Big Data** refers to systems that provide analytical capabilities for retrospective and complex analysis that may touch most or all of the data.
    * Massively Parallel Processing (MPP) database systems and MapReduce provide scalable analytical capabilities that is complementary to SQL-based systems.

| Big Data Types | Operational    | Analytical              |
|----------------|----------------|-------------------------|
| Latency        | 1ms - 100ms    | 1min - 100min           |
| Concurrency    | 1000 - 100,000 | 1 - 10                  |
| Access Pattern | Writes, Reads  | Reads                   |
| Queries        | Selective      | Unselective             |
| Data Scope     | Operational    | Retrospective           |
| End User       | Customer       | Data Scientist          |
| Technology     | NoSQL          | MapReduce, MPP Database |

* Challenges of big data include: capturing data, curation, storage, searching, sharing, transfer, analysis, and presentation.

## Big Data Solutions
* Traditional Approach
    * Use computer to store and process big data and database vendors for data storage.
    * Users interact with the application in a centralized system, which uses a database to store and analyze data.
    * Data size is not scalable through a single processor or single database bottleneck.

* Google's Approach: MapReduce
    * [MapReduce](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf) algorithm divides the task into small parts and assigns the subtasks to many computers.
    * The processed results are then collected and aggregated to form the result dataset.

* Apache's Approach: Hadoop
    * Doug Cutting develops Hadoop, an open source project based on Google's MapReduce algorithm.
    * Data is processed in parallel to perform statistical analyses on massive amounts of data.

## Hadoop
* **Hadoop** is an Apache open source framework written in Java that allows distributed processing of large datasets across clusters of computers using simple programming models.
* Hadoop Architecture
    * Processing/Computation layer: MapReduce
    * Storage layer: Hadoop Distributed File System (HDFS)
    * Other modules:
        * Hadoop Common: Java libraries and utilities required by other Hadoop modules
        * Hadoop YARN: Framework for job scheduling and cluster resource management, improved MapReduce
* **MapReduce** is a parallel programming model for writing distributed applications for efficient processing of large amounts of data (1TB+) on large clusters (1K+ nodes) in a reliable, fault-tolerant manner.
* **Hadoop Distributed File System (HDFS)** is based on Google File System (GFS) and provides a distributed file system that is highly fault-tolerant, provides high throughput access to application data, and suitable applications using large datasets.
* How does Hadoop work?
    * Divides data into directories and files (unifrom sized blocks of 128M or 64M).
    * Distributes files across various cluster nodes for processing.
        * HDFS supervises the processing.
        * Blocks are replicated for handling hardware failure.
    * Checks code was executed successfully.
    * Sorts the data between map and reduce stages
    * Sends sorted data to specified computer.
    * Writes debugging logs for each job.
* Advantages of Hadoop
    * Can quickly write and test distributed systems

## References
[^1]: Footnote