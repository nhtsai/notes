---
layout: post
title: "Semi-Supervised Classification with Graph Convolutional Networks"
description: "Notes on GCNs by Kipf & Welling."
author: "Nathan Tsai"
toc: true
comments: false
# image: 
hide: false
search_exclude: false
show_tags: true
# sticky_rank: 1
categories: [research-paper, machine-learning, graph-learning]
permalink: /gcn
---

# Semi-Supervised Classification with Graph Convolutional Networks

## Abstract
TODO

## Introduction

The authors first consider a problem: *How do we classify nodes in a graph, where labels are only available for a small subset of nodes?* Think of a citation network where some documents have labels, but most do not. The authors frame it as "graph-based semi-supervised learning, where label information is smoothed over the graph via some form explicit graph-based regularization term in the loss function."
* graph-based: working with a network
* semi-supervised: only some nodes have labels
* smoothed over the graph: labels are approximated across the graph
* explicit graph-based regularization: 

For example, using graph Laplacian regularization:  

$\mathcal{L} = \mathcal{L}_0 + \lambda \mathcal{L}_{\text{reg}}$

where $\mathcal{L}_{\text{reg}} = \sum_{i,j} A_{i,j} \lVert f(X_{i}) - f(X_{j}) \rVert ^{2} = f(X)^{T} \Delta f(X)$

* $\mathcal{L}_0$ : supervised loss with respect to labeled part of graph
* $\mathcal{L}_{\text{reg}}$ : graph Laplacian regularization term
* $f(\dots)$ : a neural network-like differentiable function
* $\lambda$ : regularization weight
* $X$ : matrix of node feature vectors $X_{i}$
* $\Delta = D - A$ : un-normalized [graph Laplacian](https://en.wikipedia.org/wiki/Laplacian_matrix) of an undirected graph $G=(V, E)$ , with $N$ nodes $v_{i} \in V$ , edges $(v_{i}, v_{j}) \in E$
    * $A \in \Reals^{N \times N}$ : adjacency matrix (binary or weighted)
        * 1 if connected, 0 otherwise.
        * Symmetric since graph is undirected, i.e. both sides of diagonal are same
    * $D_{ii} = \sum_{j} A_{ij}$ : degree matrix
        * The diagonal is the number of edges for that nodes
    * Basically, positive diagonal of each node's number of edges and -1's for each neighbor
* Assumes that connected nodes in the graph share the same label
    * This may restrict modeling capacity because edges could contain more information than node similarity

The authors want to encode the graph structure in a neural network model $f(X, A)$ .
For all nodes with labels, train on a supervised target $\mathcal{L}_{0}$ and avoid graph regularization $\mathcal{L}_{\text{reg}}$ in the loss function.

By conditioning $f(\dots)$ on adjacency matrix $A$ , the model can distribute gradient information from supervised loss $\mathcal{L}_0$ to learn representations of nodes both with and without labels. Because the model knows each node's neighbors, the labeled nodes can help inform the unlabeled nodes.

## Fast Approximate Convolutions on Graphs

The overall goal is to learn a function of graph features that uses:
* node features, like a $N \times D$ feature matrix $X$ , with $N$ nodes and $D$ input features
* graph features, like an adjacency matrix $A$
and produces a node-level $N \times F$ output matrix $Z$ , with $N$ nodes and $F$ output features

Every neural network layer $H$ can then be written as a non-linear function:

$H^{(l+1)} = f(H^{(l)}, A)$

* There are $L$ layers in the neural network
* $H^{(0)} = X$ : input feature matrix is the first layer
* $H^{(L)} = Z$ : output feature matrix is the last layer

In this case, the layer-wise propagation rule is:

$H^{(l+1)} = \sigma (\tilde{D}^{-\frac{1}{2}} \tilde{A}\tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})$

* $\tilde{A} = A + I_{N}$ : adjacency matrix of undirected graph G plus self connections
* $I_{N}$ : identity matrix
* $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$ : 
* $W^{(l)}$ : trainable weight matrix of layer $l$
* $\sigma(\dots)$ : activation function, such as $\text{ReLU}(\dots) = \text{max}(0, \dots)$
* $H^{(l)} \in \Reals^{N \times D}$ : activation matrix of layer $l$
* This propagation rule comes from first-order approximation of local spectral filters on graphs

### Spectral Graph Convolutions

A spectral graph convolution is the signal $x \in \Reals^{N}$ (scalar for every node) multiplied by a filter $g_{\theta} = \text{diag}(\theta)$ parameterized by $\theta \in \Reals^{N}$ in the Fourier domain.

$g_{\theta} \star x = U g_{\theta} U^{T} x$

* $U$ : matrix of eigenvectors of normalized graph Laplacian $L_{\text{norm}}$
* $L_{\text{norm}} = I_{N} - D^{-\frac{1}{2}} A D^{-\frac{1}{2}} = U \Lambda U^{T}$ : [symmetric normalized graph Laplacian](https://en.wikipedia.org/wiki/Laplacian_matrix#Symmetric_normalized_Laplacian)
* $\Lambda$ : diagonal matrix of $L_{\text{norm}}$ eigenvalues
* $U^T x$ : graph Fourier transform of x
* $g_{\theta}$ : function of eigenvalues of $L_{\text{norm}}$ , i.e. $g_{\theta}(\Lambda)$

Calculating graph spectral convolutions is expensive
* multiplication with eigenvector matrix $U$ is $O(N^{2})$
* eigen-decomposition of $L_{\text{norm}}$ is expensive for large graphs

We can approximate $g_{\theta}(\Lambda)$ using a truncated expansion in terms of Chebyshev polynomials $T_{k}(x)$ up to $K^{\text{th}}$ order:

$g_{\theta'}(\Lambda) \approx \sum_{k=0}^{K} \theta'_{k} T_{k}(\tilde{\Lambda})$

* $\tilde{\Lambda} = \frac{2}{\lambda_{\text{max}}} \Lambda - I_{N}$ : rescaled diagonal matrix of eigenvalues
* $\lambda_{\text{max}}$ : largest eigenvalue of $L_{\text{norm}}$
* $\theta' \in \Reals^{K}$ : vector of Chebyshev coefficients
* Chebyshev polynomials are recursively defined as $T_{k}(x) = 2x T_{k-1}(x) - T_{k-2}(x)$ , with $T_{0}(x) = 1$ and $T_{1}(x) = x$

Now, we have an approximated spectral graph convolution of a signal $x$ with a filter $g_{\theta'}$ :

$g_{\theta'} \star x \approx \sum_{k=0}^{K} \theta'_{k} T_{k}(\tilde{L}_{\text{norm}})x$

* $\tilde{L}_{\text{norm}} = \frac{2}{\lambda_{\text{max}}} L_{\text{norm}} - I_{N}$ : rescaled normalized graph Laplacian, verified by $(U \Lambda U^{T})^{k} = U \Lambda^{k} U^{T}$
* This expression is $K$-localized as a $K^{\text{th}}$-order polynomial in the Laplacian
    * It depends only on the $K^{\text{th}}$-order neighborhood, or nodes that are at maximum $K$ steps away from the central node
* Complexity of calculating this is $\mathcal{O}(\lvert \mathcal{E} \rvert)$ , linear in the number of edges

### Layer-Wise Linear Model

We can stack multiple approximated spectral graph convolutions to build a neural network, with each layer followed by a point-wise non-linearity.
What if we limited the neighborhood to nodes at most 1 step away $(K=1)$ ?
This makes the approximated convolution a function that is linear with respect to $L_{\text{norm}}$ and therefore a linear function on the graph Laplacian spectrum

Layer-Wise linear formulation: We can stack multiple of these layers without being limited to explicit parameterization of Chebyshev polynomials.
Such model can alleviate problem of over-fitting on local neighborhood structures for graphs with very wide node degree distributions, i.e. some nodes are highly connected and some nodes are barely connected.
This is common in social networks, citation networks, knowledge graphs, etc.
This layer-wise linear formulation can build deeper models using a fixed computational budget, which is known to improve modeling capacity on a number of domains.

We further approximate $\lambda_{\text{max}} \approx 2$ because the neural network parameters will adapt to during training:

$g_{\theta'} \star x \approx \theta'_{0} + \theta'_{1} (L_{\text{norm}} - I_{N}) x = \theta'_{0} x - \theta'_{1} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x$

* $\theta'_{0}$ and $\theta'_{1}$ : filter parameters

Applying the filters successively effectively convolves the $k^{\text{th}}$-order neighborhood of a node, where $k$ is the number of successive filtering operations or convolutional layers in the neural network model.

Each layer takes into consideration 1-order neighborhood, each each successive layer considers more and more neighbors.

To address over-fitting and minimize operations per layer, we constrain the number of parameters:

$g_{\theta} \star x \approx \theta (I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) x$

* $\theta = \theta'_{0} = -\theta'_{1}$ : single parameter
* $I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ has eigenvalues in range [0, 2]

Repeating this operation can lead to numerical instabilities and exploding/vanishing gradients in deep neural network models. We ca address this using a renormalization trick.

$I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$

* $\tilde{A} = A + I_{N}$ : adjusted adjacency matrix
* $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$ : degree matrix

We can generalize this definition to a signal $X \in \Reals^{N \times C}$ with $C$ input channel (features for every node) and $F$ filters for feature maps:

$Z = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta$

* $\Theta \in \Reals^{C \times F}$ : matrix of filter parameters
* $Z \in \Reals^{N \times F}$ : convolved signal matrix
* Filtering operation has complexity $\mathcal{O}(\lvert \mathcal{E} \rvert F C)$
* $\tilde{A}X$ can be efficiently implemented as product of sparse matrix and dense matrix

## Semi-Supervised Node Classification

Now we have a flexible model $f(X,A)$ for efficient information propagation on graphs.
We can relax certain assumptions typically made in graph-based semi-supervised learning by condition our model $f(X,A)$ both on the data $X$ and adjacency matrix $A$ of underlying graph structure.

When the adjacency matrix $A$ contains information not present in the data $X$ , using both can be especially powerful.
* E.g. Citation links between documents in citation network
* E.g. Relations in a knowledge graph

### Example: Cora Dataset

![GCN Model Diagram]({{ site.baseurl }}/images/gcn/gcn-model.png "GCN Model Diagram")

Consider a two-layer GCN for semi-supervised node classification on a graph with symmetric adjacency matrix $A$ (binary or weighted).

Preprocessing step: 

$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$

Forward propagation step:

$Z = f(X,A) = \text{softmax}(\hat{A} \text{ ReLU}(\hat{A} X W^{(0)}) W^{(1)})$

* $\hat{A}$ : adjacency matrix with symmetric normalization trick
* $W^{(0)} \in \Reals^{C \times H}$ : input-to-hidden weight matrix for hidden layer with $H$ feature maps
* $W^{(1)} \in \Reals^{H \times F}$ : hidden-to-output weight matrix
* $\text{softmax}(x_{i}) = \frac{1}{z} \exp{x_{i}}$ : softmax activation function
    * $z = \sum_{i} \exp{x_{i}}$ : applied row-wise

Evaluate cross-entropy error over all labeled examples:

$\mathcal{L} = - \sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{lf} \ln{Z_{lf}}$

* $\mathcal{Y}_{L}$ : set of node indices that have labels
* $F$ : number of output labels
* $Y_{lf}$ : correct value for node index $l$ and output label $f$
    * Should be 0 for incorrect output labels, 1 for correct output label
* $Z_{lf}$ : predicted value for node index $l$ and output label $f$
    * Should be higher for the correct output label, lower for incorrect output labels

Neural network weights $W^{(0)}$ and $W^{(1)}$ are trained using batch gradient descent using full dataset for every training iteration.

Using sparse representation for $A$ , memory requirement is $\mathcal{O}(\lvert \mathcal{E} \rvert)$ , linear in the number of edges.
Use dropout to introduce stochasticity in the training process.
Future work: memory-efficient extensions with mini-batch stochastic gradient descent

### Implementation
Use efficient sparse-dense matrix multiplication to make the computation complexity of the forward propagation step $\mathcal{O}(\lvert \mathcal{E} \rvert C H F)$ , linear in the number of graph edges.

## Related Work

### Graph-Based Semi-Supervised Learning
### Neural Networks on Graphs

## Experiments

### Datasets

**Citation Networks**

**NELL**

**Random graphs**

### Experimental Set-Up

### Baselines

## Results

### Semi-Supervised Node Classification

### Evaluation of Propagation Model

### Training Time per Epoch

## Discussion

### Semi-Supervised Model

### Limitations and Future Work

**Memory requirement**

**Directed edges and edge features**

**Limiting Assumptions**

## Conclusion


## References
* http://tkipf.github.io/graph-convolutional-networks/