---
layout: post
title: "Semi-Supervised Classification with Graph Convolutional Networks"
description: "Notes on GCNs by Kipf & Welling."
author: "Nathan Tsai"
toc: true
comments: false
# image: 
hide: false
search_exclude: false
show_tags: true
# sticky_rank: 1
categories: [research-paper, machine-learning, graph-learning]
permalink: /gcn
---

# Semi-Supervised Classification with Graph Convolutional Networks

## Abstract
TODO

## Introduction

How do we classify nodes in a graph, where labels are only available for a small subset of nodes? Think of a citation network where some documents have labels, but most do not. This problem can be seen as "graph-based semi-supervised learning, where label information is smoothed over the graph via some form explicit graph-based regularization term in the loss function."

For example, loss function using graph Laplacian regularization: $\mathcal{L} = \mathcal{L}_0 + \lambda \mathcal{L}_{\text{reg}}$
* $\mathcal{L}_0$ : supervised loss with respect to labeled part of graph
* $\lambda$ : regularization weight
* $\mathcal{L}_{\text{reg}}$ : graph-based regularization term

The graph-based regularization term is defined as: $\mathcal{L}_{\text{reg}} = \sum_{i,j} A_{i,j} \lVert f(X_{i}) - f(X_{j}) \rVert ^{2} = f(X)^{T} \Delta f(X)$
* $\lVert \dots \rVert ^{2}$ : squared Euclidean norm, similar to sum squared error
* $f(\dots)$ : a neural network-like differentiable function, output is $N \times 1$
* $X_{i}$ : feature vector of node $i$
* $\Delta = D - A$ : un-normalized [graph Laplacian](https://en.wikipedia.org/wiki/Laplacian_matrix) of an undirected graph $G=(V, E)$ , with $N$ nodes $v_{i} \in V$ , edges $(v_{i}, v_{j}) \in E$
    * $A \in \Reals^{N \times N}$ : adjacency matrix, binary (0/1) or weighted
    * $D_{ii} = \sum_{j} A_{ij}$ : degree matrix
    * Basically, $\Delta$ has positive diagonal of each node's number of edges and -1's for each neighbor

This regularization term can be thought of as the sum of squared errors of all pairwise node embeddings, weighted on graph edges.

The loss function assumes that connected nodes in the graph share the same label. This may restrict modeling capacity because edges could contain more information other than node similarity.

The authors encode the graph structure in a neural network model $f(X, A)$ . For all nodes with labels, the model is trained on a supervised target $\mathcal{L}_{0}$ to avoid graph regularization $\mathcal{L}_{\text{reg}}$ in the loss function.

By conditioning $f(\dots)$ on adjacency matrix $A$ , the model can distribute gradient information from supervised loss $\mathcal{L}_0$ to learn representations of nodes both with and without labels. Because the model knows each node's neighbors, the labeled nodes can help inform the unlabeled nodes.

**Main Contributions**
1. Simple, well-behaved layer-wise propagation rule for graph neural networks and how it is motivated from a first-order approximation of spectral graph convolutions
1. Graph-based neural network model for fast and scalable semi-supervised classification of nodes in a graph


## Fast Approximate Convolutions on Graphs

The overall goal is to learn a function of graph features that uses:
* node features, like a $N \times D$ feature matrix $X$ , with $N$ nodes and $D$ input features
* graph features, like an adjacency matrix $A$
and produces a node-level $N \times F$ output matrix $Z$ , with $N$ nodes and $F$ output features

Every graph neural network layer $H$ can be written as a non-linear function: $H^{(l+1)} = f(H^{(l)}, A)$
* There are $L$ layers in the neural network
* $H^{(0)} = X$ : input feature matrix is the first layer
* $H^{(L)} = Z$ : output feature matrix is the last layer
* $A$ : adjacency matrix of the graph

In this case, the layer-wise propagation rule is: $H^{(l+1)} = \sigma (\tilde{D}^{-\frac{1}{2}} \tilde{A}\tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})$
* $\tilde{A} = A + I_{N}$ : adjacency matrix of undirected graph G plus self connections
* $I_{N}$ : identity matrix
* $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$ : degree matrix of $\tilde{A}$
* $W^{(l)}$ : trainable weight matrix of layer $l$
* $\sigma(\dots)$ : non-linear activation function, such as $\text{ReLU}(\dots) = \text{max}(0, \dots)$
* $H^{(l)} \in \Reals^{N \times D}$ : activation matrix of layer $l$
* This propagation rule comes from first-order approximation of local spectral filters on graphs

### Spectral Graph Convolutions

A [spectral graph convolution](https://en.wikipedia.org/wiki/Spectral_clustering) is defined as the signal $x \in \Reals^{N}$ (scalar for every node) multiplied by a filter $g_{\theta} = \text{diag}(\theta)$ parameterized by $\theta \in \Reals^{N}$ in the Fourier domain, specifically $g_{\theta} \star x = U g_{\theta} U^{T} x$
* $L_{\text{norm}} = I_{N} - D^{-\frac{1}{2}} A D^{-\frac{1}{2}} = U \Lambda U^{T}$ : [symmetric normalized graph Laplacian](https://en.wikipedia.org/wiki/Laplacian_matrix#Symmetric_normalized_Laplacian)
* $U$ : matrix of $L_{\text{norm}}$ eigenvectors
* $\Lambda$ : diagonal matrix of $L_{\text{norm}}$ eigenvalues
* $U^T x$ : graph Fourier transform of x
* $g_{\theta}$ : function of eigenvalues of $L_{\text{norm}}$ , i.e. $g_{\theta}(\Lambda)$

However, calculating spectral graph convolutions is *expensive* because the multiplication with eigenvector matrix $U$ is $O(N^{2})$ and the eigen-decomposition of $L_{\text{norm}}$ is expensive for large graphs.

From [Hammond et al. (2011)](https://hal.inria.fr/inria-00541855/document), $g_{\theta}(\Lambda)$ can be well-approximated by a truncated expansion in terms of Chebyshev polynomials $T_{k}(x)$ up to $K^{\text{th}}$ order.
Therefore, we approximate the function of eigenvalues as: $g_{\theta'}(\Lambda) \approx \sum_{k=0}^{K} \theta'_{k} T_{k}(\tilde{\Lambda})$
* $\tilde{\Lambda} = \frac{2}{\lambda_{\text{max}}} \Lambda - I_{N}$ : rescaled diagonal matrix of $L_{\text{norm}}$ eigenvalues
* $\lambda_{\text{max}}$ : largest eigenvalue of $L_{\text{norm}}$
* $\theta' \in \Reals^{K}$ : vector of Chebyshev coefficients
* Chebyshev polynomials are recursively defined as $T_{k}(x) = 2x T_{k-1}(x) - T_{k-2}(x)$ , with $T_{0}(x) = 1$ and $T_{1}(x) = x$

Using the approximated function, the spectral graph convolution is: $g_{\theta'} \star x \approx \sum_{k=0}^{K} \theta'_{k} T_{k}(\tilde{L}_{\text{norm}})x$
* $\tilde{L}_{\text{norm}} = \frac{2}{\lambda_{\text{max}}} L_{\text{norm}} - I_{N}$ : rescaled normalized graph Laplacian, verified by noticing that $(U \Lambda U^{T})^{k} = U \Lambda^{k} U^{T}$.
* This convolution expression is $K$-localized as a $K^{\text{th}}$-order polynomial in the Laplacian, i.e. it depends only on the $K^{\text{th}}$-order neighborhood, or nodes that are at maximum $K$ steps away from the central node.
* The time complexity is $\mathcal{O}(\lvert \mathcal{E} \rvert)$ , i.e. linear in the number of edges.

### Layer-Wise Linear Model

We can stack multiple approximated spectral graph convolutions to build a neural network, with each layer followed by a point-wise non-linearity.

What if the layer-wise convolution operation was limited the neighborhood to nodes at most 1 step away $(K=1)$ ? By limiting the layers to $K=1$, stacking multiple layers can recover a rich class of convolutional filter functions without being limited to explicit parameterization of Chebyshev polynomials, since $T_{1}(x) = 2xT_{0}(x) = 2x$. This makes the approximated convolution $g_{\theta'} \star x \approx \theta'_{1} T_{1}(\tilde{L}_{\text{norm}})x \approx \theta'_{1}(2\tilde{L}_{\text{norm}})x$ linear with respect to $L_{\text{norm}}$ and therefore a linear function on the graph Laplacian spectrum.

Intuitively, the model can alleviate the problem of over-fitting on local neighborhood structures for graphs with very wide node degree distributions, i.e. some nodes are highly connected and some nodes are barely connected, commonly found in social networks, citation networks, knowledge graphs, and other real-world graph datasets. This is because the TODO
Moreover, this layer-wise linear formulation can build deeper models using a fixed computational budget, which is known to improve modeling capacity on a number of domains. We can further approximate $\lambda_{\text{max}} \approx 2$ because the neural network parameters are able adapt to the change in scale during training.

Derivation of approximated spectral graph convolution:

$$
\begin{align}
g_{\theta'} \star x &\approx \sum_{k=0}^{K} \theta'_{k} T_{k}(\tilde{L}_{\text{norm}})x\\
g_{\theta'} \star x &\approx \theta'_{0}T_{0}(\frac{2}{2} L_{\text{norm}} - I_{N})x + \theta'_{1}T_{1}(\frac{2}{2} L_{\text{norm}} - I_{N})x \text{, setting } K = 1, \lambda_{\text{max}} = 2\\
g_{\theta'} \star x &\approx \theta'_{0}x + \theta'_{1}(L_{\text{norm}} - I_{N})x \text{, where } T_{0}(x) = 1, T_{1}(x) = x\\
g_{\theta'} \star x &\approx \theta'_{0}x + \theta'_{1}((I_{N} - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) - I_{N})x\\
g_{\theta'} \star x &\approx \theta'_{0}x - \theta'_{1}(D^{-\frac{1}{2}} A D^{-\frac{1}{2}})x
\end{align}
$$

The approximated spectral graph convolution is now: $g_{\theta'} \star x \approx \theta'_{0}x - \theta'_{1} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x \text{, where } \theta'_{0}, \theta'_{1} \text{ are filter parameters}$.

Applying the filters successively over the whole graph effectively convolves the $k^{\text{th}}$-order neighborhood of a node, where $k$ is the number of successive filtering operations or convolutional layers in the neural network model. In this case, each layer takes into consideration the the first-order neighborhood of the current node, and each successive layer considers more and more neighbors.

To address over-fitting and minimize operations per layer, we define a single parameter $\theta = \theta'_{0} = -\theta'_{1}$ to constrain the number of parameters.

Using the single parameter, the convolution is now: $g_{\theta} \star x \approx \theta (I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) x$

$I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ has eigenvalues in range [0, 2]. Repeating this operation can lead to numerical instabilities and exploding/vanishing gradients in deep neural network models.

This problem is addressed by using a **renormalization trick**: $I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$
* $\tilde{A} = A + I_{N}$ : adjusted adjacency matrix
* $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$ : adjusted degree matrix

We can generalize the renormalized convolution $g_{\theta} \star x \approx \theta (\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}) x$ as: $Z = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta$
* $X \in \Reals^{N \times C}$ : signal matrix with $C$ input channels ($C \times 1$ feature vector for every node)
* $\Theta \in \Reals^{C \times F}$ : matrix of filter parameters with $F$ filters or feature maps
* $Z \in \Reals^{N \times F}$ : convolved signal matrix
* Filtering operation has complexity $\mathcal{O}(\lvert \mathcal{E} \rvert F C)$
* $\tilde{A}X$ can be efficiently implemented as product of sparse matrix and dense matrix

## Semi-Supervised Node Classification

Now we have a flexible model $f(X,A)$ for efficient information propagation on graphs.
We can relax certain assumptions typically made in graph-based semi-supervised learning by condition our model $f(X,A)$ both on the data $X$ and adjacency matrix $A$ of underlying graph structure.

When the adjacency matrix $A$ contains information not present in the data $X$ , using both can be especially powerful.
* E.g. Citation links between documents in citation network
* E.g. Relations in a knowledge graph

### Example: Cora Dataset

![GCN Model Diagram]({{ site.baseurl }}/images/gcn/gcn-model.png "GCN Model Diagram")

Consider a two-layer GCN for semi-supervised node classification on a graph with symmetric adjacency matrix $A$ (binary or weighted).

Preprocessing step: 

$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$

Forward propagation step:

$Z = f(X,A) = \text{softmax}(\hat{A} \text{ ReLU}(\hat{A} X W^{(0)}) W^{(1)})$

* $\hat{A}$ : adjacency matrix with symmetric normalization trick
* $W^{(0)} \in \Reals^{C \times H}$ : input-to-hidden weight matrix for hidden layer with $H$ feature maps
* $W^{(1)} \in \Reals^{H \times F}$ : hidden-to-output weight matrix
* $\text{softmax}(x_{i}) = \frac{1}{z} \exp{x_{i}}$ : softmax activation function
    * $z = \sum_{i} \exp{x_{i}}$ : applied row-wise

Evaluate cross-entropy error over all labeled examples:

$\mathcal{L} = - \sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{lf} \ln{Z_{lf}}$

* $\mathcal{Y}_{L}$ : set of node indices that have labels
* $F$ : number of output labels
* $Y_{lf}$ : correct value for node index $l$ and output label $f$
    * Should be 0 for incorrect output labels, 1 for correct output label
* $Z_{lf}$ : predicted value for node index $l$ and output label $f$
    * Should be higher for the correct output label, lower for incorrect output labels

Neural network weights $W^{(0)}$ and $W^{(1)}$ are trained using batch gradient descent using full dataset for every training iteration.

Using sparse representation for $A$ , memory requirement is $\mathcal{O}(\lvert \mathcal{E} \rvert)$ , linear in the number of edges.
Use dropout to introduce stochasticity in the training process.
Future work: memory-efficient extensions with mini-batch stochastic gradient descent

### Implementation
Use efficient sparse-dense matrix multiplication to make the computation complexity of the forward propagation step $\mathcal{O}(\lvert \mathcal{E} \rvert C H F)$ , linear in the number of graph edges.

## Related Work

### Graph-Based Semi-Supervised Learning
### Neural Networks on Graphs

## Experiments

### Datasets

**Citation Networks**

**NELL**

**Random graphs**

### Experimental Set-Up

### Baselines

## Results

### Semi-Supervised Node Classification

### Evaluation of Propagation Model

### Training Time per Epoch

## Discussion

### Semi-Supervised Model

### Limitations and Future Work

**Memory requirement**

**Directed edges and edge features**

**Limiting Assumptions**

## Conclusion


## References
* http://tkipf.github.io/graph-convolutional-networks/