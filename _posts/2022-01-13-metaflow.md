---
layout: post
title: "Metaflow"
description: "Notes on Netflix's Metaflow."
toc: true
comments: false
# image: 
hide: false
search_exclude: false
show_tags: true
# sticky_rank: 1
categories: [data-science, devops]
permalink: /metaflow
---

# Metaflow
Metaflow is a library developed at Netflix to help data scientists excecute data science projects, from prototype to production.
Metaflow helps data science projects achieve:
1. **Scalability**: execution across multiple nodes on larger systems
1. **Criticality**: results produced correctly in a timely manner
1. **Complexity**: multiple project components and contributors

The typical data science project starts by accessing data from a **data warehouse** (folder, database, data lake). Modelling code is then executed in a **compute environment** (laptop, large-scale container management system) with a **job scheduler** to orchestrate multiple units of work.

But how should code be **architected** (object hierarchy, python modules, packages)? How should be code, input data, or models be **versioned**? Once deployed to production, how are **model operations** maintained? How to run code reliably in production, monitor its performance, or deploy new versions of code to run in parallel with previous versions? How to **develop features and models** using data science libraries?

Metaflow takes care of the dev-ops and infrastructure stack considerations, so data scientists can focus on writing models and business logic without much new to learn.

## Structure
Metaflow uses the [dataflow paradigm](https://en.wikipedia.org/wiki/Dataflow_programming) to model programs a directed graphs of operations, called **flows**, which are well suited for data processing pipelines in data science and machine learning workflows. Every flow contains **steps**, which use operations as nodes and transitions to the next step as edges. Each flow must contain a `start` and `end` step.

## Transitions
The **linear** transition moves from one step to another step. A **data artifact** is an instance variable that is available to all steps after creation. In the example linear flow, the `my_var` data artifact is created in the `start` step and used in subsequent steps.

```python
from metaflow import FlowSpec, step

class LinearFlow(FlowSpec):
    """Demonstrates the linear flow transition."""

    @step
    def start(self):
        """Create data artifact my_var at start."""
        self.my_var = 'hello world'
        self.next(self.a)

    @step
    def a(self):
        """Data artifact my_var is propagated to step a."""
        print('the data artifact is: %s' % self.my_var)
        self.next(self.end)

    @step
    def end(self):
        """Data artifact my_var is propagated to end."""
        print('the data artifact is still: %s' % self.my_var)

if __name__ == '__main__':
    LinearFlow()
```

The **branch** transition expresses any number of parallel steps *statically*, which can be executed over multiple CPU cores or cloud instances. Every branch must be joined using a join step with additional `inputs` parameter.

In the example branch flow, the `start` step points to steps `a` and `b`, creating separate branches. The branches `a` and `b` are joined together in the `join` step. Reference the specific step in the branch to disambiguate branches in the join step.

```python
from metaflow import FlowSpec, step

class BranchFlow(FlowSpec):
    """Demonstrates the branch flow transition."""

    @step
    def start(self):
        """Start branches into steps a and b."""
        self.next(self.a, self.b)

    @step
    def a(self):
        """Step a defines data artifact x."""
        self.x = 1
        self.next(self.join)

    @step
    def b(self):
        """Step b defines another data artifact x to a different value."""
        self.x = 2
        self.next(self.join)

    @step
    def join(self, inputs):
        """Join disambiguates x using branch names: a.x and b.x"""
        print('a is %s' % inputs.a.x)
        print('b is %s' % inputs.b.x)
        print('total is %d' % sum(input.x for input in inputs))
        self.next(self.end)

    @step
    def end(self):
        """End step."""
        pass

if __name__ == '__main__':
    BranchFlow()
```

The **foreach** transition can branch based on data *dynamically*. Instead of creating many parallel branches with named step methods, foreach executes many parallel copies of steps inside the loop. The `foreach` argument in `self.next()` takes in a name of a list stored in an instance variable. The foreach loop creates separate, parallel tasks to process each item in a step, which are then joined and accessed using `inputs`

In the foreach example, the `start` step uses a foreach loop to create parallel branches for each title in `titles`.

```python
from metaflow import FlowSpec, step

class ForeachFlow(FlowSpec):
    """Demonstrates the foreach flow transition."""

    @step
    def start(self):
        """Start creates a step for each title in titles list."""
        self.titles = ['Stranger Things',
                    'House of Cards',
                    'Narcos']
        self.next(self.a, foreach='titles')

    @step
    def a(self):
        """Step a processes each title."""
        self.title = '%s processed' % self.input
        self.next(self.join)

    @step
    def join(self, inputs):
        """Join aggregates all processed titles for each step into a results list."""
        self.results = [input.title for input in inputs]
        self.next(self.end)

    @step
    def end(self):
        """End prints out each processed title in results list."""
        print('\n'.join(self.results))

if __name__ == '__main__':
    ForeachFlow()
```

Steps are indivisible units of execution that fail or succeed as a whole, and all instance variable are persisted so previous successful steps do not need to be re-executed. Steps are also checkpoints, where data artifacts present when the step finished can be inspected, but not data manipulated within a step. Since checkpointing adds some overhead, steps should not be too granular and take less than an hour to run.

**Parameters** for flows can be defined by assigning a `Parameter` object to a class variable in the flow. Parameters are accessible in all steps and specified like command-line options. Parameters are typed based on their default value or by setting the `type` argument.

```python
from metaflow import FlowSpec, Parameter, step

class ParameterFlow(FlowSpec):
    """Demonstrates the use of parameters in a flow."""

    alpha = Parameter('alpha',
                      help='Learning rate',
                      default=0.01)

    @step
    def start(self):
        """Start prints out the parameter alpha defined as a class variable."""
        print('alpha is %f' % self.alpha)
        self.next(self.end)

    @step
    def end(self):
        """Parameter alpha is propagated to end."""
        print('alpha is still %f' % self.alpha)

if __name__ == '__main__':
    ParameterFlow()
```

```bash
$ python parameter_flow.py run --alpha 0.6
```

In addition to scalar values, `Parameter` uses JSON to represent more complex values.


```python
from metaflow import FlowSpec, Parameter, step, JSONType

class JSONParameterFlow(FlowSpec):
    """Demonstrates the use of complex parameters using JSON."""

    gdp = Parameter('gdp',
                    help='Country-GDP Mapping',
                    type=JSONType,
                    default='{"US": 1939}')

    country = Parameter('country',
                        help='Choose a country',
                        default='US')

    @step
    def start(self):
        """Start uses flow parameters, or their default values."""
        print('The GDP of %s is $%dB' % (self.country, self.gdp[self.country]))
        self.next(self.end)

    @step
    def end(self):
        """End step."""
        pass

if __name__ == '__main__':
    JSONParameterFlow()
```

```bash
$ python parameter_flow.py run --gdp '{"US": 1}'
```

For linear steps, data artifacts created in previous steps can be accessed using instance variables. The value of each artifact at a linear step can be easily determined by taking the value at the end of the previous step.

For join steps, the value of data artifacts can be ambiguous if the same data artifacts are set to different values on incoming branches.


```python
from metaflow import FlowSpec, step

class MergeArtifactsFlow(FlowSpec):
    """Demonstrates using merge_artifacts to specify disambiguous values."""

    @step
    def start(self):
        """Start defines a data artifact pass_down and branches to steps a and b."""
        self.pass_down = 'a'
        self.next(self.a, self.b)

    @step
    def a(self):
        """Step a defines data artifacts: common, x, y, from_a."""
        self.common = 5
        self.x = 1
        self.y = 3
        self.from_a = 6
        self.next(self.join)

    @step
    def b(self):
        """Step b defines data artifacts: common, x, y."""
        self.common = 5
        self.x = 2
        self.y = 4
        self.next(self.join)

    @step
    def join(self, inputs):
        """In the join step, x is manually specified, merge_artifacts disambiguates:
        common, from_a, pass_down (excludes y).
        """
        self.x = inputs.a.x
        self.merge_artifacts(inputs, exclude=['y'])
        print('x is %s' % self.x)
        print('pass_down is %s' % self.pass_down)
        print('common is %d' % self.common)
        print('from_a is %d' % self.from_a)
        self.next(self.c)

    @step
    def c(self):
        """Step c branches to steps d and e."""
        self.next(self.d, self.e)

    @step
    def d(self):
        """Step d defines data artifact conflicting."""
        self.conflicting = 7
        self.next(self.join2)

    @step
    def e(self):
        """Step e defines another data artifact conflicting to a different value."""
        self.conflicting = 8
        self.next(self.join2)

    @step
    def join2(self, inputs):
        """In the join step, merge_artifacts only disambiguates pass_down and common."""
        self.merge_artifacts(inputs, include=['pass_down', 'common'])
        print('Only pass_down and common exist here')
        self.next(self.end)

    @step
    def end(self):
        """End step."""
        pass

if __name__ == '__main__':
    MergeArtifactsFlow()
```
