<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="robots" content="none, noindex, nofollow, noarchive, noimageindex, nosnippet" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to Probability | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Introduction to Probability" />
<meta name="author" content="Nathan Tsai" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Book notes on probability." />
<meta property="og:description" content="Book notes on probability." />
<link rel="canonical" href="https://nhtsai.github.io/notes/probability-book" />
<meta property="og:url" content="https://nhtsai.github.io/notes/probability-book" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-26T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-10-26T00:00:00-05:00","url":"https://nhtsai.github.io/notes/probability-book","@type":"BlogPosting","headline":"Introduction to Probability","dateModified":"2021-10-26T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nhtsai.github.io/notes/probability-book"},"author":{"@type":"Person","name":"Nathan Tsai"},"description":"Book notes on probability.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nhtsai.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/katex.min.css" integrity="sha512-SgbQw5g+dYpW7CUSasc2ontF/7/JIHORpkzukPxbO2JIcyGy7p2WtlkfBGHvekdhJawrdp+p1bEzBH+nU/cbEg==" crossorigin="anonymous" />
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/katex.min.js" integrity="sha512-vhW78Uk043jhhQnaIpK3Io8CbTmcD3fcfZyhl7qz+Md1P78MI7Hlw39lEqdddt8nobv4sclKZU8CeVDzIfLUNA==" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/contrib/auto-render.min.js" integrity="sha512-ZA/RPrAo88DlwRnnoNVqKINnQNcWERzRK03PDaA4GIJiVZvGFIWQbdWCsUebMZfkWohnfngsDjXzU6PokO4jGw==" crossorigin="anonymous"></script>
    <!-- MathJax -->
    <!-- <script defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.min.js" integrity="sha512-93xLZnNMlYI6xaQPf/cSdXoBZ23DThX7VehiGJJXB76HTTalQKPC5CIHuFX8dlQ5yzt6baBQRJ4sDXhzpojRJA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> -->
    <!-- KaTeX Auto-render Function with custom delimiter rules -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to Probability</h1><p class="page-description">Book notes on probability.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-26T00:00:00-05:00" itemprop="datePublished">
        Oct 26, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nathan Tsai</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      31 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#book-notes">book-notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#mathematics">mathematics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#probability">probability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#statistics">statistics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#introduction-to-probability-by-blitzstein--hwang">Introduction to Probability by Blitzstein &amp; Hwang</a>
<ul>
<li class="toc-entry toc-h2"><a href="#1-probability-and-counting">1. Probability and Counting</a></li>
<li class="toc-entry toc-h2"><a href="#2-conditional-probability">2. Conditional Probability</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</li>
</ul><h1 id="introduction-to-probability-by-blitzstein--hwang">
<a class="anchor" href="#introduction-to-probability-by-blitzstein--hwang" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction to Probability by Blitzstein &amp; Hwang</h1>
<ul>
  <li><a href="www.probabilitybook.net">Online Book</a></li>
</ul>

<h2 id="1-probability-and-counting">
<a class="anchor" href="#1-probability-and-counting" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Probability and Counting</h2>

<p><strong>Why study probability?</strong></p>
<ul>
  <li>Probability is the logic of uncertainty.</li>
  <li>Applications of probability: statistics, physics, biology, computer science, meteorology, gambling, finance, political science, medicine, life.</li>
</ul>

<p><strong>Sample spaces and Pebble World</strong></p>
<ul>
  <li>An experiment results in a set of outcomes, and the outcome is unknown before the experiment is run.</li>
  <li>
<strong>Sample Space</strong>: set of all possible outcomes of the experiment
    <ul>
      <li>The sample space $S$ can be finite, countably infinite, or un-countably infinite</li>
    </ul>
  </li>
  <li>
<strong>Event</strong>: subset of the sample space
    <ul>
      <li>Event $A$ <em>occurred</em> if the actual outcome of the experiment is in $A$</li>
    </ul>
  </li>
  <li>If $S$ is finite, we can visualize it as <em>pebble world</em>, in which each pebble is an outcome and each event is a set of pebbles
    <ul>
      <li>Performing an experiment is randomly selecting one pebble</li>
      <li>If all the pebbles are of the same mass, they are equally likely to be chosen, a special case</li>
    </ul>
  </li>
  <li>
<em>Example</em>: let $S$ be the sample space of an experiment and let $A,B \subseteq S$ be events
    <ul>
      <li>Union $A \cup B$ : the event that occurs if and only if <em>at least one</em> of $A, B$ occurs, “OR”</li>
      <li>Intersection $A \cap B$ : the event that occurs if and only if <em>both</em> $A$ and $B$ occur, “AND”</li>
      <li>Complement $A^C$ : the event that occurs if and only if $A$ does <em>not</em> occur, “NOT”</li>
    </ul>
  </li>
  <li>
<strong>De Morgan’s Laws</strong>
    <ul>
      <li>$(A \cup B)^C = A^C \cap B^C$ : it is not the case that at least one of $A, B$ occur is the same as $A$ does not occur and $B$ does not occur</li>
      <li>$(A \cap B)^C = A^C \cup B^C$ : it is not the case that both $A,B$ occur is the same as at least one $A$ or $B$ does not occur</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A coin is flipped 10 times.
    <ul>
      <li>A possible outcome is $HHHTHHTTHT$. The sample space is all possible strings of length 10 of $H$ and $T$. Let $H=1, T=0$, so the sample space is the set of all sequences in $(s_{1}, \dots, s_{10}), s_{j} \in {0, 1}$
        <ol>
          <li>Let $A_{1}$ be the event that the first flip is Heads.<br>
  $A_{1} = {(1, s_{2}, \dots, s_{10}) : s_{j} \in {0, 1} \text{ for } 2 \leq j \leq 10}$
  This event occurs when the first flip is Heads.</li>
        </ol>
      </li>
    </ul>

    <ol>
      <li>
        <p>Let $B$ be the event that at least one flip was Heads.
  $B = \bigcup_{j=1}^{10} A_{j}$
  Where $A_{j}$ is the event that the <em>j</em>-th flip is Heads for $j = 2, 3, \dots, 10$
  This event is the combination of events where any of the 10 coins being Heads.</p>
      </li>
      <li>
        <p>Let $C$ be the event that all flips were Heads.
  $C = \bigcap_{j=1}^{10} A_{j}$</p>
      </li>
      <li>
        <p>Let $D$ be the event there were at least two consecutive Heads.
  $D  \bigcup_{j=1}^{9} (A_{j} \cap A_{j+1})$</p>
      </li>
    </ol>
  </li>
  <li>
<em>Example</em>: Picking a card from a standard deck of 52 cards.
    <ol>
      <li>Let $A$ be the event the card is an ace.</li>
      <li>Let $B$ be the event the card has a black suit.</li>
      <li>Let $D$ be the event the card is a diamond.</li>
      <li>Let $H$ be the event the card is a heart.
        <ul>
          <li>$A \cap H$ is the event the card is ace of hearts.</li>
          <li>$A \cap B$ is the event the card is a black (spades, clubs) ace.</li>
          <li>$A \cup D \cup H$ is the event the card is red (hearts, diamonds) or an ace.</li>
          <li>$(A \cup B)^C = A^C \cap B^C$ is the event the card is not either an ace or black, or not ace and not black, or a red non-ace.</li>
          <li>$(D \cup H)^C = D^C \cap H^C = B$ is the event the card is not either a diamond or heart, or not diamond and not heart, or black</li>
          <li>If the card was a joker, then we had the wrong sample space since we assume the outcome of the experiment is guaranteed to an element of the sample space of the experiment.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p><strong>Notation</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">English</th>
      <th style="text-align: center">Sets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Events and occurrences</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: left">sample space</td>
      <td style="text-align: center">$S$</td>
    </tr>
    <tr>
      <td style="text-align: left">$s$ is a possible outcome</td>
      <td style="text-align: center">$s \in S$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ is an event</td>
      <td style="text-align: center">$A \subseteq S $</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ occurred</td>
      <td style="text-align: center">$s_{\text{actual}} \in A$</td>
    </tr>
    <tr>
      <td style="text-align: left">something must happen</td>
      <td style="text-align: center">$s_{\text{actual}} \in S$</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>New events from old events</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ or $B$ (inclusive)</td>
      <td style="text-align: center">$A \cup B$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ and $B$</td>
      <td style="text-align: center">$A \cap B$</td>
    </tr>
    <tr>
      <td style="text-align: left">not $A$</td>
      <td style="text-align: center">$A^{C}$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ or $B$ (exclusive)</td>
      <td style="text-align: center">$(A \cap B^{C}) \cup (A^{C} \cap B)$</td>
    </tr>
    <tr>
      <td style="text-align: left">at least one of $A_{1}, \dots , A_{n}$</td>
      <td style="text-align: center">$A_{1} \cup \dots \cup A_{n}$</td>
    </tr>
    <tr>
      <td style="text-align: left">all of $A_{1}, \dots , A_{n}$</td>
      <td style="text-align: center">$A_{1} \cap \dots \cap A_{n}$</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Relationships between events</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ implies $B$</td>
      <td style="text-align: center">$A \subseteq B$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ and $B$ are mutually exclusive</td>
      <td style="text-align: center">$A \cap B = \emptyset$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A_{1}, \dots , A_{n}$ are a partition of $S$</td>
      <td style="text-align: center">$A_{1} \cup \dots \cup A_{n} = S, A_{i} \cap A_{j} = \emptyset \text{ for } i \neq j$</td>
    </tr>
  </tbody>
</table>

<p><strong>Naive definition of probability</strong></p>
<ul>
  <li>The naive probability of an event is the count the number of ways the event could happen divided by total number of possible outcomes for the experiment
    <ul>
      <li>Restrictive and relies on strong assumptions</li>
    </ul>
  </li>
  <li>
    <p>Let $A$ be an event for an experiment with finite sample space $S$. The <strong>naive probability</strong> of event $A$ is:</p>

    <p>$P_{\text{naive}}(A) = \frac{\lvert A \rvert}{\lvert S \rvert} = \frac{\text{number of outcomes favorable to } A}{\text{total number of outcomes in } S}$</p>
  </li>
  <li>In terms of Pebble World, probability of $A$ is the fraction of all pebbles that are in $A$.</li>
  <li>
    <p>Similarly, the probability of the complement of event $A$ is made of the remaining events:</p>

    <p>$P_{\text{naive}}(A^{C}) = \frac{\lvert A^{C} \rvert}{\lvert S \rvert} = \frac{\lvert S \rvert - \lvert A \rvert}{\lvert S \rvert} = 1 - \frac{\lvert A \rvert}{\lvert S \rvert} = 1 - P_{\text{naive}(A)}$</p>
  </li>
  <li>The naive definition requires $S$ to be finite with equally likely outcomes.</li>
  <li>Can be applied in certain situations:
    <ul>
      <li>There is <em>symmetry</em> in the problem that makes outcomes equally likely. E.g. fair coin or deck of cards.</li>
      <li>The outcomes are equally likely <em>by design</em>. E.g. conducting a survey of $n$ people in a population of $N$ people using a simple random sample to select people.</li>
      <li>The naive definition serves as a useful <em>null model</em>. We <em>assume</em> the naive definition just to see what predictions it would yield, and then compare observed data with predicted values to assess whether outcomes are equally likely.</li>
    </ul>
  </li>
</ul>

<p><strong>How to count</strong></p>
<ul>
  <li>
    <p>Calculating probability often involves counting outcomes in event $A$ and outcomes in sample space $S$, but sets are often extremely large to count, so counting methods are used.</p>
  </li>
  <li>
<strong>Multiplication Rule</strong>: If an Experiment A has $a$ possible outcomes, and for each of those outcomes Experiment B has $b$ possible outcomes, then the compound experiment as $ab$ possible outcomes.
    <ul>
      <li>Each of the $a$ outcomes can lead to $b$ outcomes, so there are $b_{1} + \dots + b_{a} = ab$ possibilities.</li>
      <li>There’s no requirement Experiment A has to be performed before Experiment B, so no chronological order.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: Suppose 10 runners in a race, no ties and all 10 will complete the race. How many possibilities are there for first, second, and third place winners?
    <ul>
      <li>Any of the 10 runners can be in the first place, 9 remaining runners can be in second place, and 8 remaining runners can be in third place.</li>
      <li>There are $10 \times 9 \times 8 = 720$ possibilities.</li>
      <li>Could have also considered 10 runners in third place first, order of consideration not important.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many squares are there in an $8 \times 8$ chessboard?
    <ul>
      <li>To specify a square, can consider its position within 8 rows and 8 columns.</li>
      <li>There are $8 \times 8 = 64$ squares on the board.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many possible ice cream cone combinations if there are 2 types of cones and 3 types of flavors?
    <ul>
      <li>There are $2 \times 3 = 3 \times 2 = 6$ possibilities.</li>
      <li>Order of choice doesn’t matter, can either choose cone then flavor, or flavor then cone.</li>
      <li>Which flavors each cone could have doesn’t matter, only the <em>number of flavor choices</em> for each cone matters. If the waffle cones could only have 2 flavors, multiplication rule does not apply and there are $3 + 2 = 5$ possibilities.</li>
      <li>If you bought 2 ice creams in a single day $(x, y)$, there would be $6 \times 6 = 36$ possible combinations.</li>
      <li>If you didn’t want to distinguish between combinations $(x,y) = (y,x)$ , then there are 15 $(x, y), x \neq y$ possibilities and 6 $(x,x)$ possibilities, for a total of 21 possibilities.
        <ul>
          <li>Cannot just divide $36 / 2 = 18$ ! Since $(x,x)$ pairs are already only listed once each, must do $((6 \times 5) / 2) + 6 = 21$ possibilities.</li>
        </ul>
      </li>
      <li>If the original 36 cone-flavor pairs are equally likely, then the 21 possibilities are not equally likely.
        <ul>
          <li>Counting $(x,y) = (y, x)$ doubles likeliness, but $(x,x)$ pairs stay the same.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<em>Example</em>: A set of $n$ elements has $2^{n}$ subsets, including the empty set $\emptyset$ and the set itself.
    <ul>
      <li>From the multiplication rule, we can choose to include or exclude each element of the set.</li>
      <li>Multiplication rule: think of each number consideration as an experiment with 2 outcomes: include or exclude.</li>
      <li>${1, 2, 3}$ has 8 subsets: $\emptyset, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3}$
        <ul>
          <li>Consider first element 1, then consider second element 2, and then consider last element 3.</li>
          <li>$\emptyset$ is (exclude, exclude, exclude).</li>
          <li>${3}$ is (exclude, exclude, include).</li>
          <li>Order of consideration does not matter; could have considered element 3 first.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Sampling with Replacement</strong>: Consider $n$ objects and making $k$ choices from them, one at a time <em>with replacement</em>, then there are $n^{k}$ possible outcomes, where order matters $(x,y) \neq (y,x)$.</li>
  <li>
<em>Example</em>: Jar with $n$ balls, labeled from 1 to $n$.
    <ul>
      <li>Choosing a ball is an experiment with $n$ outcomes, and there are $k$ experiments.</li>
      <li>Therefore, multiplication rule says there are $n_{1} \times n_{k} = n^{k}$ possibilities.</li>
    </ul>
  </li>
  <li>
<strong>Sampling without Replacement</strong>: Consider $n$ objects and making $k$ choices from them, one at a time <em>without replacement</em>, then there are $n(n-1) \dots (n-k+1)$ possible outcomes, where order matters, for $1 \leq k \leq n$.
    <ul>
      <li>There are 0 possibilities, where order matters, for $k &gt; n$. You cannot choose more than the amount available.</li>
      <li>There are $n(n-1) \dots (n-k+1) = n$ possible outcomes for $k=1$.</li>
      <li>Multiplication rule: each sampled object is an experiment and the the number of outcomes decreases by 1 each time.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A <strong>permutation</strong> of $1, 2, \dots, n$ is an arrangement of the $n$ elements in some order.
    <ul>
      <li>Sampling without replacement: if $k=n$, there are $n!$ permutations of $n$ elements, or ways to choose $n$ items without replacement</li>
    </ul>
  </li>
  <li>We can use sampling with and without replacement as probabilities when the naive definition of probability applies.</li>
  <li>
<em>Example</em>: <strong>Birthday Problem</strong>
    <ul>
      <li>There are $k$ people in a room. Assume each person’s birthday is equally likely to be any of the 365 days of the year, and people’s birthdays are independent, e.g. no twins. What is the probability that at least one pair of people in the group have the same birthday?</li>
      <li>If we sample with replacement $k$ days from $n = 365$ total days, what is the probability at least two samples are the same?
        <ul>
          <li>There are $365^{k}$ possible outcomes.</li>
          <li>But how can we count all the ways that two more more people have the same birthday?</li>
        </ul>
      </li>
      <li>Instead, think about the complement: the number of ways to sample without replacement $k$ different days of $n = 365$ total days. What is the probability that no two people share the same birthday?
        <ul>
          <li>$P(\text{no match}) = \frac{365 \times 364 \times \dots \times (365 - k + 1)}{365^{k}}$</li>
        </ul>
      </li>
      <li>Therefore, the probability of at least one birthday match is the complement.
        <ul>
          <li>$P(\geq 1\text{ match}) = 1 - \frac{365 \times 364 \times \dots \times (365 - k + 1)}{365^{k}}$</li>
        </ul>
      </li>
      <li>At $k = 23$, there is over a 50% chance that two people share the same birthday, and over a 99% chance at $k = 57$.</li>
      <li>Intuition: For $k = 23$ people, there are $\binom{23}{2} = 253$ <em>pairs</em> of people, any of which could be a birthday match.</li>
    </ul>
  </li>
  <li>Think of the objects or people in the population as <em>named</em> or <em>labeled</em>, rather than indistinguishable.</li>
  <li>
<em>Example</em>: If we roll two fair dice, is a sum of 11 or sum of 12 more likely?
    <ul>
      <li>Label dice A and B and consider each die to be a sub-experiment. Multiplication rule says there are $6 \times 6 = 36$ possibilities, in the form $(a,b)$.</li>
      <li>11 can be made from $(5,6), (6,5)$, while 12 can only be made from $(6,6)$. Therefore, a sum of 11, which has a probability of $\frac{2}{38} = \frac{1}{18}$, is twice as likely as a sum of 12, which has a probability $\frac{1}{36}$.</li>
    </ul>
  </li>
  <li>
<strong>Overcounting</strong>: It is difficult to directly count each possible outcome once, and we may need to <em>adjust for overcounting</em> by dividing the count by some factor $c$.</li>
  <li>
<em>Example</em>: For a group of four people, how many ways are there to
    <ul>
      <li>Choose a two-person group?
        <ul>
          <li>Labeling people as $1, 2, 3, 4$, we can see possible groups are ${12, 13, 14, 23, 24, 34}$, so there are 6 possible ways.</li>
          <li>Multiplication rule and adjust for overcounting: There are 4 ways to choose the first person, and 3 ways to choose the second person, or $4 \times 3 = 12$. But since $(a,b) = (b,a)$, we have overcounted by 2, so there are $(4 \cdot 3) / 2 = 6$ possible ways.</li>
        </ul>
      </li>
      <li>Break people into two teams of two?
        <ul>
          <li>Labeling people as $1, 2, 3, 4$, we can see possible groups are $(12, 34), (13, 24), (14, 23)$, so there are 3 possible ways.</li>
          <li>We can also note that choosing a team of 2  (choosing person 1’s partner) determines other team to be formed from remaining people. There are 3 other people to pair to person 1, so there are 3 possible ways.</li>
          <li>We can also see that the 6 possible ways to choose a two-person group is overcounting by a factor of 2, since the other team is determined from choosing one team $(a,b) = (c,d)$, so there are $6 / 2 = 3$ possible ways.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Binomial Coefficient</strong>: For any nonnegative integers $k,n$, the binomial coefficient $\binom{n}{k}$ (“n choose k”) is the number of subsets of size $k$ for a set of size $n$. E.g. The number of ways to choose a group of size $k = 2$ from $n = 4$ people is $\binom{4}{2} = 6$.
    <ul>
      <li>Sets are <em>unordered</em> by definition $(a,b) = (b,a)$, so a binomial coefficient counts the number of ways to choose $k$ objects out of $n$, <em>without replacement</em> and <em>without distinguishing between different orders</em>.</li>
      <li>For $k \leq n$, $\binom{n}{k} = \frac{n (n-1) \dots (n-k+1)}{k!} = \frac{n!}{(n-k)!k!}$.</li>
      <li>For $k &gt; n, \binom{n}{k} = 0$.</li>
      <li>For naive definition problems, binomial coefficient can be used to calculate probabilities.</li>
    </ul>
  </li>
  <li>
<em>Proof</em>: Let $A$ be set with $\lvert A \rvert = n$. Any subset of $A$ has size at most $n$, so $\binom{n}{k} = 0$ for $k &gt; n$, or no ways to create a larger subset than what’s available. If $k \leq n$, then sampling without replacement tells us there are $n(n-1) \dots (n-k+1)$ ways to make <em>ordered</em> choice of $k$ elements without replacement. But this overcounts each subset by $k!$ since order does not matter for binomial coefficient $(a,b) = (b,a)$, so we divide by $k!$ to get $\binom{n}{k} = \frac{n (n-1) \dots (n-k+1)}{k!} = \frac{n!}{(n-k)!k!}$.</li>
  <li>
<em>Example</em>: In a club of $n$ people, how many ways are there to choose 3 officers?
    <ul>
      <li>There are $\binom{n}{3} = \frac{n(n-1)(n-2)}{3!}$ possible ways.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many ways are there to permute the letters in the word “LALALAAA”?
    <ul>
      <li>We can just decide the positions of either the 3 L’s or the 5 A’s. So there are $\binom{8}{3} = \binom{8}{5} = \frac{8 \cdot 7 \cdot 6}{3!} = 56$ possible permutations.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many ways are there to permute the letters in the word “STATISTICS”?
    <ul>
      <li>We can choose positions for 3 S’s, 3 T’s, 2 I’s, 1 A, and 1 C is determined. So there are $\binom{10}{3}\binom{7}{3}\binom{4}{2}\binom{2}{1} = 50400$ permutations.</li>
      <li>We can also start with $10!$ permutations and account for overcounting of a factor of $3!3!2!$ for the 3 S’s, 3 T’s, and 2 I’s because the repeated letters can be permuted among themselves in any way. So there are $\frac{10!}{3!3!2!} = 50400$ permutations.</li>
    </ul>
  </li>
  <li>
<strong>Binomial Theorem</strong>: For any nonnegative integer $n$, $(x + y)^{n} = \sum_{k=0}^{n} \binom{n}{k} x^{k} y^{n-k}$</li>
  <li>
<em>Proof</em>: We decompose the term $(x+y)^{n} = \underbrace{(x + y) (x + y) \dots (x + y)}_{n \text{ factors}}$
    <ul>
      <li>The terms of $(x + y)^{n}$ are obtained by picking either the $x$ or the $y$ from each factor $(x + y)$. This is similar to $(a + b)(c + d) = ab + ac + bc + bd$, where each product term is created from only one term of each factor.</li>
      <li>There are $\binom{n}{k}$ ways to choose exactly $k$ of the $x \text{‘s}$ to make the term $x^{k} y^{n-k}$.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A 5-card hand is dealt from a standard, well-shuffled 52-card deck. What is the probability of a full house (3 cards of some rank, 2 cards of another rank)?
    <ul>
      <li>Since all of the $\binom{52}{5}$ total possible hands are equally likely by <em>symmetry</em>, the naive definition of probability applies.</li>
      <li>Multiplication rule: Choosing a rank for the 3 cards is one experiment, and choosing the suits of the 3 cards is another experiment. Similarly, choosing a rank for the 2 cards is one experiment, and choosing the suits of the 2 cards is another experiment.</li>
      <li>So the probability of a full house is $\frac{13 \binom{4}{3} \times 12 \binom{4}{2} }{ \binom{52}{5} } = \frac{3744}{2598960} \approx 0.00144$</li>
      <li>
<em>Note</em>: We cannot use $\binom{13}{2}$ to choose the ranks because that approach treats the <em>order</em> of the ranks as interchangeable, which undercounts by a factor of 2.
        <ul>
          <li>E.g. $( \underbrace{7 \heartsuit, 7 \diamondsuit, 7 \clubsuit}<em>{3 \text{-card}}, \underbrace{5 \diamondsuit, 5 \clubsuit}</em>{2 \text{-card}} ) \neq ( \underbrace{5 \heartsuit, 5 \diamondsuit, 5 \clubsuit}<em>{3 \text{-card}}, \underbrace{7 \diamondsuit, 7 \clubsuit}</em>{2 \text{-card}} )$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>Newton-Pepys Problem</strong>
    <ul>
      <li>Which of the following events has the highest probability?
        <ul>
          <li>Event A: At least one 6 appears when 6 fair dice are rolled.</li>
          <li>Event B: At least two 6’s appear when 12 fair dice are rolled.</li>
          <li>Event C: At least three 6’s appear when 18 fair dice are rolled.</li>
        </ul>
      </li>
      <li>The three experiments have $6^{6}, 6^{12}, 6^{18}$ possible outcomes, respectively.</li>
      <li>By <em>symmetry</em>, all outcomes are equally likely, so the naive definition applies in all three experiments.</li>
      <li>Event A
        <ul>
          <li>Rather than count all possible ways to roll at least one 6, we consider the complement of all possible ways to roll no 6’s. $P(A) = 1 - P(A^{C}), P(\geq \text{ one } 6) = 1 - P(\text{no } 6)$</li>
          <li>Therefore, $P(A) = 1 - \frac{5^{6}}{6^{6}} \approx 0.67$</li>
        </ul>
      </li>
      <li>Event B
        <ul>
          <li>Similarly, we consider the complement of all possible ways to roll either zero or one 6’s in 12 dice rolls. $P(B) = 1 - P(\text{zero } 6) - P(\text{one } 6)$</li>
          <li>Therefore, $P(B) = 1 - \frac{5^{12} + \binom{12}{1} 5^{11}}{6^{12}} \approx 0.62$</li>
        </ul>
      </li>
      <li>Event C
        <ul>
          <li>Similarly, we consider the complement of all possible ways to roll either zero, one, or two 6’s in 18 dice rolls. $P(C) = 1 - P(\text{zero } 6) - P(\text{one } 6) - P(\text{two } 6)$</li>
          <li>Therefore, $P(C) = 1 - \frac{5^{18} + \binom{18}{1} 5^{17} + \binom{18}{2} 5^{16}}{6^{18}} \approx 0.62$</li>
        </ul>
      </li>
      <li>The event with the highest probability is Event A.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>Bose-Einstein Problem</strong>
    <ul>
      <li>How many ways are there to choose $k$ objects from a set of $n$ objects with replacement, if order does not matter?</li>
      <li>When order does matter, we know from sampling with replacement there are $n^{k}$ times, but what about when order does not matter?</li>
      <li>We consider an <em>isomorphic</em> (equivalent) problem: How many ways are there to put $k$ indistinguishable particles into $n$ distinguishable boxes?
        <ul>
          <li>We use $\bullet$ to represent a particle and $\vert$ to represent a wall.</li>
          <li>Consider putting $k=7$ particles in $n=4$ boxes, such as $\vert \bullet \vert \bullet \bullet \vert \bullet \bullet \bullet \vert \bullet \vert$</li>
          <li>A sequence must start and end with a wall, and it must have exactly $n - 1$ walls and $k$ particles in between. There are $(n - 1) + k$ slots between 2 outer walls to place $n - 1$ inner walls and $k$ particles, so the number of possible placements is $\binom{n + k - 1}{k}$.</li>
        </ul>
      </li>
      <li>We can consider another isomorphic problem: How many solutions $(x_{1}, \dots, x_{n})$ to the equation $x_{1} + x_{2} + \dots + x_{n} = k$, where $x_{i}$ are nonnegative integers. We can think of $x_{i}$ as the number of particles in the $i \text{th}$ box.</li>
      <li>
<em>Note</em>: Bose-Einstein result cannot be used in naive definition of probability in most cases.
        <ul>
          <li>E.g. Survey by sampling $k$ people from population of size $n$ one at a time, with replacement and equal probabilities. The $n^{k}$ <em>ordered</em> samples are equally likely (naive definition applies), but the $\binom{n + k - 1}{k}$ <em>unordered</em> samples are <em>not</em> equally likely (naive definition does not apply).</li>
          <li>E.g. How many <em>unordered</em> birthday lists are possible for $k$ people and $n=365$ days in a year? For $k=3$, we want to count lists, where $(a, b, c) = (c, b, a)$ and so on, but we cannot simply adjust for overcounting like $\frac{n^{k}}{3!}$ for the permutations because there are $3!$ permutations for $(a, b, c)$ but only $3$ permutations for $(a, a, c)$. The <em>ordered</em> birthday lists are equally likely (naive definition applies), but the <em>unordered</em> birthday lists are not equally likely (naive definition does not apply) as the number of permutations is not equal across all lists.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Story Proofs</strong></p>
<ul>
  <li>
<em>*Story Proof</em>: a proof by interpretation for explaining why results of counting problems are true</li>
  <li>
<em>Example</em>: For any nonnegative integers $n, k$ with $k \leq n$, $\binom{n}{k} = \binom{n}{n-k}$.
    <ul>
      <li>
<em>Story</em>: Consider choosing a committee of size $k$ in a group of $n$ people, which has $\binom{n}{k}$ possibilities.</li>
      <li>Another way is to choose the complement, or $n-k$ people <em>not</em> on the committee, which has $\binom{n}{n-k}$ possibilities.</li>
      <li>The two methods are equal because they count the same thing.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: For any positive integers $n, k$ with $k \leq n$, $n \binom{n - 1}{k - 1} = k \binom{n}{k}$.
    <ul>
      <li>
<em>Story</em>: Consider choosing a team of $k$ people, with one being the team captain, from $n$ total people. We could first choose the team captain and then choose the remaining $k-1$ team members for $n \binom{n-1}{k-1}$ possibilities.</li>
      <li>We could also first choose the team of $k$ members and then choose who is the team captain, for $\binom{n}{k} k$ possibilities.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>Vandermonde’s Identity</strong> states $\binom{m + n}{k} = \sum_{j=0}^{k} \binom{m}{j} \binom{n}{k-j}$
    <ul>
      <li>
<em>Story</em>: Consider an organization of $m$ juniors and $n$ seniors and choosing a committee of $k$ members, for $\binom{m + n}{k}$ possible committee formations.</li>
      <li>This is equal to all of ways to form committees where there are $j$ juniors and the remaining $k-j$ members are seniors, for $\sum_{j=0}^{k} \binom{m}{j} \binom{n}{k-j}$ possibilities.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: Proving $\frac{(2n)!}{2^{n} \cdot n!} = (2n - 1)(2n - 3) \dots (3)(1)$ describes the number of ways to break $2n$ people into $n$ partnerships.
    <ul>
      <li>
<em>Story</em>: Take $2n$ people and label them with IDs $1 \dots 2n$. We can form pairs by taking one of $(2n)!$ orderings and grouping adjacent people, but this overcounts by a factor of $n! \cdot 2^{n}$.
        <ul>
          <li>Order of pairs does not matter: $(\underbrace{1, 2}<em>{\text{pair 1}}, \underbrace{3, 4}</em>{\text{pair 2}}) = (\underbrace{3, 4}<em>{\text{pair 1}}, \underbrace{1, 2}</em>{\text{pair 2}})$</li>
          <li>Order within pairs does not matter: $(\underbrace{1, 2}<em>{\text{pair 1}}, \underbrace{3, 4}</em>{\text{pair 2}}), (\underbrace{2, 1}<em>{\text{pair 1}}, \underbrace{4, 3}</em>{\text{pair 2}})$</li>
        </ul>
      </li>
      <li>We can also consider the number of possible groups by noting there are $(2n-1)$ possible partners for person 1, $(2n - 3)$ possible partners for person 2, and so on.</li>
    </ul>
  </li>
</ul>

<p><strong>Non-naive definition of probability</strong></p>
<ul>
  <li>
<strong>General Definition of Probability</strong>: A <em>probability space</em> consists of a sample space $S$ and a <em>probability function</em> $P$ which takes an event $A \subseteq S$ as input and returns $P(A)$, a real number between $0$ and $1$, as output. The function $P$ must satisfy the following axioms:
    <ol>
      <li>$P(\emptyset) = 0, P(S) = 1$
        <ul>
          <li>Probability of a non-existent event is 0; probability of an event in S is 1 (certain).</li>
        </ul>
      </li>
      <li>If $A_{1}, A_{2}, \dots $ are <em>disjoint</em> events, then $P(\bigcup_{j=1}^{\infty} A_{j}) = \sum_{j=1}^{\infty} P(A_{j})$
        <ul>
          <li>The probability of the union of disjoint events is equal to the sum of the individual probabilities.</li>
          <li>
<em>Disjoint</em> events are <em>mutually exclusive</em> such that $A_{i} \cap A_{j} = \emptyset$ for $i \neq j$.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>In Pebble World, general probability is like mass; mass of empty pile is 0 and total mass of all pebble is 1. Given non-overlapping piles of pebbles, we can get combined mass by adding individual masses, and we can have a countably infinite number of pebbles as long as the total mass is 1.</li>
  <li>Any function $P$ that maps events to the interval $[0, 1]$ that satisfies the two axioms is a <em>valid probability function</em>, but the axioms don’t state how the probability should be <em>interpreted</em>.
    <ul>
      <li>
<em>Frequentist View</em>: Probability represents a long-run frequency over a large number of repetitions of an experiment. E.g. If we say a coin has a probability of $1/2$ of Heads, the coin will land Heads 50% of the time if tossed over and over.</li>
      <li>
<em>Bayesian View</em>: Probability represents a degree of belief about the event in question, so probabilities can be assigned to hypotheses, such as “candidate A will win the election” or “the defendant is guilty”, even if it isn’t possible to repeat the same election or same crime over and over again.</li>
      <li>
<em>Frequentist</em> and <em>Bayesian</em> views are complementary.</li>
    </ul>
  </li>
  <li>
<strong>Properties of Probability</strong>: For any events $A, B$:
    <ol>
      <li>Complement: $P(A^{C}) = 1 - P(A)$
        <ul>
          <li>
<em>Proof</em>: Since $A$ and $A^{C}$ are disjoint and their union makes up $S$, $P(S) = P(A \cup A^{C}) = P(A) + P(A^{C})$. Since $P(S) = 1$ by the first axiom, $P(A) + P(A^{C}) = 1$.</li>
        </ul>
      </li>
      <li>Subset: If $A \subseteq B$, then $P(A) \leq P(B)$
        <ul>
          <li>
<em>Proof</em>: Since $A$ and $B \cap A^{C}$ are disjoint, $P(B) = P(A \cup (B \cap A^{C})) = P(A) + P(B \cap A^{C})$ by the second axiom. Probability is nonnegative, so $P(B \cap A^{C}) \geq 0$, proving $P(B) \geq P(A)$.</li>
          <li>The probability of Event B is all outcomes in Event A and all outcomes in Event B but not in Event A. Since these groups are disjoint, we can add the individual probabilities together. Since we know group in Event B but not in Event A has a positive probability, we know Event A must be smaller than group B.</li>
        </ul>
      </li>
      <li>Inclusion-Exclusion: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        <ul>
          <li>
<em>Proof</em>: The probability of a union of two events is the sum of separate probabilities minus any overlap. $P(A \cup B) = P(A) + P(B \cap A^{C}) = P(A) + P(B \cap A^C)$ by the second axiom, since $A$ and $B \cap A^{C}$ are disjoint events. Since $A \cap B$ and $B \cap A^{C}$ are disjoint and $P(A \cap B) + P(B \cap A^{C}) = P(B)$ by the second axiom, then $P(B \cap A^{C}) = P(B) - P(A \cap B)$. Plugging this in, we get $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
<strong>Inclusion-Exclusion</strong>: For any events $A_{1}, \dots, A_{n}$, $P(\bigcup_{i=1}^{n} A_{i}) = \sum_{i} P(A_{i}) - \sum_{i &lt; j} P(A_{i} \cap A_{j}) + \sum_{i &lt; j &lt; k} P(A_{i} \cap A_{j} \cap A_{k}) - \dots + (-1)^{n + 1} P(A_{1} \cap \dots \cap A_{n})$
    <ul>
      <li>The probability of the union of events is equal to the sum of the individual probabilities minus any overlapping and add back any non-overlapping that was overly removed.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>de Montmort’s Matching Problem</strong>
    <ul>
      <li>Consider a well-shuffled deck of $n$ cards, labeled $1 \dots n$. You flip over cards one by one while saying numbers $1 \dots n$. If the number you say is the same number as the card, you win. What is the probability of winning?</li>
      <li>Let $A_{i}$ be the event that the $i \text{th}$ card in the deck has the number $i$ written on it. We want to find the probability of the union $P(A_{1} \cup \dots \cup A_{n})$, which is the win condition. A ordering with no matching card numbers to positions is called a <em>derangement</em>.</li>
      <li>We know $P(A_{i}) = \frac{1}{n}$.
        <ul>
          <li>
<em>Naive Probability</em>: The probability of a card number matching its position is $P(A_{i}) = \frac{(n-1)!}{n!} = \frac{1}{n}$, or the number of orderings where the card matches its position $(n-1)!$ divided by all possible orderings $n!$.</li>
          <li>
<em>Symmetry</em>: Alternatively, the card with number $i$ is equally likely to be in any of the $n$ positions in the deck, so the probability of a card number matching its position is $P(A_{i}) = \frac{1}{n}$.</li>
        </ul>
      </li>
      <li>Similarly, $P(A_{i} \cap A_{j}) = \frac{(n - 2)!}{n!} = \frac{1}{n(n - 1)}$.
        <ul>
          <li>Since we fix the cards with numbers $i,j$ to be in the $i \text{th}, j \text{th}$ spots in deck, there are $(n-2)!$ permutations out of $n!$ total permutations.</li>
        </ul>
      </li>
      <li>Similarly, $P(A_{i} \cap A_{j} \cap A_{k}) = \frac{1}{n(n - 1)(n - 2)}$, and so on.</li>
      <li>
<em>Inclusion-Exclusion</em>: There are $\binom{n}{i}$ terms involving $i$ events, representing the number of ways to intersect $i$ events, i.e. $n$ terms for 1-event, $\binom{n}{2}$ terms for 2-events, etc.
        <ul>
          <li>By <em>symmetry</em>, all $\binom{n}{i}$ terms are equal, e.g. $P(A_{i}) = 1/n \text{, for all }i$.</li>
          <li>Therefore, $P(\bigcup_{i=1}^{n} A_{i}) = \frac{n}{n} - \frac{\binom{n}{2}}{n(n - 1)} + \frac{\binom{n}{3}}{n(n - 1)(n - 2)} - \dots + (-1)^{n+1} \cdot \frac{1}{n!}$</li>
          <li>Simplify, $P(\bigcup_{i=1}^{n} A_{i}) = 1 - \frac{1}{2!} + \frac{1}{3!} - \dots + (-1)^{n+1} \cdot \frac{1}{n!}$</li>
        </ul>
      </li>
      <li>Similar to the Taylor series for $e^{-1} = 1 - \frac{1}{1!} + \frac{1}{2!} - \frac{1}{3!} + \dots$, we see that the probability of winning approaches $1 - 1/e \approx 0.63$ as $n$ gets larger.</li>
      <li>Rather than approaching 0 or 1, the probability of winning is determined by the competing forces of increasing possible locations for matching $(n)$ and the decreasing probability of any particular match $(1/n)$.</li>
    </ul>
  </li>
  <li>Inclusion-Exclusion is a general formula for <em>probability of a union of events</em> that is most helpful when there is <em>symmetry</em> among events (which helps us add up all terms and simplify easily). In general, consider inclusion-exclusion as a <em>last resort if no symmetry</em>.</li>
</ul>

<h2 id="2-conditional-probability">
<a class="anchor" href="#2-conditional-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Conditional Probability</h2>

<p><strong>The Importance of Thinking Conditionally</strong></p>
<ul>
  <li>All probabilities are conditional. There is always background knowledge/assumptions built into every probability. Conditional probability addresses the question: how should we update our beliefs in light of the evidence we observe?</li>
  <li>
<em>Example</em>: Let $R$ be the event that it will rain that day. Let $P(R)$ be the assessment of the probability of rain <em>before</em> looking outside. After looking outside and seeing ominous clouds, our probability of rain should increase, as in $P(R \vert C)$ (“probability of R given C”) where $C$ is the event of there being ominous clouds. We are <em>conditioning on</em> $C$ when we go from $P(R) \rightarrow P(R \vert C)$.
    <ul>
      <li>If we get more information about the weather, we can keep updating our probabilities. If we observe events $B_{1}, \dots, B_{n}$ occurred, we can write a new conditional probability of rain $P(R \vert B_{1}, \dots, B_{n})$. If it does start raining, our conditional probability becomes 1.</li>
    </ul>
  </li>
  <li>We can use <strong>conditioning</strong> to solve complex problems by decomposing it into simpler conditional probability problems. The <strong>first-step analysis</strong> strategy can help find recursive solutions to problems where the experiment has multiple stages.</li>
  <li><em>Conditioning is the soul of statistics.</em></li>
</ul>

<p><strong>Definition and Intuition</strong></p>
<ul>
  <li>
<strong>Conditional Probability</strong>: If $A, B$ are events with $P(B) &gt; 0$, then the <em>conditional probability</em> of $A$ given $B$ is: $P(A \vert B) = \frac{P(A \cap B)}{P(B)}$.
    <ul>
      <li>The chance of $A$ happening given $B$ is equal to the number of outcomes where both $A$ and $B$ happen over all the outcomes where $B$ happens.</li>
      <li>Here we update the uncertainty of event $A$, and event $B$ is the evidence we observe (or the evidence given). $P(A)$ is the <strong>prior</strong> probability of $A$ (before updating based on evidence), and $P(A \vert B)$ is the <strong>posterior</strong> probability of $A$ (after updating based on evidence).</li>
    </ul>
  </li>
  <li>For any event $A$, $P(A \vert A) = \frac{P(A \cap A)}{P(A)} = 1$. If we observe event $A$ occurs, then the updated probability $P(A \vert A) = 1$.</li>
  <li>
<em>Example</em>: Draw two cards, one at a time without replacement, randomly from a shuffled deck of cards. Let $A$ be the event that the first card is a Heart and $B$ be the event that the second card is red.
    <ul>
      <li>Since each of the 4 suits are equally likely, $P(A) = 13/52 = 1/4$.</li>
      <li>Since for each of the 26 red cards that can be the second card, the first card can be any other card, $P(B) = (\frac{26}{52}) (\frac{51}{51}) = 1/2$. Chronological order not needed in multiplication rule.
        <ul>
          <li>Another way is to see that $P(B) = 1/2$ by <em>symmetry</em>. Before the experiment, the second card is equally likely to be any card in the deck.</li>
        </ul>
      </li>
      <li>By the naive definition of probability and the multiplication rule: $P(A \cap B) = (\frac{13}{52})(\frac{25}{51}) = \frac{25}{204}$
        <ul>
          <li>There are 13 Heart cards and then there are 25 remaining red cards.</li>
        </ul>
      </li>
      <li>$P(A \vert B) = \frac{P(A \cap B)}{P(B)} = \frac{25/204}{1/2} = \frac{25}{102}$</li>
      <li>$P(B \vert A) = \frac{P(B \cap A)}{P(A)} = \frac{25/204}{1/4} = \frac{25}{51}$</li>
      <li>
<em>Important</em>:
        <ol>
          <li>$P(A \vert B) \neq P(B \vert A)$. Confusing these terms is called the <strong>prosecutor’s fallacy</strong>. If $B$ was the event the second card is a heart then $P(A \vert B) = P(B \vert A)$.</li>
          <li>$P(A \vert B), P(B \vert A)$ both make sense. The chronological order in which cards were chosen does not matter. We only consider what <em>information</em> observing one event provides about another event, not whether one event <em>causes</em> another, in conditional probabilities.
            <ul>
              <li>
<em>Intuition</em>: Imagine drawing cards using left and right hands at the same time. Defining events $A,B$ based on the left and right card rather than first and second card would not change the structure of the problem in any important way.</li>
            </ul>
          </li>
          <li>$P(B \vert A) = 25/51$ means: if a first card drawn is a heart, then the remaining cards consist of 25 red cards and 26 black cards that are equally likely to be drawn next.</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

  </div><a class="u-url" href="/notes/probability-book" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes and thoughts from a lifelong student.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nhtsai" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/nhtsai" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
