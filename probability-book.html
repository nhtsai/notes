<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="robots" content="none, noindex, nofollow, noarchive, noimageindex, nosnippet" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Introduction to Probability | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Introduction to Probability" />
<meta name="author" content="Nathan Tsai" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Book notes on probability." />
<meta property="og:description" content="Book notes on probability." />
<link rel="canonical" href="https://nhtsai.github.io/notes/probability-book" />
<meta property="og:url" content="https://nhtsai.github.io/notes/probability-book" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-26T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Introduction to Probability" />
<script type="application/ld+json">
{"url":"https://nhtsai.github.io/notes/probability-book","@type":"BlogPosting","headline":"Introduction to Probability","dateModified":"2021-10-26T00:00:00-05:00","datePublished":"2021-10-26T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nhtsai.github.io/notes/probability-book"},"author":{"@type":"Person","name":"Nathan Tsai"},"description":"Book notes on probability.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nhtsai.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
    <!-- KaTeX Auto-render Function with custom delimiter rules 
         see: https://katex.org/docs/autorender.html
    -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement( document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "[%", right: "%]", display: true},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true}
                ]}
                );
            });
        </script>
    <!-- MathJax -->
    <!-- <script defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.min.js" integrity="sha512-93xLZnNMlYI6xaQPf/cSdXoBZ23DThX7VehiGJJXB76HTTalQKPC5CIHuFX8dlQ5yzt6baBQRJ4sDXhzpojRJA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> -->


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to Probability</h1><p class="page-description">Book notes on probability.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-26T00:00:00-05:00" itemprop="datePublished">
        Oct 26, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nathan Tsai</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      46 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#book-notes">book-notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#mathematics">mathematics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#probability">probability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#statistics">statistics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#introduction-to-probability-by-blitzstein--hwang">Introduction to Probability by Blitzstein &amp; Hwang</a>
<ul>
<li class="toc-entry toc-h2"><a href="#1-probability-and-counting">1. Probability and Counting</a></li>
<li class="toc-entry toc-h2"><a href="#2-conditional-probability">2. Conditional Probability</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</li>
</ul><h1 id="introduction-to-probability-by-blitzstein--hwang">
<a class="anchor" href="#introduction-to-probability-by-blitzstein--hwang" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction to Probability by Blitzstein &amp; Hwang</h1>
<ul>
  <li><a href="www.probabilitybook.net">Online Book</a></li>
</ul>

<h2 id="1-probability-and-counting">
<a class="anchor" href="#1-probability-and-counting" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Probability and Counting</h2>

<p><strong>Why study probability?</strong></p>
<ul>
  <li>Probability is the logic of uncertainty.</li>
  <li>Applications of probability: statistics, physics, biology, computer science, meteorology, gambling, finance, political science, medicine, life.</li>
</ul>

<p><strong>Sample spaces and Pebble World</strong></p>
<ul>
  <li>An experiment results in a set of outcomes, and the outcome is unknown before the experiment is run.</li>
  <li>
<strong>Sample Space</strong>: set of all possible outcomes of the experiment
    <ul>
      <li>The sample space $S$ can be finite, countably infinite, or un-countably infinite</li>
    </ul>
  </li>
  <li>
<strong>Event</strong>: subset of the sample space
    <ul>
      <li>Event $A$ <em>occurred</em> if the actual outcome of the experiment is in $A$</li>
    </ul>
  </li>
  <li>If $S$ is finite, we can visualize it as <em>pebble world</em>, in which each pebble is an outcome and each event is a set of pebbles
    <ul>
      <li>Performing an experiment is randomly selecting one pebble</li>
      <li>If all the pebbles are of the same mass, they are equally likely to be chosen, a special case</li>
    </ul>
  </li>
  <li>
<em>Example</em>: let $S$ be the sample space of an experiment and let $A,B \subseteq S$ be events
    <ul>
      <li>Union $A \cup B$ : the event that occurs if and only if <em>at least one</em> of $A, B$ occurs, “OR”</li>
      <li>Intersection $A \cap B$ : the event that occurs if and only if <em>both</em> $A$ and $B$ occur, “AND”</li>
      <li>Complement $A^C$ : the event that occurs if and only if $A$ does <em>not</em> occur, “NOT”</li>
    </ul>
  </li>
  <li>
<strong>De Morgan’s Laws</strong>
    <ul>
      <li>$(A \cup B)^C = A^C \cap B^C$ : it is not the case that at least one of $A, B$ occur is the same as $A$ does not occur and $B$ does not occur</li>
      <li>$(A \cap B)^C = A^C \cup B^C$ : it is not the case that both $A,B$ occur is the same as at least one $A$ or $B$ does not occur</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A coin is flipped 10 times.
    <ul>
      <li>A possible outcome is $HHHTHHTTHT$. The sample space is all possible strings of length 10 of $H$ and $T$. Let $H=1, T=0$, so the sample space is the set of all sequences in $(s_{1}, \dots, s_{10}), s_{j} \in {0, 1}$</li>
      <li>Let $A_{1}$ be the event that the first flip is Heads.<br>
  $A_{1} = {(1, s_{2}, \dots, s_{10}) : s_{j} \in {0, 1} \text{ for } 2 \leq j \leq 10}$.</li>
      <li>Let $B$ be the event that at least one flip was Heads.<br>
  $B = \bigcup_{j=1}^{10} A_{j}$<br>
  Where $A_{j}$ is the event that the <em>j</em>-th flip is Heads for $j = 1, 2, \dots, 10$.</li>
      <li>Let $C$ be the event that all flips were Heads.<br>
  $C = \bigcap_{j=1}^{10} A_{j}$</li>
      <li>Let $D$ be the event there were at least two consecutive Heads.<br>
  $D = \bigcup_{j=1}^{9} (A_{j} \cap A_{j+1})$</li>
    </ul>
  </li>
  <li>
<em>Example</em>: Picking a card from a standard deck of 52 cards.
    <ol>
      <li>Let $A$ be the event the card is an ace.</li>
      <li>Let $B$ be the event the card has a black suit.</li>
      <li>Let $D$ be the event the card is a diamond.</li>
      <li>Let $H$ be the event the card is a heart.
        <ul>
          <li>$A \cap H$ is the event the card is ace of hearts.</li>
          <li>$A \cap B$ is the event the card is a black (spades, clubs) ace.</li>
          <li>$A \cup D \cup H$ is the event the card is red (hearts, diamonds) or an ace.</li>
          <li>$(A \cup B)^C = A^C \cap B^C$ is the event the card is not either an ace or black, or not ace and not black, or a red non-ace.</li>
          <li>$(D \cup H)^C = D^C \cap H^C = B$ is the event the card is not either a diamond or heart, or not diamond and not heart, or black</li>
          <li>If the card was a joker, then we had the wrong sample space since we assume the outcome of the experiment is guaranteed to an element of the sample space of the experiment.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p><strong>Notation</strong></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">English</th>
      <th style="text-align: center">Sets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Events and occurrences</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: left">sample space</td>
      <td style="text-align: center">$S$</td>
    </tr>
    <tr>
      <td style="text-align: left">$s$ is a possible outcome</td>
      <td style="text-align: center">$s \in S$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ is an event</td>
      <td style="text-align: center">$A \subseteq S $</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ occurred</td>
      <td style="text-align: center">$s_{\text{actual}} \in A$</td>
    </tr>
    <tr>
      <td style="text-align: left">something must happen</td>
      <td style="text-align: center">$s_{\text{actual}} \in S$</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>New events from old events</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ or $B$ (inclusive)</td>
      <td style="text-align: center">$A \cup B$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ and $B$</td>
      <td style="text-align: center">$A \cap B$</td>
    </tr>
    <tr>
      <td style="text-align: left">not $A$</td>
      <td style="text-align: center">$A^{C}$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ or $B$ (exclusive)</td>
      <td style="text-align: center">$(A \cap B^{C}) \cup (A^{C} \cap B)$</td>
    </tr>
    <tr>
      <td style="text-align: left">at least one of $A_{1}, \dots , A_{n}$</td>
      <td style="text-align: center">$A_{1} \cup \dots \cup A_{n}$</td>
    </tr>
    <tr>
      <td style="text-align: left">all of $A_{1}, \dots , A_{n}$</td>
      <td style="text-align: center">$A_{1} \cap \dots \cap A_{n}$</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Relationships between events</strong></td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ implies $B$</td>
      <td style="text-align: center">$A \subseteq B$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A$ and $B$ are mutually exclusive</td>
      <td style="text-align: center">$A \cap B = \emptyset$</td>
    </tr>
    <tr>
      <td style="text-align: left">$A_{1}, \dots , A_{n}$ are a partition of $S$</td>
      <td style="text-align: center">$A_{1} \cup \dots \cup A_{n} = S, A_{i} \cap A_{j} = \emptyset \text{ for } i \neq j$</td>
    </tr>
  </tbody>
</table>

<p><strong>Naive definition of probability</strong></p>
<ul>
  <li>The naive probability of an event is the count the number of ways the event could happen divided by total number of possible outcomes for the experiment
    <ul>
      <li>The naive definition requires $S$ to be finite with equally likely outcomes.</li>
    </ul>
  </li>
  <li>
    <p>Let $A$ be an event for an experiment with finite sample space $S$. The <strong>naive probability</strong> of event $A$ is:</p>

    <p>$P_{\text{naive}}(A) = \frac{\lvert A \rvert}{\lvert S \rvert} = \frac{\text{number of outcomes favorable to } A}{\text{total number of outcomes in } S}$</p>
  </li>
  <li>In terms of Pebble World, probability of $A$ is the fraction of all pebbles that are in $A$.</li>
  <li>
    <p>Similarly, the probability of the complement of event $A$ is made of the remaining events:</p>

    <p>$P_{\text{naive}}(A^{C}) = \frac{\lvert A^{C} \rvert}{\lvert S \rvert} = \frac{\lvert S \rvert - \lvert A \rvert}{\lvert S \rvert} = 1 - \frac{\lvert A \rvert}{\lvert S \rvert} = 1 - P_{\text{naive}(A)}$</p>
  </li>
  <li>Can be applied in certain situations:
    <ul>
      <li>There is <em>symmetry</em> in the problem that makes outcomes equally likely. E.g. fair coin or deck of cards.</li>
      <li>The outcomes are equally likely <em>by design</em>. E.g. conducting a survey of $n$ people in a population of $N$ people using a simple random sample to select people.</li>
      <li>The naive definition serves as a useful <em>null model</em>. We <em>assume</em> the naive definition just to see what predictions it would yield, and then compare observed data with predicted values to assess whether outcomes are equally likely.</li>
    </ul>
  </li>
</ul>

<p><strong>How to count</strong></p>
<ul>
  <li>
    <p>Calculating probability often involves counting outcomes in event $A$ and outcomes in sample space $S$, but sets are often extremely large to count, so counting methods are used.</p>
  </li>
  <li>
<strong>Multiplication Rule</strong>: If an Experiment A has $a$ possible outcomes, and for each of those outcomes Experiment B has $b$ possible outcomes, then the compound experiment as $ab$ possible outcomes.
    <ul>
      <li>Each of the $a$ outcomes can lead to $b$ outcomes, so there are $b_{1} + \dots + b_{a} = ab$ possibilities.</li>
      <li>There’s no requirement Experiment A has to be performed before Experiment B, so no chronological order.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: Suppose 10 runners in a race, no ties and all 10 will complete the race. How many possibilities are there for first, second, and third place winners?
    <ul>
      <li>Any of the 10 runners can be in the first place, 9 remaining runners can be in second place, and 8 remaining runners can be in third place. So there are $10 \times 9 \times 8 = 720$ possibilities.</li>
      <li>The order of consideration not important: could have considered 10 runners in third place first.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many squares are there in an $8 \times 8$ chessboard?
    <ul>
      <li>To specify a square, can consider its row position and its column position. So there are $8 \times 8 = 64$ squares on the board.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many possible ice cream cone combinations if there are 2 types of cones and 3 types of flavors?
    <ul>
      <li>There are $2 \times 3 = 3 \times 2 = 6$ possibilities.</li>
      <li>Order of choice doesn’t matter, can either choose cone then flavor, or flavor then cone.</li>
      <li>Which flavors each cone could have doesn’t matter, only the <em>number of flavor choices</em> for each cone matters. If the waffle cones could only have 2 flavors, multiplication rule does not apply and there are $3 + 2 = 5$ possibilities.</li>
      <li>If you bought 2 ice creams in a single day $(x_{\text{cone}}, y_{\text{flavor}})$, there would be $6 \times 6 = 36$ possible combinations.</li>
      <li>If you didn’t want to distinguish between combinations $(x,y) = (y,x)$ , then we have  $15 \text{ of } (x, y), x \neq y$ possibilities and $6 \text{ of } (x, x)$ possibilities, for a total of 21 possibilities.
        <ul>
          <li>Cannot just divide $36 / 2 = 18$ ! Since $(x,x)$ pairs are already only listed once each, must do $((6 \times 5) / 2) + 6 = 21$ possibilities.</li>
        </ul>
      </li>
      <li>If the original 36 cone-flavor pairs are equally likely, then the 21 possibilities are not equally likely.
        <ul>
          <li>Counting $(x, y) = (y, x)$ doubles likeliness, but the likeliness of $(x, x)$ pairs stays does not change.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<em>Example</em>: A set of $n$ elements has $2^{n}$ subsets, including the empty set $\emptyset$ and the set itself.
    <ul>
      <li>From the multiplication rule, we can choose to include or exclude each element of the set. Think of each number consideration as an experiment with 2 outcomes: include or exclude.</li>
      <li>A set of 3 elements ${1, 2, 3}$ has 8 subsets: $\emptyset, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3}$
        <ul>
          <li>Consider first element 1, then consider second element 2, and then consider last element 3.</li>
          <li>$\emptyset$ is (exclude, exclude, exclude).</li>
          <li>${3}$ is (exclude, exclude, include).</li>
          <li>Order of consideration does not matter: could have considered element 3 first.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Sampling with Replacement</strong>: Consider $n$ objects and making $k$ choices from them, one at a time <em>with replacement</em>, then there are $n^{k}$ possible outcomes, where order matters $(x,y) \neq (y,x)$.</li>
  <li>
<em>Example</em>: Jar with $n$ balls, labeled from 1 to $n$.
    <ul>
      <li>Choosing a ball is an experiment with $n$ outcomes, and there are $k$ experiments.</li>
      <li>Therefore, multiplication rule says there are $n_{1} \times n_{k} = n^{k}$ possibilities.</li>
    </ul>
  </li>
  <li>
<strong>Sampling without Replacement</strong>: Consider $n$ objects and making $k$ choices from them, one at a time <em>without replacement</em>, then there are $n(n-1) \dots (n-k+1)$ possible outcomes, where order matters, for $1 \leq k \leq n$.
    <ul>
      <li>There are 0 possibilities, where order matters, for $k &gt; n$. You cannot choose more than the amount available.</li>
      <li>There are $n(n-1) \dots (n-k+1) = n$ possible outcomes for $k=1$.</li>
      <li>Multiplication rule: each sampled object is an experiment and the the number of outcomes decreases by 1 each time.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A <strong>permutation</strong> of $1, 2, \dots, n$ is an arrangement of the $n$ elements in some order.
    <ul>
      <li>Sampling without replacement: if $k=n$, there are $n!$ permutations of $n$ elements, or ways to choose $n$ items without replacement</li>
    </ul>
  </li>
  <li>We can use sampling with and without replacement as probabilities when the naive definition of probability applies.</li>
  <li>
<em>Example</em>: <strong>Birthday Problem</strong>
    <ul>
      <li>There are $k$ people in a room. Assume each person’s birthday is equally likely to be any of the 365 days of the year, and people’s birthdays are independent, e.g. no twins. What is the probability that at least one pair of people in the group have the same birthday?</li>
      <li>If we sample with replacement $k$ days from $n = 365$ total days, what is the probability at least two samples are the same?
        <ul>
          <li>There are $365^{k}$ possible outcomes.</li>
          <li>But how can we count all the ways that two more more people have the same birthday?</li>
        </ul>
      </li>
      <li>Instead, think about the complement: the number of ways to sample without replacement $k$ different days of $n = 365$ total days. What is the probability that no two people share the same birthday?
        <ul>
          <li>$P(\text{no match}) = \frac{365 \times 364 \times \dots \times (365 - k + 1)}{365^{k}}$</li>
        </ul>
      </li>
      <li>Therefore, the probability of at least one birthday match is the complement.
        <ul>
          <li>$P(\geq 1\text{ match}) = 1 - \frac{365 \times 364 \times \dots \times (365 - k + 1)}{365^{k}}$</li>
        </ul>
      </li>
      <li>At $k = 23$, there is over a 50% chance that two people share the same birthday, and over a 99% chance at $k = 57$.</li>
      <li>Intuition: For $k = 23$ people, there are $\binom{23}{2} = 253$ <em>pairs</em> of people, any of which could be a birthday match.</li>
    </ul>
  </li>
  <li>Think of the objects or people in the population as <em>named</em> or <em>labeled</em>, rather than indistinguishable.</li>
  <li>
<em>Example</em>: If we roll two fair dice, is a sum of 11 or sum of 12 more likely?
    <ul>
      <li>Label dice A and B and consider each die to be a sub-experiment. Multiplication rule says there are $6 \times 6 = 36$ possibilities, in the form $(a,b)$.</li>
      <li>11 can be made from $(5,6), (6,5)$, while 12 can only be made from $(6,6)$. Therefore, a sum of 11, which has a probability of $\frac{2}{38} = \frac{1}{18}$, is twice as likely as a sum of 12, which has a probability $\frac{1}{36}$.</li>
    </ul>
  </li>
  <li>
<strong>Overcounting</strong>: It is difficult to directly count each possible outcome once, and we may need to <em>adjust for overcounting</em> by dividing the count by some factor $c$.</li>
  <li>
<em>Example</em>: For a group of four people, how many ways are there to
    <ul>
      <li>Choose a two-person group?
        <ul>
          <li>Labeling people as $1, 2, 3, 4$, we can see possible groups are ${12, 13, 14, 23, 24, 34}$, so there are 6 possible ways.</li>
          <li>Multiplication rule and adjust for overcounting: There are 4 ways to choose the first person, and 3 ways to choose the second person, or $4 \times 3 = 12$. But since $(a,b) = (b,a)$, we have overcounted by 2, so there are $(4 \cdot 3) / 2 = 6$ possible ways.</li>
        </ul>
      </li>
      <li>Break people into two teams of two?
        <ul>
          <li>Labeling people as $1, 2, 3, 4$, we can see possible groups are $(12, 34), (13, 24), (14, 23)$, so there are 3 possible ways.</li>
          <li>We can also note that choosing a team of 2  (choosing person 1’s partner) determines other team to be formed from remaining people. There are 3 other people to pair to person 1, so there are 3 possible ways.</li>
          <li>We can also see that the 6 possible ways to choose a two-person group is overcounting by a factor of 2, since the other team is determined from choosing one team $(a,b) = (c,d)$, so there are $6 / 2 = 3$ possible ways.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<strong>Binomial Coefficient</strong>: For any nonnegative integers $k,n$, the binomial coefficient $\binom{n}{k}$ (“n choose k”) is the number of subsets of size $k$ for a set of size $n$. E.g. The number of ways to choose a group of size $k = 2$ from $n = 4$ people is $\binom{4}{2} = 6$.
    <ul>
      <li>Sets are <em>unordered</em> by definition $(a,b) = (b,a)$, so a binomial coefficient counts the number of ways to choose $k$ objects out of $n$, <em>without replacement</em> and <em>without distinguishing between different orders</em>.</li>
      <li>For $k \leq n$, $\binom{n}{k} = \frac{n (n-1) \dots (n-k+1)}{k!} = \frac{n!}{(n-k)!k!}$.</li>
      <li>For $k &gt; n, \binom{n}{k} = 0$.</li>
      <li>For naive definition problems, binomial coefficient can be used to calculate probabilities.</li>
    </ul>
  </li>
  <li>
<em>Proof</em>: Let $A$ be set with $\lvert A \rvert = n$. Any subset of $A$ has size at most $n$, so $\binom{n}{k} = 0$ for $k &gt; n$, or no ways to create a larger subset than what’s available. If $k \leq n$, then sampling without replacement tells us there are $n(n-1) \dots (n-k+1)$ ways to make <em>ordered</em> choice of $k$ elements without replacement. But this overcounts each subset by $k!$ since order does not matter for binomial coefficient $(a,b) = (b,a)$, so we divide by $k!$ to get $\binom{n}{k} = \frac{n (n-1) \dots (n-k+1)}{k!} = \frac{n!}{(n-k)!k!}$.</li>
  <li>
<em>Example</em>: In a club of $n$ people, how many ways are there to choose 3 officers?
    <ul>
      <li>There are $\binom{n}{3} = \frac{n(n-1)(n-2)}{3!}$ possible ways.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many ways are there to permute the letters in the word “LALALAAA”?
    <ul>
      <li>We can just decide the positions of either the 3 L’s or the 5 A’s. So there are $\binom{8}{3} = \binom{8}{5} = \frac{8 \cdot 7 \cdot 6}{3!} = 56$ possible permutations.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: How many ways are there to permute the letters in the word “STATISTICS”?
    <ul>
      <li>We can choose positions for 3 S’s, 3 T’s, 2 I’s, 1 A, and 1 C is determined. So there are $\binom{10}{3}\binom{7}{3}\binom{4}{2}\binom{2}{1} = 50400$ permutations.</li>
      <li>We can also start with $10!$ permutations and account for overcounting of a factor of $3!3!2!$ for the 3 S’s, 3 T’s, and 2 I’s because the repeated letters can be permuted among themselves in any way. So there are $\frac{10!}{3!3!2!} = 50400$ permutations.</li>
    </ul>
  </li>
  <li>
<strong>Binomial Theorem</strong>: For any nonnegative integer $n$, $(x + y)^{n} = \sum_{k=0}^{n} \binom{n}{k} x^{k} y^{n-k}$</li>
  <li>
<em>Proof</em>: We decompose the term $(x+y)^{n} = \underbrace{(x + y) (x + y) \dots (x + y)}_{n \text{ factors}}$
    <ul>
      <li>The terms of $(x + y)^{n}$ are obtained by picking either the $x$ or the $y$ from each factor $(x + y)$. This is similar to $(a + b)(c + d) = ab + ac + bc + bd$, where each product term is created from only one term of each factor.</li>
      <li>There are $\binom{n}{k}$ ways to choose exactly $k$ of the $x \text{‘s}$ to make the term $x^{k} y^{n-k}$.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A 5-card hand is dealt from a standard, well-shuffled 52-card deck. What is the probability of a full house (3 cards of some rank, 2 cards of another rank)?
    <ul>
      <li>Since all of the $\binom{52}{5}$ total possible hands are equally likely by <em>symmetry</em>, the naive definition of probability applies.</li>
      <li>Multiplication rule: Choosing a rank for the 3 cards is one experiment, and choosing the suits of the 3 cards is another experiment. Similarly, choosing a rank for the 2 cards is one experiment, and choosing the suits of the 2 cards is another experiment.</li>
      <li>So the probability of a full house is $\frac{13 \binom{4}{3} \times 12 \binom{4}{2} }{ \binom{52}{5} } = \frac{3744}{2598960} \approx 0.00144$</li>
      <li>
<em>Note</em>: We cannot use $\binom{13}{2}$ to choose the ranks because that approach treats the <em>order</em> of the ranks as interchangeable, which undercounts by a factor of 2.
        <ul>
          <li>E.g. $(7 \heartsuit, 7 \diamondsuit, 7 \clubsuit, 5 \diamondsuit, 5 \clubsuit) \neq (5 \heartsuit, 5 \diamondsuit, 5 \clubsuit, 7 \diamondsuit, 7 \clubsuit)$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>Newton-Pepys Problem</strong>
    <ul>
      <li>Which of the following events has the highest probability?
        <ul>
          <li>Event A: At least one 6 appears when 6 fair dice are rolled.</li>
          <li>Event B: At least two 6’s appear when 12 fair dice are rolled.</li>
          <li>Event C: At least three 6’s appear when 18 fair dice are rolled.</li>
        </ul>
      </li>
      <li>The three experiments have $6^{6}, 6^{12}, 6^{18}$ possible outcomes, respectively.</li>
      <li>By <em>symmetry</em>, all outcomes are equally likely, so the naive definition applies in all three experiments.</li>
      <li>Event A
        <ul>
          <li>Rather than count all possible ways to roll at least one 6, we consider the complement of all possible ways to roll no 6’s. $P(A) = 1 - P(A^{C}), P(\geq \text{ one } 6) = 1 - P(\text{no } 6)$</li>
          <li>Therefore, $P(A) = 1 - \frac{5^{6}}{6^{6}} \approx 0.67$</li>
        </ul>
      </li>
      <li>Event B
        <ul>
          <li>Similarly, we consider the complement of all possible ways to roll either zero or one 6’s in 12 dice rolls. $P(B) = 1 - P(\text{zero } 6) - P(\text{one } 6)$</li>
          <li>Therefore, $P(B) = 1 - \frac{5^{12} + \binom{12}{1} 5^{11}}{6^{12}} \approx 0.62$</li>
        </ul>
      </li>
      <li>Event C
        <ul>
          <li>Similarly, we consider the complement of all possible ways to roll either zero, one, or two 6’s in 18 dice rolls. $P(C) = 1 - P(\text{zero } 6) - P(\text{one } 6) - P(\text{two } 6)$</li>
          <li>Therefore, $P(C) = 1 - \frac{5^{18} + \binom{18}{1} 5^{17} + \binom{18}{2} 5^{16}}{6^{18}} \approx 0.60$</li>
        </ul>
      </li>
      <li>The event with the highest probability is Event A.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>Bose-Einstein Problem</strong>
    <ul>
      <li>How many ways are there to choose $k$ objects from a set of $n$ objects with replacement, if order does not matter?</li>
      <li>When order does matter, we know from sampling with replacement there are $n^{k}$ times, but what about when order does not matter?</li>
      <li>We consider an <em>isomorphic</em> (equivalent) problem: How many ways are there to put $k$ indistinguishable particles into $n$ distinguishable boxes?
        <ul>
          <li>We use $\bullet$ to represent a particle and $\vert$ to represent a wall.</li>
          <li>Consider putting $k=7$ particles in $n=4$ boxes, such as $\vert \bullet \vert \bullet \bullet \vert \bullet \bullet \bullet \vert \bullet \vert$</li>
          <li>A sequence must start and end with a wall, and it must have exactly $n - 1$ walls and $k$ particles in between. There are $(n - 1) + k$ slots between 2 outer walls to place $n - 1$ inner walls and $k$ particles, so the number of possible placements is $\binom{n + k - 1}{k}$.</li>
        </ul>
      </li>
      <li>We can consider another isomorphic problem: How many solutions $(x_{1}, \dots, x_{n})$ to the equation $x_{1} + x_{2} + \dots + x_{n} = k$, where $x_{i}$ are nonnegative integers. We can think of $x_{i}$ as the number of particles in the $i \text{th}$ box.</li>
      <li>
<em>Note</em>: Bose-Einstein result cannot be used in naive definition of probability in most cases.
        <ul>
          <li>E.g. Survey by sampling $k$ people from population of size $n$ one at a time, with replacement and equal probabilities. The $n^{k}$ <em>ordered</em> samples are equally likely (naive definition applies), but the $\binom{n + k - 1}{k}$ <em>unordered</em> samples are <em>not</em> equally likely (naive definition does not apply).</li>
          <li>E.g. How many <em>unordered</em> birthday lists are possible for $k$ people and $n=365$ days in a year? For $k=3$, we want to count lists, where $(a, b, c) = (c, b, a)$ and so on, but we cannot simply adjust for overcounting like $\frac{n^{k}}{3!}$ for the permutations because there are $3!$ permutations for $(a, b, c)$ but only $3$ permutations for $(a, a, c)$. The <em>ordered</em> birthday lists are equally likely (naive definition applies), but the <em>unordered</em> birthday lists are not equally likely (naive definition does not apply) as the number of permutations is not equal across all lists.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Story Proofs</strong></p>
<ul>
  <li>
<em>*Story Proof</em>: a proof by interpretation for explaining why results of counting problems are true</li>
  <li>
<em>Example</em>: For any nonnegative integers $n, k$ with $k \leq n$, $\binom{n}{k} = \binom{n}{n-k}$.
    <ul>
      <li>
<em>Story</em>: Consider choosing a committee of size $k$ in a group of $n$ people, which has $\binom{n}{k}$ possibilities.</li>
      <li>Another way is to choose the complement, or $n-k$ people <em>not</em> on the committee, which has $\binom{n}{n-k}$ possibilities.</li>
      <li>The two methods are equal because they count the same thing.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: For any positive integers $n, k$ with $k \leq n$, $n \binom{n - 1}{k - 1} = k \binom{n}{k}$.
    <ul>
      <li>
<em>Story</em>: Consider choosing a team of $k$ people, with one being the team captain, from $n$ total people. We could first choose the team captain and then choose the remaining $k-1$ team members for $n \binom{n-1}{k-1}$ possibilities.</li>
      <li>We could also first choose the team of $k$ members and then choose who is the team captain, for $\binom{n}{k} k$ possibilities.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>Vandermonde’s Identity</strong> states $\binom{m + n}{k} = \sum_{j=0}^{k} \binom{m}{j} \binom{n}{k-j}$
    <ul>
      <li>
<em>Story</em>: Consider an organization of $m$ juniors and $n$ seniors and choosing a committee of $k$ members, for $\binom{m + n}{k}$ possible committee formations.</li>
      <li>This is equal to all of ways to form committees where there are $j$ juniors and the remaining $k-j$ members are seniors, for $\sum_{j=0}^{k} \binom{m}{j} \binom{n}{k-j}$ possibilities.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: Proving $\frac{(2n)!}{2^{n} \cdot n!} = (2n - 1)(2n - 3) \dots (3)(1)$ describes the number of ways to break $2n$ people into $n$ partnerships.
    <ul>
      <li>
<em>Story</em>: Take $2n$ people and label them with IDs $1 \dots 2n$. We can form pairs by taking one of $(2n)!$ orderings and grouping adjacent people, but this overcounts by a factor of $n! \cdot 2^{n}$.
        <ul>
          <li>Order of pairs does not matter: $(1, 2, 3, 4) = (3, 4, 1, 2)$</li>
          <li>Order within pairs does not matter: $(1, 2, 3, 4) = (2, 1, 4, 3)$</li>
        </ul>
      </li>
      <li>We can also consider the number of possible groups by noting there are $(2n-1)$ possible partners for person 1, $(2n - 3)$ possible partners for person 2, and so on.</li>
    </ul>
  </li>
</ul>

<p><strong>Non-naive definition of probability</strong></p>
<ul>
  <li>
<strong>General Definition of Probability</strong>: A <em>probability space</em> consists of a sample space $S$ and a <em>probability function</em> $P$ which takes an event $A \subseteq S$ as input and returns $P(A)$, a real number between $0$ and $1$, as output. The function $P$ must satisfy the following axioms:
    <ol>
      <li>$P(\emptyset) = 0, P(S) = 1$
        <ul>
          <li>Probability of a non-existent event is 0; probability of an event in S is 1 (certain).</li>
        </ul>
      </li>
      <li>If $A_{1}, A_{2}, \dots $ are <em>disjoint</em> events, then $P(\bigcup_{j=1}^{\infty} A_{j}) = \sum_{j=1}^{\infty} P(A_{j})$
        <ul>
          <li>The probability of the union of disjoint events is equal to the sum of the individual probabilities.</li>
          <li>
<em>Disjoint</em> events are <em>mutually exclusive</em> such that $A_{i} \cap A_{j} = \emptyset$ for $i \neq j$.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>In Pebble World, general probability is like mass; mass of empty pile is 0 and total mass of all pebble is 1. Given non-overlapping piles of pebbles, we can get combined mass by adding individual masses, and we can have a countably infinite number of pebbles as long as the total mass is 1.</li>
  <li>Any function $P$ that maps events to the interval $[0, 1]$ that satisfies the two axioms is a <em>valid probability function</em>, but the axioms don’t state how the probability should be <em>interpreted</em>.
    <ul>
      <li>
<em>Frequentist View</em>: Probability represents a long-run frequency over a large number of repetitions of an experiment. E.g. If we say a coin has a probability of $1/2$ of Heads, the coin will land Heads 50% of the time if tossed over and over.</li>
      <li>
<em>Bayesian View</em>: Probability represents a degree of belief about the event in question, so probabilities can be assigned to hypotheses, such as “candidate A will win the election” or “the defendant is guilty”, even if it isn’t possible to repeat the same election or same crime over and over again.</li>
      <li>
<em>Frequentist</em> and <em>Bayesian</em> views are complementary.</li>
    </ul>
  </li>
  <li>
<strong>Properties of Probability</strong>: For any events $A, B$:
    <ol>
      <li>Complement: $P(A^{C}) = 1 - P(A)$
        <ul>
          <li>
<em>Proof</em>: Since $A$ and $A^{C}$ are disjoint and their union makes up $S$, $P(S) = P(A \cup A^{C}) = P(A) + P(A^{C})$. Since $P(S) = 1$ by the first axiom, $P(A) + P(A^{C}) = 1$.</li>
        </ul>
      </li>
      <li>Subset: If $A \subseteq B$, then $P(A) \leq P(B)$
        <ul>
          <li>
<em>Proof</em>: Since $A$ and $B \cap A^{C}$ are disjoint, $P(B) = P(A \cup (B \cap A^{C})) = P(A) + P(B \cap A^{C})$ by the second axiom. Probability is nonnegative, so $P(B \cap A^{C}) \geq 0$, proving $P(B) \geq P(A)$.</li>
          <li>The probability of Event B is all outcomes in Event A and all outcomes in Event B but not in Event A. Since these groups are disjoint, we can add the individual probabilities together. Since we know group in Event B but not in Event A has a positive probability, we know Event A must be smaller than group B.</li>
        </ul>
      </li>
      <li>Inclusion-Exclusion: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        <ul>
          <li>
<em>Proof</em>: The probability of a union of two events is the sum of separate probabilities minus any overlap. $P(A \cup B) = P(A) + P(B \cap A^{C}) = P(A) + P(B \cap A^C)$ by the second axiom, since $A$ and $B \cap A^{C}$ are disjoint events. Since $A \cap B$ and $B \cap A^{C}$ are disjoint and $P(A \cap B) + P(B \cap A^{C}) = P(B)$ by the second axiom, then $P(B \cap A^{C}) = P(B) - P(A \cap B)$. Plugging this in, we get $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
<strong>Inclusion-Exclusion</strong>: For any events $A_{1}, \dots, A_{n}$, $P(\bigcup_{i=1}^{n} A_{i}) = \sum_{i} P(A_{i}) - \sum_{i &lt; j} P(A_{i} \cap A_{j}) + \sum_{i &lt; j &lt; k} P(A_{i} \cap A_{j} \cap A_{k}) - \dots + (-1)^{n + 1} P(A_{1} \cap \dots \cap A_{n})$
    <ul>
      <li>The probability of the union of events is equal to the sum of the individual probabilities minus any overlapping and add back any non-overlapping that was overly removed.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: <strong>de Montmort’s Matching Problem</strong>
    <ul>
      <li>Consider a well-shuffled deck of $n$ cards, labeled $1 \dots n$. You flip over cards one by one while saying numbers $1 \dots n$. If the number you say is the same number as the card, you win. What is the probability of winning?</li>
      <li>Let $A_{i}$ be the event that the $i \text{th}$ card in the deck has the number $i$ written on it. We want to find the probability of the union $P(A_{1} \cup \dots \cup A_{n})$, which is the win condition. A ordering with no matching card numbers to positions is called a <em>derangement</em>.</li>
      <li>We know $P(A_{i}) = \frac{1}{n}$.
        <ul>
          <li>
<em>Naive Probability</em>: The probability of a card number matching its position is $P(A_{i}) = \frac{(n-1)!}{n!} = \frac{1}{n}$, or the number of orderings where the card matches its position $(n-1)!$ divided by all possible orderings $n!$.</li>
          <li>
<em>Symmetry</em>: Alternatively, the card with number $i$ is equally likely to be in any of the $n$ positions in the deck, so the probability of a card number matching its position is $P(A_{i}) = \frac{1}{n}$.</li>
        </ul>
      </li>
      <li>Similarly, $P(A_{i} \cap A_{j}) = \frac{(n - 2)!}{n!} = \frac{1}{n(n - 1)}$.
        <ul>
          <li>Since we fix the cards with numbers $i,j$ to be in the $i \text{th}, j \text{th}$ spots in deck, there are $(n-2)!$ permutations out of $n!$ total permutations.</li>
        </ul>
      </li>
      <li>Similarly, $P(A_{i} \cap A_{j} \cap A_{k}) = \frac{1}{n(n - 1)(n - 2)}$, and so on.</li>
      <li>
<em>Inclusion-Exclusion</em>: There are $\binom{n}{i}$ terms involving $i$ events, representing the number of ways to intersect $i$ events, i.e. $n$ terms for 1-event, $\binom{n}{2}$ terms for 2-events, etc.
        <ul>
          <li>By <em>symmetry</em>, all $\binom{n}{i}$ terms are equal, e.g. $P(A_{i}) = 1/n \text{, for all }i$.</li>
          <li>Therefore, $P(\bigcup_{i=1}^{n} A_{i}) = \frac{n}{n} - \frac{\binom{n}{2}}{n(n - 1)} + \frac{\binom{n}{3}}{n(n - 1)(n - 2)} - \dots + (-1)^{n+1} \cdot \frac{1}{n!}$</li>
          <li>Simplify, $P(\bigcup_{i=1}^{n} A_{i}) = 1 - \frac{1}{2!} + \frac{1}{3!} - \dots + (-1)^{n+1} \cdot \frac{1}{n!}$</li>
        </ul>
      </li>
      <li>Similar to the Taylor series for $e^{-1} = 1 - \frac{1}{1!} + \frac{1}{2!} - \frac{1}{3!} + \dots$, we see that the probability of winning approaches $1 - 1/e \approx 0.63$ as $n$ gets larger.</li>
      <li>Rather than approaching 0 or 1, the probability of winning is determined by the competing forces of increasing possible locations for matching $(n)$ and the decreasing probability of any particular match $(1/n)$.</li>
    </ul>
  </li>
  <li>Inclusion-Exclusion is a general formula for <em>probability of a union of events</em> that is most helpful when there is <em>symmetry</em> among events (which helps us add up all terms and simplify easily). In general, consider inclusion-exclusion as a <em>last resort if no symmetry</em>.</li>
</ul>

<h2 id="2-conditional-probability">
<a class="anchor" href="#2-conditional-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Conditional Probability</h2>

<p><strong>The Importance of Thinking Conditionally</strong></p>
<ul>
  <li>All probabilities are conditional. There is always background knowledge/assumptions built into every probability. Conditional probability addresses the question: how should we update our beliefs in light of the evidence we observe?</li>
  <li>
<em>Example</em>: Let $R$ be the event that it will rain that day. Let $P(R)$ be the assessment of the probability of rain <em>before</em> looking outside. After looking outside and seeing ominous clouds, our probability of rain should increase, as in $P(R \vert C)$ (“probability of R given C”) where $C$ is the event of there being ominous clouds. We are <em>conditioning on</em> $C$ when we go from $P(R) \rightarrow P(R \vert C)$.
    <ul>
      <li>If we get more information about the weather, we can keep updating our probabilities. If we observe events $B_{1}, \dots, B_{n}$ occurred, we can write a new conditional probability of rain $P(R \vert B_{1}, \dots, B_{n})$. If it does start raining, our conditional probability becomes 1.</li>
    </ul>
  </li>
  <li>We can use <strong>conditioning</strong> to solve complex problems by decomposing it into simpler conditional probability problems. The <strong>first-step analysis</strong> strategy can help find recursive solutions to problems where the experiment has multiple stages.</li>
  <li><em>Conditioning is the soul of statistics.</em></li>
</ul>

<p><strong>Definition and Intuition</strong></p>
<ul>
  <li>
<strong>Conditional Probability</strong>: If $A, B$ are events with $P(B) &gt; 0$, then the <em>conditional probability</em> of $A$ given $B$ is: $P(A \vert B) = \frac{P(A \cap B)}{P(B)}$.
    <ul>
      <li>The chance of $A$ happening given $B$ is equal to the number of outcomes where both $A$ and $B$ happen over all the outcomes where $B$ happens.</li>
      <li>Here we update the uncertainty of event $A$, and event $B$ is the evidence we observe (or the evidence given). $P(A)$ is the <strong>prior</strong> probability of $A$ (before updating based on evidence), and $P(A \vert B)$ is the <strong>posterior</strong> probability of $A$ (after updating based on evidence).</li>
    </ul>
  </li>
  <li>For any event $A$, $P(A \vert A) = \frac{P(A \cap A)}{P(A)} = 1$. If we observe event $A$ occurs, then the updated probability $P(A \vert A) = 1$.</li>
  <li>
<em>Example</em>: Draw two cards, one at a time without replacement, randomly from a shuffled deck of cards. Let $A$ be the event that the first card is a Heart and $B$ be the event that the second card is red.
    <ul>
      <li>Since each of the 4 suits are equally likely, $P(A) = 13/52 = 1/4$.</li>
      <li>
<em>Multiplication Rule</em>: The second card can be any of the 26 red cards, and for each of those, the first card can be any other of the 51 remaining cards. (Chronological order not needed in multiplication rule.) Therefore, $P(B) = (\frac{26}{52}) (\frac{51}{51}) = 1/2$.
        <ul>
          <li>Another way is to see that $P(B) = 1/2$ by <em>symmetry</em>. Before the experiment and before we know anything about Event A, the second card is equally likely to be any card in the deck, where the 26 red cards are favorable.</li>
        </ul>
      </li>
      <li>By the naive definition of probability and the multiplication rule: $P(B \cap A) = P(A \cap B) = (\frac{13}{52})(\frac{25}{51}) = \frac{25}{204}$
        <ul>
          <li>The first card has 13 favorable Heart cards and the second card has 25 remaining, favorable red cards.</li>
        </ul>
      </li>
      <li>$P(A \vert B) = \frac{P(A \cap B)}{P(B)} = \frac{25/204}{1/2} = \frac{25}{102}$
        <ul>
          <li>The probability that the first card is a heart, given the second card is red. If we know the second card is red, we are interested in knowing if it is a diamond or a heart.</li>
        </ul>
      </li>
      <li>$P(B \vert A) = \frac{P(B \cap A)}{P(A)} = \frac{25/204}{1/4} = \frac{25}{51}$
        <ul>
          <li>The probability that the second card is red, given the first card is a heart. If we know the first card is a heart, then we know there are 25 of the remaining 51 cards are red.</li>
        </ul>
      </li>
      <li>
<em>Important</em>:
        <ol>
          <li>$P(A \vert B) \neq P(B \vert A)$. Confusing these terms is called the <strong>prosecutor’s fallacy</strong>. If $B$ was the event the second card is a heart then $P(A \vert B) = P(B \vert A)$.</li>
          <li>$P(A \vert B), P(B \vert A)$ both make sense. The chronological order in which cards were chosen does not matter. We only consider what <em>information</em> observing one event provides about another event, not whether one event <em>causes</em> another, in conditional probabilities.
            <ul>
              <li>
<em>Intuition</em>: Imagine drawing cards using left and right hands at the same time. Defining events $A,B$ based on the left and right card rather than first and second card would not change the structure of the problem in any important way.</li>
            </ul>
          </li>
          <li>$P(B \vert A) = 25/51$ means: if a first card drawn is a heart, then the remaining cards consist of 25 red cards and 26 black cards that are equally likely to be drawn next.</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>
<em>Intuition</em>: Consider a finite sample space, with the outcomes visualized as pebbles with total mass 1. Events $A,B$ are sets of pebbles. If we are given that event $B$ occurred, then we can remove all pebbles in $B^{C}$ and know $P(A \cap B)$ is the total mass of the pebbles remaining in $A$. We <em>renormalize</em> to make the mass of the remaining pebbles sum to 1. Therefore, $P(A \vert B) = P(A \cap B) / P(B)$.
    <ul>
      <li>The probability is updated when new evidence is observed. We remove outcomes that contradict the observed evidence and renormalize among the remaining outcomes to preserve their relative masses.</li>
    </ul>
  </li>
  <li>
<em>Frequentist Interpretation</em>: Frequentists interpret probability as the relative frequency over many repeated trials. The conditional probability $P(A \vert B)$ can be thought of as the fraction of times that $A$ occurs, restricting attention to the trials where $B$ occurs.
    <ul>
      <li>Of the trials that satisfy $B$, what fraction of them also satisfy $A$?</li>
    </ul>
  </li>
  <li>
<em>Example</em>: “Mr. Jones has two children. The older child is a girl. What is the probability that both children are girls?” and “Mr. Smith has two children. At least one of them is a girl. What is the probability that both children are girls?”
    <ul>
      <li>
<em>Assumptions</em>
        <ul>
          <li>Gender is binary, boy or girl.</li>
          <li>$P(\text{boy}) = P(\text{girl})$ for older child and younger child.</li>
          <li>Genders of the two children are independent.</li>
        </ul>
      </li>
      <li>Jones: $P(\text{both girls} \vert \text{older is girl}) = \frac{P(\text{both girls, older is girl})}{P(\text{older is girl})} = \frac{1/4}{1/2} = \frac{1}{2}$
        <ul>
          <li>The probability that both children are girls, given the older child is a girl.</li>
        </ul>
      </li>
      <li>Smith: $P(\text{both girls} \vert \text{at least one girl}) = \frac{P(\text{both girls, at least one girl})}{P(\text{at least one girl})} = \frac{1/4}{3/4} = \frac{1}{3}$
        <ul>
          <li>The probability that both children are girls, given that at least one of them is a girl.</li>
        </ul>
      </li>
      <li>Why is knowing the older child’s gender different from knowing at least one child’s gender?
        <ul>
          <li>If we know the older child is a girl, the probability of both girls depends on only if the younger child is a girl, or ${GB, GG}$. Conditioning on a specific child removes 2 of the 4 outcomes ${BG, BB}$ in the sample space.</li>
          <li>If we know at least one child is a girl (non-specific), the probability of both girls depends on how many children are girls, or ${GB, BG, GG}$. Conditioning on at least one child being a girl knocks away only 1 of the 4 outcomes ${BB}$ in the sample space.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<em>Example</em>: A family has two children, and you randomly learn one of the two is a girl. What is the conditional probability that both are girls?
    <ul>
      <li>
<em>Assumptions</em>
        <ul>
          <li>Gender is binary, boy or girl.</li>
          <li>$P(\text{boy}) = P(\text{girl})$ for elder child and younger child.</li>
          <li>Genders of the two children are independent.</li>
          <li>You are equally likely to learn about either child.</li>
          <li>The gender of the child does not affect if you learn about them.</li>
        </ul>
      </li>
      <li>
<em>Intuition</em>: The chance of both girls just depends on the gender of the child you didn’t learn about. Since you learned about one child, both children being girls would mean the one you don’t know is a girl, which has nothing to do with the fact the child you learned about is a girl. Because you haven’t met the other child, the chance they are either boy or girl is $P(\text{other child is girl}) = 1/2$.</li>
      <li>
<em>Solution</em>
        <ul>
          <li>Let $G_{1}, G_{2}, G_{3}$ be the events that the elder, younger, and random child is a girl, respectively.</li>
          <li>By assumption, $P(G_{1}) = P(G_{2}) = P(G_{3}) = 1/2$.</li>
          <li>By the naive definition of probability, $P(G_{1}) \cap P(G_{2}) = 1/4$.</li>
          <li>$P(G_{1} \cap G_{2} \vert G_{3}) = \frac{P(G_{1} \cap G_{2} \cap G_{3})}{P(G_{3})} = \frac{1/4}{1/2} = 1/2$
            <ul>
              <li>The probability that both children are girls, given the random child is a girl.</li>
              <li>If both elder and younger children are girls, the random child must be a girl, so $G_{1} \cap G_{2} \cap G_{3} = G_{1} \cap G_{2}$.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
<em>Note</em>: The assumption of a <em>random sample</em> was needed to determine how the random child was selected. If there was some factor of selection bias, e.g. a law that forbids a boy from leaving the house if he has a sister, then “the random child is a girl” is equivalent to “at least one of the children is a girl”.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A family has two children. Given that at least one of the two is a girl who was born in the winter, find the probability that both children are girls.
    <ul>
      <li>
<em>Assumptions</em>
        <ul>
          <li>Gender is binary, boy or girl.</li>
          <li>$P(\text{boy}) = P(\text{girl})$ for elder child and younger child.</li>
          <li>Genders of the two children are independent.</li>
          <li>The four seasons are equally likely.</li>
          <li>Gender of the child is independent of the birth season.</li>
        </ul>
      </li>
      <li>$P(\text{both girls} \vert \text{at least one winter girl}) = \frac{P(\text{both girls, at least one winter girl})}{P(\text{at least one winter girl})}$</li>
      <li>$P(\text{at least one winter girl}) = 1 - P(\text{no winter girls}) = 1 - (\frac{7}{8})^{2}$
        <ul>
          <li>We take the complement, or the probability that there are no winter girls among the two children.</li>
        </ul>
      </li>
      <li>$P(\text{both girls, at least one winter girl}) = P(\text{both girls, at least one winter child}) = (1/4)P(\text{at least one winter child}) = (1/4)(1 - P(\text{no winter child})) = (1/4)(1 - (3/4)^{2})$
        <ul>
          <li>We first notice that “at least one winter girl” is equivalent to “at least one winter child” if “both girls” is true.</li>
          <li>From the assumptions that gender and season are independent, we can multiply the individual probabilities (Axiom 2) of gender (both girls) and season (at least one winter child).
            <ul>
              <li>$P(\text{both girls}) = 1/4$</li>
              <li>$P(\text{at least one winter child}) = 1 - P(\text{no winter children}) = 1 - (3/4)^{2}$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Therefore, $P(\text{both girls} \vert \text{at least one winter girl}) = \frac{(1/4)(1 - (3/4)^{2})}{1 - (7/8)^{2}} = \frac{7/64}{15/64} = 7/15$.</li>
      <li>How can conditioning on “at least one winter girl” be $7/15$ when conditioning on “at least one girl” is only $1/3$? Why does knowing the birth season increase the likelihood of “both girls”?
        <ul>
          <li>Information about birth season brings “at least one is a girl” closer to “a specific child is a girl”, e.g. conditioning on “older is a girl” is $1/2$.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Bayes’ Rule and The Law of Total Probability</strong></p>
<ul>
  <li>
<strong>Probability of the Intersection of Two Events</strong>: For any events $A, B$ with positive probabilities, $P(A \cap B) = P(B) P(A \vert B) = P(A) P(B \vert A)$.
    <ul>
      <li>The probability of both events happening is the same as the probability of one event happening, given the other.</li>
      <li>
<em>Proof</em>: Multiply $P(A \vert B) = \frac{P(A \cap B)}{P(B)}$ by $P(B)$ on both sides. Multiply $P(B \vert A) = \frac{P(B \cap A)}{P(A)}$ by $P(A)$ on both sides.</li>
    </ul>
  </li>
  <li>
<strong>Probability of the Intersection of N Events</strong>: For any events $A_{1}, A_{2}, \dots, A_{n} \text{ with } P(A_{1}, A_{2}, \dots, A_{n}) &gt; 0$, $P(A_{1}, A_{2}, \dots, A_{n}) = P(A_{1}) P(A_{2} \vert A_{1}) P(A_{3} \vert A_{1}, A_{2}) \dots P(A_{n} \vert A_{1}, A_{2}, \dots, A_{n-1})$.
    <ul>
      <li>This generalized form of the probability of intersections is $n!$ theorems in one, since we can permute $A_{1}, A_{2}, \dots, A_{n}$ without affecting the left hand side. Some permutations may be easier to calculate than others.</li>
      <li>
<em>Example</em>: $P(A_{1}, A_{2}, A_{3}) = P(A_{1}) P(A_{2} \vert A_{1}) P(A_{3} \vert A_{1}, A_{2}) = P(A_{2}) P(A_{3} \vert A_{2}) P(A_{1} \vert A_{2}, A_{3}) = \dots$.</li>
    </ul>
  </li>
  <li>
<strong>Bayes’ Rule</strong>: From the probability of the intersection of two events, we derive $P(A \vert B) = \frac{P(B \vert A) P(A)}{P(B)}$.
    <ul>
      <li>Bayes’ rule is often used when $P(B \vert A)$ is much easier to find directly than $P(A \vert B)$ or vice versa.</li>
      <li>Another way to define Bayes’ rule is in terms of odds, rather than probability.</li>
    </ul>
  </li>
  <li>
<strong>Odds</strong>: The odds of an event $A$ are $\text{odds}(A) = P(A) / P(A^{C})$.
    <ul>
      <li>
<em>Example</em>: If $P(A) = 2/3$, the odds in favor of $A$ are 2 to 1.</li>
      <li>To convert from odds back to probability, $P(A) = \text{odds}(A) / (1 + \text{odds}(A))$</li>
    </ul>
  </li>
  <li>
<strong>Odds form of Bayes’ Rule</strong>: For any events $A,B$ with positive probabilities, the odds of $A$ after conditioning on $B$ are $\frac{P(A \vert B)}{P(A^{C} \vert B)} = \frac{P(B \vert A)}{P(B \vert A^{C})} \frac{P(A)}{P(A^{C})}$.
    <ul>
      <li>
<em>Proof</em>: We divide $P(A \vert B) = \frac{P(B \vert A) P(A)}{P(B)}$ by $P(A^{C} \vert B) = \frac{P(B \vert A^{C}) P(A^{C})}{P(B)}$.</li>
      <li>The <em>posterior odds</em> $\frac{P(A \vert B)}{P(A^{C} \vert B)}$ are equal to the <em>prior odds</em> $\frac{P(A)}{P(A^{C})}$ times the <em>likelihood ratio</em> $\frac{P(B \vert A)}{P(B \vert A^{C})}$.</li>
    </ul>
  </li>
  <li>
<strong>Law of Total Probability</strong>: Let $A_{1}, \dots, A_{n}$ be a <em>partition</em> of the sample space $S$ (i.e. the $A_{i}$ are disjoint events whose union is $S$), with $P(A_{i}) &gt; 0$ for all $i$. Then $P(B) = \sum_{i=1}^{n} P(B \vert A_{i}) P(A_{i})$.
    <ul>
      <li>
<em>Proof</em>
        <ul>
          <li>Since the $A_{i}$ form of a <em>partition</em> of $S$, we can decompose $B$ as $B = (B \cap A_{1}) \cup (B \cap A_{2}) \cup \dots \cup (B \cap A_{n})$.</li>
          <li>From the <em>second axiom</em>, we can add up these disjoint probabilities to get $P(B) = P(B \cap A_{1}) + P(B \cap A_{2}) + \dots + P(B \cap A_{n})$.</li>
          <li>Then we can apply the <em>probability of the intersection of two events</em> to each term to get $P(B) = P(B \vert A_{1}) P(A_{1}) + \dots + P(B \vert A_{n}) P(A_{n})$.</li>
        </ul>
      </li>
      <li>The <em>law of total probability</em> relates conditional probability to unconditional probability. It tells us that to get the unconditional probability of $B$, we can divide the sample space into disjoint slices $A_{i}$, find the conditional probability of $B$ within each of the slices, then take the weighted sum of the conditional probabilities, where the weights are the probabilities $P(A_{i})$.
        <ul>
          <li>Choosing a partition carefully is important in reducing a complicated problem into simpler pieces.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
<em>Example</em>: Given a fair coin and a biased coin with $P(H) = 3/4$, you pick one of the coins at random and flip it three times. It lands on Heads all three times. Given this information, what is the probability that the coin you picked is the fair one?
    <ul>
      <li>Let $A$ be the event that the chosen coin lands Heads three times and let $F$ be the event that we picked the fair coin. We’re interested in $P(F \vert A)$, but it’s easier to find $P(A \vert F) \text{ and } P(A \vert F^{C})$ since it helps to know which coin we have.</li>
      <li>We use Bayes’ Rule and the Law of Total Probability.
        <ul>
          <li>$P(F \vert A) = \frac{P(A \vert F) P(F)}{P(A)}$</li>
          <li>$P(F \vert A) = \frac{P(A \vert F) P(F)}{P(A \vert F) P(F) + P(A \vert F^{C}) P(F^{C})}$</li>
          <li>$P(F \vert A) = \frac{(1/2)^{3} (1/2)}{(1/2)^{3} (1/2) + (3/4)^{3} (1/2)} \approx 0.23$</li>
        </ul>
      </li>
      <li>Before flipping the coin, choosing a coin at random was equally likely, $P(F) = P(F^{C}) = 1/2$. After observing three Heads, we see that it was more likely that we chose the biased coin over the fair coin, so $P(F \vert A) \approx 0.23$.</li>
      <li>
<em>Note</em>: It is <em>not correct</em> to say “$P(A) = 1$ because we know $A$ happened.” It is true that $P(A \vert A) = 1$, but $P(A)$ is the <em>prior</em> probability of $A$ and $P(F)$ is the <em>prior</em> probability of $F$, which are probabilities before observing the experiment. These should not be confused with <em>posterior</em> probabilities conditional on evidence $A$.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: A patient tests positive for a disease that affects 1% of the population. Let $D$ be the event that the patient has the disease and let $T$ be the event that the patient tests positive. If the test is 95% accurate, the <strong>sensitivity</strong> or <strong>true positive rate</strong> of the test is $P(T \vert D) = 0.95$, and the <strong>specificity</strong> or the <strong>true negative rate</strong> is $P(T^{C} \vert D^{C}) = 0.95$. Find the conditional probability the patient has the disease, given the evidence of the test result.
    <ul>
      <li>We use Bayes’ Rule and the Law of Total Probability.
        <ul>
          <li>$P(D \vert T) = \frac{P(T \vert D) P(D)}{P(T)}$</li>
          <li>$P(D \vert T) = \frac{P(T \vert D) P(D)}{P(T \vert D)P(D) + P(T \vert D^{C})P(D^{C})}$</li>
          <li>$P(D \vert T) = \frac{(0.95)(0.01)}{(0.95)(0.01) + (0.05)(0.99)} \approx 0.16$</li>
        </ul>
      </li>
      <li>There is a 16% chance the patient has the disease, given a positive test result, despite the high test accuracy. How is the <em>posterior</em> probability only 16%?</li>
      <li>
<em>Intuition</em>: The evidence from the test and our <em>prior information</em> about the prevalence of the disease affect the <em>posterior</em> probability. Although the test is accurate, the disease is also very rare! The conditional probability $P(D \vert T)$ is a balance between the two factors, weighing the rarity of the disease against the rarity of a mistaken test result.</li>
      <li>
<em>Intuition</em>: Consider a population of 10000 people, where 100 people (1%) have the disease and 9900 people don’t. If we tested everybody in the population, we would expect that, of the 100 diseased people, 95 would test positive and 5 would test negative (false negatives). Out of the 9900 healthy individuals, we would expect 9405 (95%) to test negative and 495 (5%) to test positive (false positives). Of the people who test positive, 95 people are true positives, and 495 people are false positives. Most of the people who test positive don’t actually have the disease, so that is why despite an accurate test, the <em>posterior</em> probability of having the disease given a positive test result is $95/(95 + 495) \approx 0.16$.</li>
    </ul>
  </li>
  <li>
<em>Example</em>: The perpetrator of a crime is one of $n$ men. Initially, all $n$ men are equally likely to be the perpetrator. An eyewitness then reports that the crime was committed by a man with six fingers on his right hand.
    <ul>
      <li>Let $p_{0}$ be the probability that an innocent man has six fingers on his right hand, and $p_{1}$ be the probability that the perpetrator has six fingers on his right hand, with $p_{0} &lt; p_{1} &lt; 1$ since witnesses aren’t 100% reliable. Let $a = p_{0}/p_{1}$ and $b = (1 - p_{1})/(1 - p_{0})$.</li>
      <li>A possible suspect Gary is found with six fingers on his right hand. What is the probability that Gary is the perpetrator?
        <ul>
          <li>Let $G$ be the event that Gary is guilty and $H$ be the event that the perpetrator has six fingers on his right hand.</li>
          <li>We use Bayes’ Rule and the Law of Total Probability.
            <ul>
              <li>$P(G \vert H) = \frac{P(H \vert G) P(G)}{P(H \vert G)P(G) + P(H \vert G^{C})P(G^{C})}$</li>
              <li>$P(G \vert H) = \frac{(p_{1})(\frac{1}{n})}{(p_{1})(\frac{1}{n}) + (p_{0})(1 - \frac{1}{n})} \times (\frac{(n / p_{1})}{(n / p_{1})})$</li>
              <li>$P(G \vert H) = \frac{1}{1 + a(n - 1)}$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>All $n$ men get their right hands checked, and Gary is the only one with six fingers on his right hand. What is the probability that Gary is the perpetrator?
        <ul>
          <li>Let $N$ be the event that none of the other men have six fingers on their right hands.</li>
          <li>We use Bayes’ Rule and the Law of Total Probability.
            <ul>
              <li>$P(G \vert H, N) = \frac{P(H, N \vert G) P(G)}{P(H, N \vert G)P(G) + P(H, N \vert G^{C})P(G^{C})}$
                <ul>
                  <li>$P(H, N \vert G) = p_{1}(1 - p_{0})^{n-1}$ is the probability that the perpetrator has six fingers and none of the other men have six fingers, given Gary is guilty. This means</li>
                </ul>
              </li>
              <li>$P(G \vert H, N) = \frac{p_{1}(1 - p_{0})^{n-1}(\frac{1}{n})}{p_{1}(1 - p_{0})^{n-1}(\frac{1}{n}) + p_{0}(1 - p_{1})(1 - p_{0})^{n-2}(1 - \frac{1}{n})}$</li>
              <li>$P(G \vert H, N) = \frac{p_{1} (\frac{1}{n})}{p_{1} (\frac{1}{n}) + p_{0}(\frac{1 - p_{1}}{1 - p_{0}})(1 - \frac{1}{n})} \times (\frac{(n / p_{1})}{(n / p_{1})})$</li>
              <li>$P(G \vert H, N) = \frac{1}{1 + ab(n - 1)}$</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><strong>Conditional Probabilities are Probabilities</strong></p>
<ul>
  <li>Conditional probabilities satisfy all the properties of probability. The only difference is we condition on an event and limit ourselves to a new universe where that event occurs.
    <ul>
      <li>Conditional probabilities are between 0 and 1.</li>
      <li>$P(S \vert E) = 1, P(\emptyset \vert E) = 0$</li>
      <li>If $A_{1}, A_{2}, \dots$ are disjoint, then $P(\bigcup_{j=1}^{\infty} A_{j} \vert E) = \sum_{j=1}^{\infty} P(A_{j} \vert E)$</li>
      <li>$P(A^{C} \vert E) = 1 - P(A \vert E)$</li>
      <li>Inclusion-Exclusion: $P(A \cup B \vert E) = P(A \vert E) + P(B \vert E) - P(A \cap B \vert E)$</li>
    </ul>
  </li>
  <li>
<em>Proof</em>: For an event $E,  P(E) &gt; 0$, define $\tilde{P}(A) = P(A \vert E)$.
    <ul>
      <li>$\tilde{P}(\emptyset) = P(\emptyset \vert E) = \frac{P(\emptyset \cap E)}{P(E)} = 0$</li>
      <li>$\tilde{P}(S) = P(S \vert E) = \frac{P(S \cap E)}{P(E)} = 1$</li>
      <li>If $A_{1}, A_{2}, \dots$ are disjoint events, then<br>
  $\tilde{P}(A_{1} \cup A_{2} \cup \dots) = \frac{P((A_{1} \cap E) \cup (A_{2} \cap E) \cup \dots)}{P(E)} = \frac{\sum_{j = 1}^{\infty} P(A_{j} \cap E)}{P(E)} = \sum_{j = 1}^{\infty} \tilde{P}(A_{j})$</li>
    </ul>
  </li>
  <li>Conversely, all probabilities can be thought of as conditional probabilities, conditioning on some implicit background information.
    <ul>
      <li>For the prior probability of rain today $P(R)$, we are naturally basing this probability on information about days or locations that had rain occur. Though people may come up with different prior probabilities, everyone can agree on how to update probabilities given new evidence.</li>
      <li>We can now think of all probabilities as $P(A) = P(A \vert \text{background knowledge})$</li>
    </ul>
  </li>
  <li><em>Conditional probabilities are probabilities, and all probabilities are conditional.</em></li>
</ul>

<p><strong>Independence of Events</strong></p>

<p><strong>Coherency of Bayes’ Rule</strong></p>

<p><strong>Conditioning as a Problem-Solving Tool</strong></p>

<p><strong>Pitfalls and Paradoxes</strong></p>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

  </div><a class="u-url" href="/notes/probability-book" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes and thoughts from a lifelong student.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nhtsai" target="_blank" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/nhtsai" target="_blank" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
