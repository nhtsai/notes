{
  
    
  
    
        "post1": {
            "title": "System Design Overview",
            "content": "System Design . Scalability . Scalability for Dummies | A Word on Scalability | . Clones . Load balancers evenly distribute user requests to public web servers. | Rule #1: Every server contains exactly the same codebase and does not store any user-related data, like sessions or profile pictures, on local disc or memory. | Sessions need to stored in a centralized data store that is accessible by all application servers. The data store can be an external database or an external persistent cache (e.g. Redis), which will have better performance than an external database. | External means data store is somewhere in or near the data center of application servers, does not reside on application servers themselves. | . | Deployment ensures that a code change is sent to all servers without an outdated server still serving old code. | Clones are instances of an machine image based upon a “super-clone” that is created from one of your servers. Just do an initial deployment of your latest code to a new clone and everything is ready. | . | . Databases . Using cloning, you can now horizontally scale across multiple servers to handle lots of requests. | But one day, your application slows and breaks due to the MySQL database. | Path 1: Keep MySQL Apply active-passive replication strategy on the database. | Upgrade server components like RAM. | Consider sharding, denormalization, SQL tuning, etc. | Eventually the upkeep will become too expensive. | . | Path 2: NoSQL Denormalize right from the beginning. | Remove joins from any database query. | Use MySQL as a NoSQL database or switch to MongoDB. | If database requests still get too slow, consider a cache. | . | . Cache . An in-memory cache (e.g. Memcached, Redis) is a simple key-value store that resides between the application and data storage. | The application should try to read from the cache first before hitting the database because the cache is lightning fast. The cache holds every dataset in RAM and handles requests as fast as possible. | . | Cached Database Queries Whenevery a query on the database is run, store the result dataset in a cache. | Use a hashed version of query as cache key. | Expiration is a large issue. All cached results that include piece a data needed to be deleted when that piece of data changes. | . | . | Cached Objects See the data as an object or class. Let the class assemble the dataset from the database. | Store the complete instance of the class or assembled dataset into the cache. | . | Rather than storing results of multiple queries, we can aggregate the results as data for a class instance and store the instance in the cache. If this ID is not present in the cache, load the data from DB, translate it and return to the client. You can cache this result by translating this data again, into the rawdata your cache has, and put it into the cache. | This makes it easy to delete the object when a piece of data changes. | This makes asynchronous processing possible. Servers query the database to assemble the data in the class. | The application just serves the latest cached object and never touches the database. | . | Example Objects: user sessions, fully rendered blog articles, activity streams, user-friend relationships The blog object has multiple methods that query the database for data. | Instead of caching the result of these separate database calls, cache the entire blog object. | When something changes, the blog object queries the database for updated data. | The application only has to serve the latest cached blog post instead of querying the database. | . | . | . Asynchronism . Asynchronously doing work in advance and serving finished work with low request time. Used to turn dynamic content into static content. Think pre-rendering pages into static HTML files to be served quicker. | The rendering can be scripted to run every hour by a cronjob. | This pre-computing process helps make web applications scalable and performant | . | . | Referring unforseen, immediate requests to an asynchronous service. Upon receiving a computing intensive task, the job is added to the job queue. | The job is then sent to an asynchronous server to be processed in the background. | The results are returned once the server is done processing. | . | Basic idea: Have a queue of tasks or jobs that a worker can process. Backends become scalable and frontends become responsive. | Tools to implement async processing: RabbitMQ | . | . Performance vs Scalability . A service is scalable if it results in increased performance in a manner proportional to resources added. E.g. adding another server to handle requests speeds up the website response time | . | If the system is slow for a single user, there is a performance problem. | If the system is fast for a single user but slow under heavy load, there is a scalability problem. . | An always-on service is scalable if adding resources to facilitate redundancy does not result in loss of performance. E.g. adding multiple copies of a database for redundancy does not decrease the query response time | . | Scalability requires applications be designed with scaling in mind. | Scalability also has to handle heterogeneity. As new resources (hardware, software) come online, some nodes will be able to process faster or store more data than other nodes in a system. | . | . Latency vs Throughput . Latency is the time to perform some action or to produce some result. | Throughput is the number of such actions or results per unit of time. | Generally, you should aim for maximal throughup with acceptable latency. | . Availability vs Consistency . CAP Theorem . * In a distributed system, you can only support 2 guarantees: * **Consistency**: every read receives most recent write or an error * **Availability**: every request receives a response, without guarantee that it contains the most recent version of the data * **Partition Tolerance**: system continues to operate despite arbitrary partitioning due to network failures, e.g. a server crashes or goes offline * Networks aren&#39;t reliable, so *partition tolerance needs to be supported*. * You need to make a *software tradeoff between consistency and availability*. . Consistency and Partition Tolerance (CP) . * System is consistent across servers and can handle network failures, but responses to requests are not always available. * Waiting for a response from the partitioned node might result in a timeout error. * Good choice if **atomic reads and writes** are required. * **Atomic** refers to performing operations one at a time. . Availability and Partition Tolerance (AP) . * System is always available and can handle network failures, but the data is not always consistent or up to date across nodes. * Responses return the most readily available version of the data on any node, which might be outdated. * Writes might take some time to propagate when the partition/failure is resolved. * Good choice if **eventual consistency** is needed or when the system needs to continue working despite external errors. . Consistency Patterns . With multiple copies of the same data (redundancy), how do we synchronize them across nodes (consistency) to provide all users the same view of the data? The CAP Theorem need to respond to every read with the most recent write or an error to be consistent. | . | . Weak Consistency . * After a write, reads *may or may not* see it, and a best effort approach is taken. * Weak consistency works well for real-time use cases, such as VoIP, video chat, and realtime multiplayer video games. * If you briefly lose reception during a phone call, you don&#39;t really care or hear what was lost during connection loss. * Weak consistency is used in systems like memcached, where the result might or might not be there. . Eventual Consistency . * After a write, reads *will eventually* see it, typically within milliseconds. * Data is **replicated asynchronously**. * Eventual consistency works well in highly available systems. * Eventual consistency is used in systems like DNS and email. . Strong Consistency . * After a write, reads *will* see it. * Data is **replicated synchronously**. * Strong Consistency works well in systems that need transactions. * Strong consistency is used in systems like file systems and relational database management systems (RDBMSes). . Transactions . * An extended form of consistency across multiple operations. * E.g. transfering money from account A to account B * Operation 1: subtract from A * Operation 2: add to B * What if something happens in between operations? * E.g. Another transaction A or B, machine crashes * You want some kind of guarantee that the invariants will be maintained. * Money subtracted from A will go back to A. * Money created will eventually be added to B. * Transactions are useful because... * Correctness * Consistency * Enforce invariants * ACID: atomic, consistent, isolated, durable . Availability Patterns . Fail-Over . * Active-Passive * Heartbeats are sent between the active server and the passive server on standby. Only the active server handles traffic. * If a heartbeat is interrupted, the passive server takes over the active server&#39;s IP address and resumes service to maintain availability. * Downtime duration is determined by whether passive servier is already running in &#39;hot&#39; standby or starting from &#39;cold&#39; standby. * Active-Active * Both servers are managing traffic, spreading load between them * If servers are public-facing, DNS needs to know about public IPs of both servers. * If servers are private-facing, application logic needs to know about both servers. * Disadvantages * More hardware and additional complexity * Potential for loss of data if active system fails before any newly written data can be replicated to the passive . Replication . * Master-Slave * One master node handles all writes, which are then replicated onto multiple slave nodes. * Master-Master * Both master nodes handle all write requests, spreading load between them. The changes are then replicated onto multiple slave nodes. . Availability in Numbers . * **Uptime** or **downtime** is the percentage of time the service is available/not available. * Availability is generally measured in 9s, by which a service with 99.99% availability is described as having &quot;four 9s&quot;. | Acceptable Downtime Duration | 99.9% Availability | 99.99% Availability | | -- | : | : | | Downtime per year | 8h 45m 57.0s | 52m 35.7s | | Downtime per month | 43m 49.7s | 4m 23.0s | | Downtime per week | 10m 04.8s | 1m 05.0s | | Downtime per day | 1m 26.4s | 08.6s | . Availability in Sequence . * Overall availability *decreases* when two components with &lt; 100% availability are in **sequence**. * $$ text{Availability}( text{Total}) = text{Availability}( text{Foo}) * text{Availabiilty}( text{Bar})$$ * If both Foo and Bar have 99.9% availability each, their total availability in sequence would be 99.8%. . Availability in Parallel . * Overall availability *increases* when two componenets with &lt; 100% availabiilty are in **parallel**. * $$ text{Availability}( text{Total}) = 1 - (1 - text{Availability}( text{Foo})) * (1 - text{Availabiilty}( text{Bar}))$$ * If both Foo and Bar have 99.9% availability each, their total availability in parallel would be 99.9999%. . Domain Name System (DNS) . A Domain Name System (DNS) translates a domain name, e.g. www.example.com, to an IP address, e.g. 8.8.8.8. DNS is hierarchical, with a few authoritative servers at the top level. | Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. | Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. | DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL). | . | A Name Server (NS) Record specifieds the DNS servers for your domain/subdomain. | A Mail Exchange (MX) Record specifies the mail servers for accepting messages. | An Address (A) Record points a name to an IP address. | A Canonical Name (CNAME) points a name to another name or to an A Record, e.g. pointing example.com to www.example.com. | Services that provide managed DNS services include: CloudFlare, Route 53, etc. | . DNS Traffic Routing Methods . * **Round Robin** * Pairs an incoming request to a specific machine by circling through a list of servers capaable of handling the request * May not result in a perfectly-balanced load distribution * **Weighted Round Robin** * Each server machine is assigned a performance value, or weight, relative to the other servers in the pool, usually in an automated benchmark testing. * This weight determines how many more or fewer requests are sent to that server, compared to other servers in the pool. * The result is a more even or equal load distribution. * Prevents traffic from going to servers under maintenance. * Weights can help load balance between varying cluster sizes. * A/B Testing * **Latency-Based** * Create latency records between servers in multiple regions. * When a request arrives, the DNS queries the NS, which looks at the most recent latency data. * The load balancer with the lowest latency is the one chosen to serve the user. * **Geolocation-Based**: * Choosing servers to serve traffic based on the geographic location of users. * E.g. routing all European traffic to a European load balancer * Can localize content and restrict content distribution based on region. * Can load balance predicatably so each user location is consistently routed to the same endpoint. . Disadvantages of DNS . * Accessing a DNS server introduces a slight delay, which can be mitigated by caching. * DNS server management is complex and generally managed by governments, ISPs, and large companies. * DNS services are susceptible to **Distributed Denial of Service (DDoS)** attacks, which prevent users from accessing websites without knowing Twitter&#39;s IP address(es). . Content Delivery Network (CDN) . A Content Delivery Network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files (e.g. HTML, CSS, JS, photos, videos) are served from CDNs. Some CDNs like AWS CloudFront supports serving dynamic content. | . | The website’s DNS resolution tells clients which CDN server to contact. | . | Advantages Improved performance because users receive content from data centers close to them. | Reduced load because your servers do not have to serve requests that the CDN fulfills. | . | . Push CDNs . * Receive new content whenever changes occur on your server. * Content is only uploaded when it is new or changed, minimizing traffic but maximizing storage. * You take full responsibility for providing content, uploading directly to the CDN, and rewriting URLs to point to the CDN. * You can configure when content expires and when it is updated using TTLs. * Push CDNs work well for sites with small amounts of traffic or content that isn&#39;t often updated. * Content is pushed to the CDNs when needed, instead of being re-pulled at regular intervals. * For lots of updates, pushing content to the Push CDN places load on the server. * For heavy traffic, the Push CDN&#39;s cached content may not be sufficient and will place more load on the server to push content to the Push CDN. . Pull CDNs . * Grabs new content from your server when the first user requests the content. * You take full responsibility for providing content and rewriting URLs to point to the CDN. * This results in slower requests until content is cached on the CDN, as users need to pull from the server upon the first request. * A **time to live (TTL)** determines the life of the cached content, which you do not typically have control of. * Pull CDNs minimizing storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed. * Pull CDNs work well for sites with heavy traffic because the traffic spread out more evenly with only recently-requested content remaining on the CDN. * Older requested content is expired by the TTL, making space for new content. * For lots of updates, the Pull CDN is able to pull and cache the updated content when requested or old content is expired. * For heavy traffic, the Pull CDN can serve the most requested, cached content, only pulling from the server when for less requested content. . Disadvantages of CDNs . * CDN costs could be significant depending on traffic, although this should be compared against additional costs of not using a CDN. * Cached content might be stale if it is updated before the TTL expires it to be updated. * CDNs require changing URLs for static content to point to the CDN, e.g. directing `facebook.com` to `cdn-images.fb.com`. . Load Balancer . | . Reverse Proxy . Advantages | | . | Disadvantages | . Load Balancer vs Reverse Proxy . Application Layer . Microservices . Service Discovery . Disadvantages . Database . SQL/Relational Database Management System (RDBMS) ACID | Master-Slave Replication | Master-Master Replication | Disadvantages of Replication . | Federation . | Sharding . | Denormalization . | SQL Tuning | . | NoSQL BASE | Key-Value Store | Document Store | Wide-Column Store | Graph Database | . | SQL vs NoSQL | . Cache . Client Caching . | CDN Caching . | Web Server Caching . | Database Caching Query Caching | Object Caching | . | Application Caching . | Updating Cache Cache-Aside | Write-Through | Write-Behind/Write-Back | Refresh-Ahead | . | Cache Disadvantages | . Asynchronism . Message Queues | Task Queues | Back Pressure | Aynchronism Disadvantages | . Communication . Hypertext Transfer Protocol (HTTP) | Transmission Control Protocol (TCP) | User Datagram Protocol (UDP) | Remote Procedure Call (RPC) | Representational State Transfer (REST) | RPC vs REST Calls | . Security . References .",
            "url": "https://nhtsai.github.io/notes/system-design-overview",
            "relUrl": "/system-design-overview",
            "date": " • Sep 5, 2021"
        }
        
    
  
    
  
    
  
    
  
    
        ,"post5": {
            "title": "Learning Object-Specific Distance From a Monocular Image",
            "content": "Learning Object-Specific Distance From a Monocular Image . Research paper by Jing Zhu, Yi Fang, Husam Abu-Haimed, Kuo-Chin Lien, Dongdong Fu, and Junli Gu 1. . Abstract . TODO . Introduction . In computer vision research for self-driving cars, researchers have not recognized the importance of environment perception in favor of more popular tasks, like object classification, detection, and segmentation. Object-specific distance estimation is important for car safety and collision avoidance, and the lack of deep learning applications is likely due to the lack of datasets with distance measures for each object in the scene. . Current self-driving systems predict object distance using the traditional inverse perspective mapping (IPM) algorithm. This method first locates a point on the object (usually on the lower edge of the bounding box), then projects the located point onto a bird’s-eye view coordinate map using camera parameters, and finally estimates the object distance using the constructed bird’s-eye view coordinate map. This simple method can provide reasonable distance estimates for objects close and in front of the camera, but it performs poorly when objects are located to the sides of the camera or curved roads and when objects are over 40 meters away. . The authors first sought “to develop an end-to-end learning based approach that directly predicts distances for given objects in the RGB images.” End-to-end meaning that object detection and distance estimation parameters are trained jointly. The base model extracts features from RGB images, then utilizes region of interest (ROI) pooling to generate a fixed-size feature vector for each object, and feeds the ROI feature vectors into a distance regressor to predict a distance (Z) for each object. However, this method was not sufficiently precise for self-driving. . The authors then created an enhanced model with a keypoint regressor to predict (X,Y) of the 3D keypoint coordinates (X,Y,Z). Leveraging the camera projection matrix, the authors defined a projection loss between the projected 3D point (X,Y,Z) and the ground truth keypoint (X*,Y*,Z*). The keypoint regressor and projection loss are used for training only. During inference, the trained model takes in an image with bounding boxes and outputs the object-specific distance, without any camera parameters invervention. . The authors constructed an extended dataset from the KITTI object detection dataset and the nuScenes mini dataset by “computing the distance for each object using its corresponding LiDAR point cloud and camera parameters.” . Enhanced model performs better at distance estimation, compared to the traditional IPM algorithm and the support vector regressor, is also more precise than the base model, and is twice as fast during inference than IPM algorithm. . Summary . Base model: use deep learning to predict distance from given objects on RGB image without camera parameter intervention | Enhanced model: base model with keypoint regressor and new projection loss | Dataset: extended KITTI and nuScenes mini object-specific distance datasets | . Related Work . Distance Estimation . Inverse Perspective Mapping (IPM): convert a point or a bounding box in the image to the corresponding bird’s-eye view coordinate | Support Vector Regressor: predict object-specific distance given width and height of a bounding box | DistNet: use YOLO for bounding boxes prediction instead of image features learning for distance estimation, the distance regressor studied geometric mapping from bounding box with certain width and height to distance value In contrast, this paper directly predicts distances from learned image features. | . | Marker-based Methods: use auxiliary information, create segmented markers in the image and estimate distance using the marker area and camera parameters | Calibration Patterns: predict physical distance based on rectangular pattern, where 4 image points are needed to compute camera calibration | . 2D Visual Perception . R-CNNs: boost accuracy, decrease processing time for object detection, classfiication, dsegmentation | SSD and YOLO: end-to-end frameworks to detect and classify objects in RGB images | Monocular Depth Estimation: predict dense depth maps for given monocular color images | . Methods . The authors propose a base model that predicts physical, object-specific distance from given RGB images and object bounding boxes and an enhanced model with keypoint regressor for improved distance estimation. . Base Method . . Feature Extractor . Extract feature map for entire RGB image using image feature learning network. | Use existing architecture (e.g. vgg16, resnet50) as feature extractors. | Output of last layer is max-pooled and extracted as feature map for input RGB image. | . Distance Regressor and Classifier . Feed feature map and object bounding boxes into ROI pooling layer to generate fixed-size feature vector Fi boldsymbol{F_i}Fi​ for each object in the image. | Pass pooled object feature vector into distance regressor for predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) and object classifier for predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Distance Regressor: 3 fully-connected (FC) layers, {2048, 512, 1} for vgg16 and {1024, 512, 1} for resnet50. Softplus activation layer applied on output of last FC layer to ensure predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) is positive. | . | Distance Classifier: A FC layer with (number of categories in dataset) neurons, then softmax function to get predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Loss Functions Regressor Loss: . Ldist=1N∑i=1NL1;smooth(D(Fi),di∗)L_{dist}= frac{1}{N} sum_{i=1}^{N} L_{ text{1;smooth}}(D( boldsymbol{F_i}), d^{*}_{i})Ldist​=N1​i=1∑N​L1;smooth​(D(Fi​),di∗​) L1;smooth={0.5(xi−yi)2/β,if ∣xi−yi∣&lt;β∣xi−yi∣−0.5∗β,otherwiseL_{ text{1;smooth}} = begin{cases}0.5(x_{i} - y_{i})^2/ beta, &amp; text{if $|x_{i}-y_{i}|&lt; beta$} |x_{i}-y_{i}|-0.5* beta, &amp; text{otherwise} end{cases}L1;smooth​={0.5(xi​−yi​)2/β,∣xi​−yi​∣−0.5∗β,​if ∣xi​−yi​∣&lt;βotherwise​ | Classifier Loss: . Lcla=1N∑i=1NLcross-entropy(C(Fi),yi∗)L_{cla}= frac{1}{N} sum_{i=1}^{N} L_{ text{cross-entropy}}(C( boldsymbol{F_i}), y^{*}_{i})Lcla​=N1​i=1∑N​Lcross-entropy​(C(Fi​),yi∗​) Lcross-entropy=−∑i=1Myi∗∗log(C(Fi))L_{ text{cross-entropy}}=- sum_{i=1}^{M} y^{*}_{i} * log(C( boldsymbol{F_i}))Lcross-entropy​=−i=1∑M​yi∗​∗log(C(Fi​)) | NNN: number of objects in the image | MMM: number of categories | β betaβ: smooth loss scaling factor, usually 1.01.01.0 | D(Fi)D( boldsymbol{F_i})D(Fi​): predicted distance | C(Fi)C( boldsymbol{F_i})C(Fi​): predicted category label | di∗d^{*}_{i}di∗​: ground truth distance of the iii-th object | yi∗y^{*}_{i}yi∗​: ground truth category label of the iii-th object | . | . Model Learning and Inference . Train feature extractor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}minLbase​=Lcla​+λ1​Ldist​ Set λ1=1.0 lambda_{1}=1.0λ1​=1.0 during training. | . | Use ADAM optimizer with beta value β=0.5 beta=0.5β=0.5, learning rate of 0.001, exponentially decayed after 10 epochs. | Classifier encourages model to learn features used in estimating more accurate distances, only used during training. | After training, base model can predict object-specific distances given RGB images and object bounding boxes as input. | . Enhanced Method . . Add keypoint regressor to optimize base model by introducing projection constraint for better distance prediction. . Feature Extractor . Same architecture as base model: vgg16 or resnet50 into ROI pooling layer for object specific features Fi boldsymbol{F_i}Fi​. | . Keypoint Regressor . Keypoint regressor KKK learns to predict approximate keypoint position in 3D camera coordinate system. | Distance regressor predicts Z coordinate, while keypoint regressor predicts (X, Y). | Keypoint Regressor: 3 fully-connected (FC) layers, {2048, 512, 2} for vgg16 and {1024, 512, 2} for resnet50. | Since there is no ground truth of 3D keypoint available, project the generated 3D point (X,Y,Z)=([K(Fi),D(Fi)])(X,Y,Z)=([K( boldsymbol{F_i}), D( boldsymbol{F_i})])(X,Y,Z)=([K(Fi​),D(Fi​)]) back to image plane using camera projection matrix PPP. | Compute errors between ground truth 2D keypoint ki∗k^{*}_{i}ki∗​ and projected point P⋅([K(Fi),D(Fi)])P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})])P⋅([K(Fi​),D(Fi​)]). | Keypoint Function . L3Dpoint=1N∑i=1N1di∗∥P⋅([K(Fi),D(Fi)])−ki∗∥2L_{3Dpoint}= frac{1}{N} sum_{i=1}^{N} frac{1}{d^{*}_{i}} | P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})]) - k^{*}_{i} |_{2}L3Dpoint​=N1​i=1∑N​di∗​1​∥P⋅([K(Fi​),D(Fi​)])−ki∗​∥2​ Use weight with regard to ground truth distance to encourage better predictions for closer objects. | . | . Distance Regressor and Classifier . Same architecture as base model and training losses LdistL_{dist}Ldist​ and LclaL_{cla}Lcla​. | Distance regressor parameters also optimized by projection loss L3DpointL_{3Dpoint}L3Dpoint​. | . Model Learning and Inference . Train feature extractor, keypoint regressor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist+λ2L3Dpoint text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}+ lambda_{2}L_{3Dpoint}minLbase​=Lcla​+λ1​Ldist​+λ2​L3Dpoint​ Distance loss weight constant: lambda1=10.0lambda_{1}=10.0lambda1​=10.0 | Keypoint loss weight constant: lambda2=0.05lambda_{2}=0.05lambda2​=0.05 | . | Use same optimizer, beta, and learning rate as the base model. | Training Only: use camera projection matrix PPP, keypoint regressor, and object classifier. | Testing: Given RGB image and bounding boxes, directly predicts object-specific distances without any camera parameter intervention. | Both models trained for 20 epochs with batch size of 1 on the training subset, augmented with horizontally-flipped training images. | After training, input RGB image with bounding boxes into trained model to get the output of the distance regressor as the estimated object-specific distance in the validation subset. | . Dataset Construction . KITTI and nuScenes mini both provide RGB images, 2D/3D bounding boxes, category labels for objects in the images, and the corresponding velodyne point cloud for each image. . Object Distance Ground Truth Generation . Use 3D bounding boxes to segment velodyne point cloud for each object. | Sort the segmented points based on depth values. | Extract the n-th depth value as object-specific ground truth distance, where n=0.1∗(number of segmented points)n=0.1*( text{number of segmented points})n=0.1∗(number of segmented points) to avoid extracing depth values from noise points. | Project velodyne points to corresponding RGB image planes and get their image coordinates as keypoint ground truth distance. | Append both ground truths to the object detection dataset labels. | KITTI . Split KITTI into training (3,712 RGB images, 23,841 objects) and validation sets (3,768 RGB images, 25,052 objects) using 1:1 split ratio. | All KITTI objects are categorized into 9 classes, i.e. Car, Cyclist, Pedestrian, Misc, Person_sitting, Tram, Truck, Van, DontCare. | Generated KITTI ground truth distances should be between [0, 80] meters. | . nuScenes mini . Split nuScenes mini into training (200 images, 1,549 objects) and validation sets (199 images, 1,457 objects). | All nuScenes mini objects are categorized into 8 classes, i.e. Car, Bicycle, Pedestrian, Motorcycle, Bus, Trailer, Truck, Construction_vehicle. | Generated nuScenes mini ground truth distances should be between [2, 105] meters. | . Evaluation . Evaluation Metrics . Use the same metrics as depth prediction. Let di∗d^{*}_{i}di∗​ and did_{i}di​ denote the ground truth distance and the predicted distance, respectively. . Threshold . % of di s.t. max(di/di∗,di∗/di)=δ&lt;threshold % text{ of } d_i text{ s.t. max}(d_i/d^{*}_{i},d^{*}_{i}/d_i)= delta&lt; text{threshold}% of di​ s.t. max(di​/di∗​,di∗​/di​)=δ&lt;threshold | Absolute Relative Difference . Abs Rel=1N∑i=1N∣di−di∗∣/di∗ text{Abs Rel}= frac{1}{N} sum_{i=1}^{N}|d_{i}-d^{*}_{i}|/d^{*}_{i}Abs Rel=N1​i=1∑N​∣di​−di∗​∣/di∗​ | Squared Relative Difference . Squa Rel=1N∑i=1N∥di−di∗∥2/di∗ text{Squa Rel}= frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}/d^{*}_{i}Squa Rel=N1​i=1∑N​∥di​−di∗​∥2/di∗​ | RMSE (linear) . RMSElinear=1N∑i=1N∥di−di∗∥2 text{RMSE}_{ text{linear}}= sqrt{ frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}}RMSElinear​=N1​i=1∑N​∥di​−di∗​∥2​ | RMSE (log) . RMSElog=1N∑i=1N∥log⁡di−log⁡di∗∥2 text{RMSE}_{ text{log}}= sqrt{ frac{1}{N} sum_{i=1}^{N} | log{d_{i}}- log{d^{*}_{i}} |^{2}}RMSElog​=N1​i=1∑N​∥logdi​−logdi∗​∥2​ | . Note: don’t include distances predicted for DontCare objects when calculating errors. . Compared Approaches . Inverse Perpective Mapping Algorithm (IPM): predicts object-specific distance by approximating a transformation matrix between a normal RGB image and its bird’s-eye view image using camera parameters Use IPM from MATLAB’s computer vision toolkit to get transformation matrices for RGB images from the validation subset. | Project middle points of the lower edges of the object bounding boxes into bird’s-eye view coordinates using the transformation matrices. | Take values along forward direction as estimated distances. | . | Support Vector Regressor (SVR): predicts object-specific distance given width and height of a bounding box Compute width and height of each bounding box in the training subset. | Train SVR with the ground truth distance. | Input widths and heights of each bounding box in the validation set to get estimated object-specific distances. | . | . Results . Both proposed models have much lower relative errors and higher accuracies, compared to IPM and SVR, on KITTI. | Enhanced model performs the best, implying effectiveness of keypoint regressor and projection constraint, on both KITTI and nuScenes mini. | . References . J. Zhu, Y. Fang, H. Abu-Haimed, K. Lien, D. Fu, and J. Gu. Learning Object-Specific Distance from a Monocular Image. arXiv:1909.04182, 2019. &#8617; . |",
            "url": "https://nhtsai.github.io/notes/distance-estimation",
            "relUrl": "/distance-estimation",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://nhtsai.github.io/notes/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nhtsai.github.io/notes/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a university graduate interested in data science and machine learning. Currently looking for internship and job opportunities. .",
          "url": "https://nhtsai.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nhtsai.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}