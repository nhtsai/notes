{
  
    
        "post0": {
            "title": "SQL Overview",
            "content": "SQL Overview . Mode SQL | Select Star SQL | W3 Schools SQL | Window Functions Video | . SQL . SQL, or Structured Query Language, is a declarative language used to access and manipulate databases. | SQL can execute queries, retrieve data, insert records, update records, and delete records. | SQL can create new databases, tables, stored procedures, views. | SQL can set permissions on tables, procedures, and views. | . RDBMS . RDBMS, or Relational Database Management System, is the basis for SQL and all modern database systems (MS SQL Server, MySQL, etc.). | A database is an organized collection of data tables. | A schema is an overview of all tables in a database. | A table is a collection of related data entries, formatted in columns and rows. Every table is broken up into smaller entities called fields. | Reference a table using database.table. | . | A field is a table column (vertical entity) designed to maintain specific information about every record in the table. | A record is a table row (horizontal entity) that holds specific information about each individual entry that exists in a table. | . SQL Syntax . SQL keywords are not case-sensitive. | Semicolons separate each SQL statement, often used to execute more than one statement in the same query call. | Use -- comment for single line comments, /* comment */ for multi-line comments | . SELECT . Extracts data from a database | Use double-quotes for column names if column name is a keyword. . -- Selecting all columns and rows SELECT * FROM table_name -- Using double quotes for column name (since max is a keyword) SELECT &quot;Max&quot; FROM table_name . | . SELECT TOP/LIMIT . Specify the number of records to return | Use LIMIT for MySQL, other databases have different ones . -- SQL Server/MS Access SELECT TOP 10 col1, col2 FROM table_name -- MySQL/PostgreSQL SELECT col1, col2 FROM table_name LIMIT 10 ORDER BY col1 DESC -- use ORDER BY to get meaningful row order . | . FROM . Denotes the database and table to query . -- Selecting all rows of col1 SELECT col1 FROM table_name . | . WHERE . Filters the data retrieved from the database based on a specified condition . -- Filtering for rows where col1 is positive SELECT col1 FROM table_name WHERE col1 &gt; 0 . | . Comparison Operators Symbol . Equal To | = | . Not Equal To | &lt;&gt;, != | . Greater Than | &gt; | . Greater Than or Equal To | &gt;= | . Less Than | &lt; | . Less Than or Equal To | &lt;= | . Arithmetic Operators Symbol . Addition | + | . Subtraction | - | . Multiplication | * | . Division | / | . Use operators to compare across columns in the same row. Need aggregate functions to compare across rows. | . | . Logical Operators Symbol Example . Match similar values | LIKE, % is a multiple wildcard character, _ is an individual wildcard character, [ab] is any single character, [^ab] is any single character not in brackets, [a-b] is any single character in a range of characters | WHERE col LIKE &#39;n%&#39; | . Match similar values, case insensitive | ILIKE | WHERE col LIKE &#39;n%&#39; | . Match on list of values, or query result | IN | WHERE col IN (1, 2, 3) | . Match on a range, inclusive | BETWEEN ... AND ... | WHERE col BETWEEN 5 AND 10 | . Match null values | IS NULL | WHERE col IS NULL | . Match non-null values | IS NOT NULL | WHERE col IS NOT NULL | . Match on 2 conditions | AND | WHERE col &gt;= 1 AND col &lt;= 2 | . Match on either of 2 conditions | OR | WHERE col = 1 OR (col = 2 AND col = 3) | . Match on not a condition | NOT | WHERE col NOT LIKE &#39;%n%&#39; WHERE col NOT BETWEEN 2 AND 3 | . Match on any values meeting condition | ANY | WHERE id = ANY(SELECT id FROM users WHERE score = 10) | . Match on all values meeting condition | ALL | WHERE id != ALL(SELECT id FROM users WHERE score &lt; 90) | . Exists if subquery returns 1+ rows | EXISTS | WHERE EXISTS (SELECT id FROM users WHERE age &lt; 18) | . ORDER BY . Sorts data based on 1+ columns | Ascending sort by default, use DESC for descending sort | Can also use #s in place of column names, corresponding to (1-indexed) order of columns in SELECT . -- Ordering using column names SELECT col1, col2 FROM table_name ORDER BY col1 DESC, col2 -- Ordering using column numbers SELECT col1, col2 FROM table_name ORDER BY 1 DESC, 2 . | . Aggregate Functions . Aggregation Command Example . Gets count of all non-null values | COUNT | SELECT COUNT(col1) | . Gets sum of all values, null values are 0 | SUM | SELECT SUM(col1) | . Gets average of all values, null values are 0 | AVG | SELECT AVG(col1) | . Gets minimum of all values | MIN | SELECT MIN(col1) | . Gets maximum of all values | MAX | SELECT MAX(col1) | . AS (Aliasing) . Renames a table or column . SELECT col1 AS &quot;Sales&quot; FROM table_name AS T . | . GROUP BY . Aggregates across smaller groupings instead of the whole table | Required for non-aggregated columns when performing aggregation | Can also reference columns by 1-indexed order in SELECT | Order of column names in grouping does not matter . SELECT A, B, COUNT(C) FROM table_name GROUP BY A, B -- Equivalent query SELECT A, B, COUNT(C) FROM table_name GROUP BY 2, 1 . | . HAVING . Filters the result of aggregate columns | Like WHERE but for aggregated columns . SELECT A, MAX(B) -- max aggregation FROM table_name WHERE A &gt;= 4 -- filter before aggregation GROUP BY A -- group by non-aggregated cols HAVING MAX(B) &gt; 10 -- filter after aggregation ORDER BY 2 DESC -- descending sort by MAX(B) LIMIT 100 -- return only first 100 rows . | . CASE WHEN . Used to handle if/else logic | Always goes in the SELECT clause | Default value is NULL if no ELSE clause . SELECT CASE WHEN condition THEN value WHEN condition THEN value ELSE value -- optional, default is NULL END AS col_name FROM table_name . | Use with aggregate functions to create different aggregation groups can use CASE WHEN alias in GROUP BY or copy and paste the whole CASE WHEN | . -- vertical table example SELECT CASE -- processes all c values into 5 groups WHEN col=1 THEN &#39;Freshman&#39; WHEN col=2 THEN &#39;Sophomore&#39; WHEN col=3 THEN &#39;Junior&#39; WHEN col=4 THEN &#39;Senior&#39; ELSE &#39;None&#39; END AS grade, COUNT(1) AS count FROM table_name GROUP BY grade -- groups by grade -- horizontal table example SELECT COUNT(CASE WHEN col=1 THEN 1 ELSE NULL END) AS fr_count, COUNT(CASE WHEN col=2 THEN 1 ELSE NULL END) AS so_count, COUNT(CASE WHEN col=3 THEN 1 ELSE NULL END) AS jr_count, COUNT(CASE WHEN col=4 THEN 1 ELSE NULL END) AS sr_count FROM table_name . | . DISTINCT . Selects unique values of 1+ columns | Only need to include DISTINCT once | Slow performance, especially in aggregations . SELECT DISTINCT year, month FROM table_name -- use with aggregate SELECT COUNT(DISTINCT month) AS unique_months FROM table . | . Joins . Uses common identifiers to join related tables Rows without matching common identifiers will default to NULL | . | Use aliases for tables when joining Aliases distinguish identical column names when joining two tables | . | Can use aliases in SELECT clause, like SELECT alias.col_name | . JOIN/INNER JOIN . Joins two tables, returns only matched rows from both tables | Unmatched rows are not included . -- Keeps matched rows SELECT A.*, B.col2 FROM table1 A JOIN table2 B ON A.col=B.col . | . LEFT JOIN/LEFT OUTER JOIN . Joins two tables, returns matched rows and unmatched rows from left table. . -- Keeps all A rows and matched B rows SELECT A.*, B.col2 FROM table1 A LEFT JOIN table2 B ON A.col=B.col . | . RIGHT JOIN/RIGHT OUTER JOIN . Joins two tables, returns matched rows and unmatched rows from right table. | Usually use flip the table order and use LEFT JOIN. . -- Keeps match A rows and all B rows SELECT A.*, B.col2 FROM table1 A RIGHT JOIN table2 B ON A.col=B.col . | . FULL OUTER JOIN . Joins two tables, returns all matched and unmatched rows. | Commonly used in aggregations to find overlap between two tables . -- Keeps all matched/unmatched rows SELECT A.*, B.col2 FROM table1 A JOIN table2 B ON A.col=B.col -- Example SELECT COUNT( CASE WHEN A.a IS NOT NULL AND B.b IS NULL THEN A.a -- must use A.a otherwise NULL and COUNT will be 0 ELSE NULL END ) AS A_only, COUNT( CASE WHEN A.a IS NOT NULL AND B.b IS NOT NULL THEN A.a -- can keep either A.a or B.b ELSE NULL END ) AS both, COUNT( CASE WHEN A.a IS NULL AND B.b IS NOT NULL THEN B.b -- must use B.b otherwise NULL and COUNT will be 0 ELSE NULL END ) AS B_only FROM table1 A JOIN table2 B ON A.a = B.b . | . Joins with Conditions . Can put conditions either in the WHERE or ON clauses . -- Finds rows that fit the condition rather -- than joining all rows then filtering SELECT A.*, B.* FROM table1 A JOIN table2 B ON A.id = B.id AND A.year &gt; B.year + 5 . | Conditioning WHERE vs. ON clauses WHERE joins all rows that match, then filters the rows based on that condition May be less efficient when joining large tables | . | ON only joins rows that match the conditions listed | For INNER JOINs, the two methods effectively produce the same results because unmatched (NULL) rows are discarded. | For OUTER JOINs, this is a slight difference. | . | Example documents table | . id name . 1 | doc1 | . 2 | doc2 | . 3 | doc3 | . 4 | doc4 | . 5 | doc5 | . downloads table | . id doc_id user . 1 | 1 | sandeep | . 2 | 1 | simi | . 3 | 2 | sandeep | . 4 | 2 | reya | . 5 | 3 | simi | . | WHERE conditioning vs. ON conditioning . SELECT documents.name, downloads.id FROM documents LEFT OUTER JOIN downloads ON documents.id=downloads.doc_id WHERE user=&#39;sandeep&#39; -- WHERE condition SELECT documents.name, downloads.id FROM documents LEFT OUTER JOIN downloads ON documents.id=downloads.doc_id AND user=&#39;sandeep&#39; -- ON condition . | Intermediate JOIN table for WHERE conditioning Matches every row using key, filters after the join. | . documents.id name downloads.id doc_id user . 1 | doc1 | 1 | 1 | sandeep | . 1 | doc1 | 2 | 1 | simi | . 2 | doc2 | 3 | 2 | sandeep | . 2 | doc2 | 4 | 2 | reya | . 3 | doc3 | 5 | 3 | simi | . 4 | doc4 | NULL | NULL | NULL | . 5 | doc5 | NULL | NULL | NULL | . | Intermediate JOIN table for ON conditioning Does not match rows that are not user=&#39;sandeep&#39;, filters one table before the join. | . documents.id name downloads.id doc_id user . 1 | doc1 | 1 | 1 | sandeep | . 2 | doc2 | 3 | 2 | sandeep | . 3 | doc3 | NULL | NULL | NULL | . 4 | doc4 | NULL | NULL | NULL | . 5 | doc5 | NULL | NULL | NULL | . | Result of WHERE conditioning . name downloads.id . doc1 | 1 | . doc2 | 3 | . | Result of ON conditioning . name downloads.id . doc1 | 1 | . doc2 | 3 | . doc3 | NULL | . doc3 | NULL | . doc4 | NULL | . doc5 | NULL | . | Because the WHERE condition filters after the LEFT OUTER JOIN, THE NULL rows of the user column are removed. You can see that because the ON condition filters before the LEFT OUTER JOIN, the NULL rows are kept in the final result. | For INNER JOINs, put join conditions in the ON clause and put where conditions in the WHERE clause. | For LEFT OUTER JOINs, put join conditions in the ON clause and put where conditions that reference the right table in the ON clause also. Referencing the right table after the LEFT JOIN in the WHERE clause converts the LEFT JOIN into an INNER JOIN. | Exception: reference the right table in the WHERE clause when looking for records not in the table, e.g. WHERE t2.id IS NULL. | Think of conditions in the ON clause as “right table WHERE clause filters” that are applied prior to joining. | . | . Joins with Multiple Foreign Keys . Accuracy of joining is improved | Performances: SQL uses indexes to speed up queries Using multiple join keys can speed up performance for large datasets | . SELECT A.col1, B.col1 FROM table1 A JOIN table2 B ON A.id = B.id AND A.col = B.col . | . Self Joins . Joins a table with itself | Self joins can help compare different records of the same table . -- WHERE: Find pairs from the same city SELECT A.name, B.name, A.city FROM users A, users B WHERE A.id &lt;&gt; B.id -- avoid matching users to themselves AND A.city = B.city -- find pairs from the same city -- ON: Find all pairs SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id != B.id -- prevents identical pairs, e.g. (5, 5) -- ON: Find all unique pairs SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id &lt; B.id -- prevents identical pairs and permutations, e.g. (1, 5) but not (5, 1) -- ON: Find unique pairs of duplicates SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id &lt; B.id -- gets different pairs AND A.last = B.last -- with the same value, eg. different people with same last name . | . UNION . Stacks two tables vertically | Only appends DISTINCT values, identical rows are dropped | Use UNION ALL to append all values, including identical rows | Can write 2 SELECT queries, and put the results on top of each other to create one table Result set’s column names usually same as first SELECT clause | . | Every SELECT statement within UNION must have same number of columns The columns must have similar data types | The columns in every SELECT statement must also be in the same order | . -- UNION Example SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; -- Union Example SELECT &#39;Customer&#39; AS Type, ContactName, City, Country FROM Customers UNION SELECT &#39;Supplier&#39;, ContactName, City, Country FROM Suppliers; -- UNION ALL Example SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; ORDER BY City . | . Data Types . Differences vary across various versions of SQL | . Imported As Stored As . String | VARCHAR(1024) | . Date/Time | DATE, DATETIME, TIMESTAMP, INTERVAL, YEAR | . Number | Exact: INTEGER, SMALLINT, DECIMAL, NUMERIC Approx: FLOAT, REAL, DOUBLE PRECISION | . Boolean | BOOLEAN | . CAST . In MySQL, changes data type of a column | In PostgreSQL, can also use expression::type casting syntax . SELECT CAST(col AS Integer) AS int_col FROM table_name WHERE CAST(col AS Integer) &gt; 10 . | . Dates . DATE, DATETIME, TIMESTAMP are date types used in SQL versions | YYYY-MM-DD is used for easy ordering, identical orders for dates and strings | Date ranges are stored as INTERVAL Intervals are defined in English terms, e.g. 10 seconds, 5 days, or 2 months | . -- Using intervals SELECT (CAST(table_name.date AS Timestamp) + INTERVAL &#39;1 week&#39;) AS week_after FROM table_name . | EXTRACT(field FROM date_col): helps deconstruct dates field includes: YEAR, MONTH, DAY, HOUR, SECOND, DECADE, DOW (day of week) | . SELECT EXTRACT(YEAR FROM date_col) AS year FROM table_name . | DATE_TRUNC(field, date_col): rounds date to a specified precision field includes: YEAR, MONTH, DAY, HOUR, SECOND, DECADE | . SELECT DATE_TRUNC(DAY, date_col) AS rounded_day FROM table_name . | System Datetime Variables CURRENT_DATE() | CURRENT_TIME() | CURRENT_TIMESTAMP() | LOCALTIME() | LOCALTIMESTAMP() | NOW() | To any above, can add AT TIME ZONE timezone_code to convert to a different timezone. | . SELECT NOW() SELECT CURRENT_DATE() AT TIME ZONE &#39;PST&#39; AS current_date_pst . | . Null Functions . Functions vary by SQL version | IFNULL(col, value): returns default value if expression is null | COALESCE(col, value): returns default value if expression is null . SELECT name, price * (inventory + COALESCE(ordered, 0)) FROM products . | . String Functions . LEFT(str, n): get n characters from left end of string | RIGHT(str, n): get n characters from right end of string | TRIM(type chars FROM str): trim characters from string TRIM(both &#39;()&#39; FROM col1): trim from both ends | TRIM(leading &#39;()&#39; FROM col1): trim from front | TRIM(trailing &#39;()&#39; FROM col1): trim from back | . | POSITION(substr IN str): get position of substring in string, case-sensitive | STRPOS(str, substr): get position of substring in string, case-sensitive | SUBSTR(str, start_idx, n): get n-length substring starting at index of string | CONCAT(str, str, ...): concatenate strings | str || str || ...: concatenate strings | UPPER(str): convert string to upper case | LOWER(str): convert string to lower case | . Subqueries . Subqueries are nested/inner queries to perform operations in multiple steps | Subqueries are required to have aliases . SELECT sub.* FROM ( SELECT * FROM students WHERE grade &gt; 80 ) sub WHERE sub.class_level = &#39;Senior&#39; . | Example: How many incidents happen, on average, on Fridays in December? . -- Count # of incidents that happen on each day. -- Average # of daily incidents over each day of week (5: Friday) in month (December). SELECT EXTRACT(MONTH FROM sub.incident_date) AS &quot;Month&quot;, sub.day_of_week, AVG(sub.daily_incidents) FROM ( SELECT EXTRACT(DOW FROM incident_date) AS day_of_week, incident_date, COUNT(incident_id) AS incidents FROM log_table GROUP BY 1, 2 ) sub WHERE sub.day_of_week = 5 AND EXTRACT(MONTH FROM sub.incident_date) = 12 . | Example: How many incidents happen for each category, averaged over all months? . -- Count # of incidents per category that happen on each month -- Average # of incidents per category over all months SELECT sub.category, AVG(sub.incidents) AS avg_incidents_per_month FROM ( SELECT EXTRACT(MONTH FROM incident_date) AS &quot;month&quot;, category, COUNT(incident_id) as incidents FROM log_table GROUP BY 1, 2 ) sub GROUP BY 1 . | Subqueries in Conditional Logic Don’t use aliases for subqueries in conditional statements . | Example: get all products that have min price . | . SELECT * FROM products WHERE price = (SELECT MIN(price) FROM products) -- subquery returns single cell . Example: get the products in the top 5 prices | . SELECT * FROM products WHERE price IN ( SELECT DISTINCT price FROM products ORDER BY price DESC LIMIT 5 ) -- subquery returns multiple cells (of one column) . | Joining Subqueries Using filtering in the ON clause can handle cases when subquery returns one or multiple rows | ON filtering is the same as WHERE filtering for INNER JOINS . | Example: get the products in the top 5 prices | . SELECT * FROM products JOIN ( SELECT DISTINCT price FROM products ORDER BY price DESC LIMIT 5 ) sub ON products.price = sub.price . | Joining Subqueries with Aggregation Functions . Example: Find the products from the top 3 categories with the least counts. | . -- subquery returns top 3 categories with least product counts -- query selects all products in the specified 3 categories -- result set is ordered by count and descending price -- (products with same category have same counts) SELECT products.*, sub.c AS counts FROM products JOIN ( SELECT category, COUNT(id) AS c FROM products GROUP BY category ORDER BY c LIMIT 3 ) sub ON products.category = sub.category ORDER BY sub.c, products.price DESC . Unlike using WHERE col IN (subquery), JOIN (subquery) ON table.col=sub.col allows aggregated values to be passed into the outer query because an IN clause is limited to one set of values. Additionally, MySQL does not allow LIMIT in subqueries in the WHERE clause. | . | . | Using Subqueries to Improve Performance Inefficient: FULL JOIN creates a huge intermediate join table | . -- Get counts of A and B entities per month SELECT COALESCE(A.month, B.month) AS month, -- fill null values of outer join COUNT(DISTINCT A.id) AS count_A, COUNT(DISTINCT B.id) AS count_B FROM A FULL OUTER JOIN B -- creates a large intermediate join table ON A.month = B.month GROUP BY 1 . Intermediate Join Table . A.id A.month B.month B.id . 1 | A | A | 5 | . 1 | A | A | 6 | . 2 | B | B | 7 | . 3 | B | B | 7 | . 4 | C | NULL | NULL | . NULL | NULL | D | 8 | . | Result . month count_A count_B . A | 1 | 2 | . B | 2 | 1 | . C | 1 | 0 | . D | 0 | 1 | . | Optimized: Use subqueries to divide the problem into smaller tables before joining . | . -- Get counts of A and B entities per month -- Pre-aggregate counts in A and B separately before joining SELECT COALESCE(A.month, B.month) as month, subA.count_A, subB.count_B FROM ( SELECT month, COUNT(DISTINCT id) AS count_A FROM A GROUP BY 1 ) subA FULL OUTER JOIN ( SELECT month, COUNT(DISTINCT id) AS count_B FROM B GROUP BY 1 ) subB ON subA.month = subB.month . Intermediate Join Tables . month count_A . A | 1 | . B | 2 | . C | 1 | . month count_B . A | 2 | . B | 1 | . D | 1 | . | Result . month count_A count_B . A | 1 | 2 | . B | 2 | 1 | . C | 1 | 0 | . D | 0 | 1 | . | . | The two methods get the same result, but the join using subqueries is faster because operating on smaller intermediate join tables is more efficient. Method 1 joins both tables then gets counts for A and B per month. | Method 2 gets counts for A and B per month separately, then joins the results together. | . | . Window Functions . A window is a group of related rows that are somehow related the current row, e.g. all rows with same month or same city | A window function performs calculation across a window, Unlike aggregation functions, window functions don’t group rows into a single output row No need to group by non-aggregated rows | . | Window functions are able to access more than just the current row of query result | A window function takes in a column/row and a window of related rows that includes the row | . | Aggregation functions can serve as window functions Inside OVER(), use ORDER BY to sort the window by a column | Cannot use GROUP BY with window functions | . SELECT SUM(col) OVER (...) SELECT COUNT(col) OVER (...) SELECT AVG(col) OVER (...) -- Example: Get a running total -- Using SUM as a window function -- ORDER BY to calculate chronologically SELECT duration, SUM(duration) OVER (ORDER BY start_time) AS running_total FROM rides . Window Result Durations are summed and ordered by start_time to create a running total | Without ORDER BY, each value would be sum of all seconds in its partition | . | . duration running_total . 1 | 1 | . 1 | 2 | . 2 | 4 | . 1 | 5 | . 4 | 9 | . 3 | 12 | . | Apply window function over individual groups Inside OVER(), use PARTITION BY to specify how the window function builds the window Each partition will be treated as one window, upon which the function will be applied | . | Without PARTITION BY, the whole table will be treated as one window | . -- using aggregation, cannot get individual information from all employees SELECT dept_name, MAX(salary) AS max_salary FROM employees GROUP BY dept_name -- better to use window function -- PARTITION BY specifies how to group rows to form a window SELECT E.*, MAX(salary) OVER(PARTITION BY dept_name) AS max_salary FROM employees E . The aggregation will return each department and the max salary of that department, without any other employee data. | The window function will return every employee data with the max salary of the department that employee works in The window function duplicates the department’s max salary to every row | Without PARTITION BY, the max salary of all departments will be duplicated for every row | . | . | ROW_NUMBER(): window function that returns number of row . | RANK(): window function that ranks values Like ROW_NUMBER() but gives same values when tied, e.g. 1, 2, 2, 4, 4, 4, 7 | . | DENSE_RANK(): window function that ranks values without skipping numbers Like RANK() but does not skip values, e.g. 1, 2, 2, 3, 3, 3, 4 | . SELECT team, score, ROW_NUMBER(score) OVER(PARTITION BY team ORDER BY score), RANK(score) OVER(PARTITION BY team ORDER BY score), DENSE_RANK(score) OVER(PARTITION BY team ORDER BY score) FROM table_name . Team Score ROW_NUMBER() RANK() DENSE_RANK() . A | 30 | 1 | 1 | 1 | . A | 40 | 2 | 2 | 2 | . A | 40 | 3 | 2 | 2 | . A | 55 | 4 | 4 | 3 | . B | 20 | 1 | 1 | 1 | . B | 20 | 2 | 1 | 1 | . B | 35 | 3 | 3 | 2 | . C | 20 | 1 | 1 | 1 | . -- Fetch the top 3 employees in each department by salary -- Using ROW_NUMBER() may miss out on other employees tied for top 3 salary values SELECT * FROM ( SELECT E.*, ROW_NUMBER() OVER (PARTITION BY dept_name ORDER BY salary DESC) AS salary_num FROM employee E ) sub WHERE sub.salary_num &lt; 4 -- Fetch the top employees in each department -- who earn the top 3 salaries in their department -- Returns every employee earning one of the top 3 salaries in their department SELECT * FROM ( SELECT E.*, RANK() OVER (PARTITION BY dept_name ORDER BY salary DESC) AS salary_rank FROM employee E ) sub WHERE sub.salary_rank &lt; 4 . | NTILE(n): window function that determines which bucket row falls in e.g. use n = 4 for quartile, or n = 100 for percentile | For small partition rows with less rows than tiles, the tiles will resemble a numerical ranking | . -- get quartile and percentile of student scores for each class SELECT class, score, NTILE(4) OVER (PARTITION BY class ORDER BY score DESC) AS quartile, NTILE(100) OVER (PARTITION BY class ORDER BY score DESC) AS percentile FROM table_name . | LAG(col, n): window function that creates a column that pulls from the previous n rows of col First n rows of column will be NULL because no previous rows to pull from | Some versions of SQL allow default values, i.e. LAG(col, n, default), to fill in NULL values | . | LEAD(col, n): window function that creates a column that pulls from next n rows of col Last n rows of column will be NULL because no previous rows to pull from | Some versions of SQL allow default values, i.e. LEAD(col, n, default), to fill in NULL values | . -- Example of lag and lead score, partitioned on team SELECT team, score, LAG(score, 1) OVER (PARTITION BY team ORDER BY score) AS lag_score, LEAD(score, 1) OVER (PARTITION BY team ORDER BY score) AS lead_score FROM table_name . team score lag_score lead_score . A | 20 | NULL | 30 | . A | 30 | 20 | 40 | . A | 40 | 30 | NULL | . B | 30 | NULL | 60 | . B | 60 | 30 | 70 | . B | 70 | 50 | 75 | . B | 75 | 70 | NULL | . -- Show if the salary of an employee is higher, lower, or equal to the previous employee. SELECT E.*, LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id) AS prev_emp_salary, CASE WHEN E.salary &gt; LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id THEN &#39;Higher&#39; WHEN E.salary &lt; LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id) THEN &#39;Lower&#39; ELSE &#39;Equal&#39; END AS salary_range FROM employee E . id name dept_name salary prev_emp_salary salary_range . 1 | Anna | Admin | 4000 | 0 | Higher | . 3 | Bert | Admin | 5000 | 4000 | Higher | . 4 | Dirk | Admin | 3000 | 5000 | Lower | . 6 | Cris | Admin | 6000 | 3000 | Higher | . 2 | Kurt | Finance | 4000 | 0 | Higher | . 5 | Fred | Finance | 5000 | 4000 | Higher | . 7 | Paul | Finance | 5000 | 5000 | Equal | . 8 | Evan | Finance | 2000 | 5000 | Lower | . -- Example of removing null rows using subquery SELECT sub.* FROM ( SELECT location, duration - LAG(duration, 1) OVER (PARTITION BY location ORDER BY duration) AS difference FROM rides ) sub WHERE sub.difference IS NOT NULL . | Window Aliases can name and re-use same window for several queries | should always come after the WHERE clause | . SELECT location, duration, NTILE(4) OVER ntile_window AS quartile, NTILE(100) OVER ntile_window AS percentile FROM rides WINDOW ntile_window AS (PARTITION BY location ORDER BY duration) -- window alias ORDER BY location, duration . | . SELECT INTO . Copies data from one table into a new table (in external database) . SELECT * INTO adults IN &#39;backup.db&#39; -- optional: [IN external_db] FROM users WHERE age &gt;= 18 . | . UPDATE . Modifies existing records in a table . UPDATE table_name SET col1 = val1, col2 = val2, ... WHERE condition -- without WHERE clause, all rows will be updated . | . DELETE . Deletes data in a database | . INSERT INTO . Inserts new data into a database . -- specifying which columns, values to insert INSERT INTO table_name (col1, col2, col3, ...) VALUES (val1, val2, val3, ...) -- don&#39;t need to specify if adding all columns INSERT INTO table_name VALUES (val1, val2, val3, val4, val5) . | . INSERT INTO SELECT . Copies data from one table and inserts it into another table | data types in source and destination tables must match . INSERT INTO all_profits SELECT * FROM current_profits WHERE company = &#39;Apple&#39; . | . Common Table Expressions (CTE) . A temporary named result set that can be referenced within SELECT, INSERT, UPDATE, or DELETE statements, also used in a CREATE to create a view | Advantage over subqueries: CTEs can be use multiple times in a query . -- Find average grades for every senior student WITH cte_student_grades (student, class_year, avg_grade) AS ( SELECT S.first_name + &#39; &#39; + S.last_name, YEAR(S.graduation_date), AVG(E.grade) FROM students S JOIN exams E ON S.id = E.student_id GROUP BY first_name + &#39; &#39; + last_name, YEAR(graduation_date) ) -- use CTE in a SELECT statement SELECT student, avg_grade, &#39;Senior&#39; AS class_level FROM cte_student_grades WHERE class_year = 2021 . | Use to perform multi-level aggregations, e.g. average minimum grade AVG(MIN(grade)) SQL does not allow subqueries or aggregate functions inside an aggregate function | SO you have to do MIN(grade) first in a CTE, then do AVG(min_grade) | . -- Find average min and average max exam grades across all subjects WITH min_max_grade AS ( SELECT SU.id, SU.subject_name, MIN(E.grade) AS min_grade, MAX(E.grade) AS max_grade FROM subjects SU JOIN exams E ON SU.id = E.subject_id GROUP BY SU.id, SU.subject_name ) SELECT AVG(min_grade) AS avg_min_grade, AVG(max_grade) AS avg_max_grade FROM min_max_grade; -- use CTE in a query -- Alternatively, subqueries are not as readable, reusable, -- and opposite in terms of thought process (AVG -&gt; MIN/MAX) SELECT AVG(min_grade) AS avg_min_grade, AVG(max_grade) AS avg_max_grade FROM ( SELECT SU.id, SU.subject_name, MIN(E.grade) AS min_grade MAX(E.grade) AS max_grade FROM subjects SU JOIN exams E ON SU.id = E.subject_id GROUP BY 1, 2 ) . | . Stored Procedures . A prepared SQL code that can be saved and reused, like a function . -- Storing procedure CREATE PROCEDURE SelectAllUsers AS SELECT * FROM Users GO -- Executing procedure EXEC SelectAllUsers . | A procedure can take in parameters . -- Storing procedure CREATE PROCEDURE SelectAllUsers @City nvarchar(30), @MinAge Int AS SELECT * FROM Users WHERE city = @City AND age &gt;= @MinAge GO -- Executing procedure EXEC SelectAllUsers @City = &#39;Los Angeles&#39;, @MinAge = 18 . | . SQL Database . CREATE DATABASE . Creates a new database: CREATE DATABASE testDB | . DROP DATABASE . Drop an existing database: DROP DATABASE testDB | . BACKUP DATABASE . Create full backup of existing database . BACKUP DATABASE testDB TO DISK = &#39;D: backups latest_backup.bak&#39; WITH DIFFERENTIAL -- include to only back up changes since last full backup . | . SQL Table . Creating a table . -- Create new table CREATE TABLE Users ( id int, name varchar(255), city varchar(255), age int, is_loyal boolean ) -- Create new table from another table CREATE TABLE NYC_Users AS SELECT id, name, city, age FROM Users WHERE city = &#39;New York City&#39; . | . SQL Constraints . Specifies rules for the data in the table | . Constraint Function . NOT NULL | column cannot have null values | . UNIQUE | column has all different values | . PRIMARY KEY | column is NOT NULL and UNIQUE, serves as row id | . FOREIGN KEY | prevents actions that would destroy links between tables | . CHECK | ensures column values specify a specific condition | . DEFAULT | set default value for column if no value specified | . CREATE INDEX | used to quickly create and retrieve data from database | . AUTO_INCREMENT | starting from 1, increments by 1 every time row inserted | . sql -- Create table with various constraints CREATE TABLE Users ( id int NOT NULL AUTO_INCREMENT, dept_id int NOT NULL, full_name varchar(255) NOT NULL, email varchar(255) UNIQUE, age int, city varchar(255) DEFAULT &#39;Seattle&#39;, CHECK (age &gt;= 18), PRIMARY KEY (id), FOREIGN KEY (dept_id) ) . Alter or add, delete, modify columns or various constraints of a table . -- Add column ALTER TABLE Users ADD email VARCHAR(255) -- Drop column ALTER TABLE Users DROP column is_loyal -- Change datatype ALTER TABLE Users MODIFY COLUMN city varchar(300) . | Remove a table . -- Delete a table DROP TABLE Users -- Truncate: delete data inside but not table itself TRUNCATE TABLE Users . | . SQL Index . An index is used to retrieve data more quickly, not seen by users . -- Create index CREATE INDEX idx_name ON Users (last_name) -- Create unique index CREATE UNIQUE INDEX idx_name ON Users (first_name, last_name) -- Drop index DROP INDEX idx_name ON Users . | . Views . A virtual table based on the result-set of an SQL statement . -- Create the view CREATE VIEW [Luxury Products] AS SELECT product_name, price FROM products WHERE price &gt; (SELECT AVG(price) FROM products) -- Query the view SELECT * FROM [Luxury Products] -- Update the view CREATE OR REPLACE VIEW [Luxury Products] AS SELECT product_name, price FROM products WHERE price &gt; (SELECT AVG(price) FROM products) -- Drop the View DROP VIEW [Luxury Products] . | . References .",
            "url": "https://nhtsai.github.io/notes/sql-overview",
            "relUrl": "/sql-overview",
            "date": " • Sep 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "System Design Overview",
            "content": "System Design . Scalability . Scalability for Dummies | A Word on Scalability | . Clones . Load balancers evenly distribute user requests to public web servers. | Rule #1: Every server contains exactly the same codebase and does not store any user-related data, like sessions or profile pictures, on local disc or memory. | Sessions need to stored in a centralized data store that is accessible by all application servers. The data store can be an external database or an external persistent cache (e.g. Redis), which will have better performance than an external database. | External means data store is somewhere in or near the data center of application servers, does not reside on application servers themselves. | . | Deployment ensures that a code change is sent to all servers without an outdated server still serving old code. | Clones are instances of an machine image based upon a “super-clone” that is created from one of your servers. Just do an initial deployment of your latest code to a new clone and everything is ready. | . | . Databases . Using cloning, you can now horizontally scale across multiple servers to handle lots of requests. | But one day, your application slows and breaks due to the MySQL database. | Path 1: Keep MySQL Apply active-passive replication strategy on the database. | Upgrade server components like RAM. | Consider sharding, denormalization, SQL tuning, etc. | Eventually the upkeep will become too expensive. | . | Path 2: NoSQL Denormalize right from the beginning. | Remove joins from any database query. | Use MySQL as a NoSQL database or switch to MongoDB. | If database requests still get too slow, consider a cache. | . | . Cache . An in-memory cache (e.g. Memcached, Redis) is a simple key-value store that resides between the application and data storage. | The application should try to read from the cache first before hitting the database because the cache is lightning fast. The cache holds every dataset in RAM and handles requests as fast as possible. | . | Cached Database Queries Whenevery a query on the database is run, store the result dataset in a cache. | Use a hashed version of query as cache key. | Expiration is a large issue. All cached results that include piece a data needed to be deleted when that piece of data changes. | . | . | Cached Objects See the data as an object or class. Let the class assemble the dataset from the database. | Store the complete instance of the class or assembled dataset into the cache. | . | Rather than storing results of multiple queries, we can aggregate the results as data for a class instance and store the instance in the cache. If this ID is not present in the cache, load the data from DB, translate it and return to the client. You can cache this result by translating this data again, into the rawdata your cache has, and put it into the cache. | This makes it easy to delete the object when a piece of data changes. | This makes asynchronous processing possible. Servers query the database to assemble the data in the class. | The application just serves the latest cached object and never touches the database. | . | Example Objects: user sessions, fully rendered blog articles, activity streams, user-friend relationships The blog object has multiple methods that query the database for data. | Instead of caching the result of these separate database calls, cache the entire blog object. | When something changes, the blog object queries the database for updated data. | The application only has to serve the latest cached blog post instead of querying the database. | . | . | . Asynchronism . Asynchronously doing work in advance and serving finished work with low request time. Used to turn dynamic content into static content. Think pre-rendering pages into static HTML files to be served quicker. | The rendering can be scripted to run every hour by a cronjob. | This pre-computing process helps make web applications scalable and performant | . | . | Referring unforseen, immediate requests to an asynchronous service. Upon receiving a computing intensive task, the job is added to the job queue. | The job is then sent to an asynchronous server to be processed in the background. | The results are returned once the server is done processing. | . | Basic idea: Have a queue of tasks or jobs that a worker can process. Backends become scalable and frontends become responsive. | Tools to implement async processing: RabbitMQ | . | . Performance vs Scalability . A service is scalable if it results in increased performance in a manner proportional to resources added. E.g. adding another server to handle requests speeds up the website response time | . | If the system is slow for a single user, there is a performance problem. | If the system is fast for a single user but slow under heavy load, there is a scalability problem. . | An always-on service is scalable if adding resources to facilitate redundancy does not result in loss of performance. E.g. adding multiple copies of a database for redundancy does not decrease the query response time | . | Scalability requires applications be designed with scaling in mind. | Scalability also has to handle heterogeneity. As new resources (hardware, software) come online, some nodes will be able to process faster or store more data than other nodes in a system. | . | . Latency vs Throughput . Latency is the time to perform some action or to produce some result. | Throughput is the number of such actions or results per unit of time. | Generally, you should aim for maximal throughup with acceptable latency. | . Availability vs Consistency . CAP Theorem . * In a distributed system, you can only support 2 guarantees: * **Consistency**: every read receives most recent write or an error * **Availability**: every request receives a response, without guarantee that it contains the most recent version of the data * **Partition Tolerance**: system continues to operate despite arbitrary partitioning due to network failures, e.g. a server crashes or goes offline * Networks aren&#39;t reliable, so *partition tolerance needs to be supported*. * You need to make a *software tradeoff between consistency and availability*. . Consistency and Partition Tolerance (CP) . * System is consistent across servers and can handle network failures, but responses to requests are not always available. * Waiting for a response from the partitioned node might result in a timeout error. * Good choice if **atomic reads and writes** are required. * **Atomic** refers to performing operations one at a time. . Availability and Partition Tolerance (AP) . * System is always available and can handle network failures, but the data is not always consistent or up to date across nodes. * Responses return the most readily available version of the data on any node, which might be outdated. * Writes might take some time to propagate when the partition/failure is resolved. * Good choice if **eventual consistency** is needed or when the system needs to continue working despite external errors. . Consistency Patterns . With multiple copies of the same data (redundancy), how do we synchronize them across nodes (consistency) to provide all users the same view of the data? The CAP Theorem need to respond to every read with the most recent write or an error to be consistent. | . | . Weak Consistency . * After a write, reads *may or may not* see it, and a best effort approach is taken. * Weak consistency works well for real-time use cases, such as VoIP, video chat, and realtime multiplayer video games. * If you briefly lose reception during a phone call, you don&#39;t really care or hear what was lost during connection loss. * Weak consistency is used in systems like memcached, where the result might or might not be there. . Eventual Consistency . * After a write, reads *will eventually* see it, typically within milliseconds. * Data is **replicated asynchronously**. * Eventual consistency works well in highly available systems. * Eventual consistency is used in systems like DNS and email. . Strong Consistency . * After a write, reads *will* see it. * Data is **replicated synchronously**. * Strong Consistency works well in systems that need transactions. * Strong consistency is used in systems like file systems and relational database management systems (RDBMSes). . Transactions . * An extended form of consistency across multiple operations. * E.g. transfering money from account A to account B * Operation 1: subtract from A * Operation 2: add to B * What if something happens in between operations? * E.g. Another transaction A or B, machine crashes * You want some kind of guarantee that the invariants will be maintained. * Money subtracted from A will go back to A. * Money created will eventually be added to B. * Transactions are useful because... * Correctness * Consistency * Enforce invariants * ACID: atomic, consistent, isolated, durable . Availability Patterns . Fail-Over . * Active-Passive * Heartbeats are sent between the active server and the passive server on standby. Only the active server handles traffic. * If a heartbeat is interrupted, the passive server takes over the active server&#39;s IP address and resumes service to maintain availability. * Downtime duration is determined by whether passive servier is already running in &#39;hot&#39; standby or starting from &#39;cold&#39; standby. * Active-Active * Both servers are managing traffic, spreading load between them * If servers are public-facing, DNS needs to know about public IPs of both servers. * If servers are private-facing, application logic needs to know about both servers. * Disadvantages * More hardware and additional complexity * Potential for loss of data if active system fails before any newly written data can be replicated to the passive . Replication . * Master-Slave * One master node handles all writes, which are then replicated onto multiple slave nodes. * Master-Master * Both master nodes handle all write requests, spreading load between them. The changes are then replicated onto multiple slave nodes. . Availability in Numbers . * **Uptime** or **downtime** is the percentage of time the service is available/not available. * Availability is generally measured in 9s, by which a service with 99.99% availability is described as having &quot;four 9s&quot;. | Acceptable Downtime Duration | 99.9% Availability | 99.99% Availability | | -- | : | : | | Downtime per year | 8h 45m 57.0s | 52m 35.7s | | Downtime per month | 43m 49.7s | 4m 23.0s | | Downtime per week | 10m 04.8s | 1m 05.0s | | Downtime per day | 1m 26.4s | 08.6s | . Availability in Sequence . * Overall availability *decreases* when two components with &lt; 100% availability are in **sequence**. * $$ text{Availability}( text{Total}) = text{Availability}( text{Foo}) * text{Availabiilty}( text{Bar})$$ * If both Foo and Bar have 99.9% availability each, their total availability in sequence would be 99.8%. . Availability in Parallel . * Overall availability *increases* when two componenets with &lt; 100% availabiilty are in **parallel**. * $$ text{Availability}( text{Total}) = 1 - (1 - text{Availability}( text{Foo})) * (1 - text{Availabiilty}( text{Bar}))$$ * If both Foo and Bar have 99.9% availability each, their total availability in parallel would be 99.9999%. . Domain Name System (DNS) . A Domain Name System (DNS) translates a domain name, e.g. www.example.com, to an IP address, e.g. 8.8.8.8. DNS is hierarchical, with a few authoritative servers at the top level. | Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. | Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. | DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL). | . | A Name Server (NS) Record specifieds the DNS servers for your domain/subdomain. | A Mail Exchange (MX) Record specifies the mail servers for accepting messages. | An Address (A) Record points a name to an IP address. | A Canonical Name (CNAME) points a name to another name or to an A Record, e.g. pointing example.com to www.example.com. | Services that provide managed DNS services include: CloudFlare, Route 53, etc. | . DNS Traffic Routing Methods . * **Round Robin** * Pairs an incoming request to a specific machine by circling through a list of servers capaable of handling the request * May not result in a perfectly-balanced load distribution * **Weighted Round Robin** * Each server machine is assigned a performance value, or weight, relative to the other servers in the pool, usually in an automated benchmark testing. * This weight determines how many more or fewer requests are sent to that server, compared to other servers in the pool. * The result is a more even or equal load distribution. * Prevents traffic from going to servers under maintenance. * Weights can help load balance between varying cluster sizes. * A/B Testing * **Latency-Based** * Create latency records between servers in multiple regions. * When a request arrives, the DNS queries the NS, which looks at the most recent latency data. * The load balancer with the lowest latency is the one chosen to serve the user. * **Geolocation-Based**: * Choosing servers to serve traffic based on the geographic location of users. * E.g. routing all European traffic to a European load balancer * Can localize content and restrict content distribution based on region. * Can load balance predicatably so each user location is consistently routed to the same endpoint. . Disadvantages of DNS . * Accessing a DNS server introduces a slight delay, which can be mitigated by caching. * DNS server management is complex and generally managed by governments, ISPs, and large companies. * DNS services are susceptible to **Distributed Denial of Service (DDoS)** attacks, which prevent users from accessing websites without knowing Twitter&#39;s IP address(es). . Content Delivery Network (CDN) . A Content Delivery Network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files (e.g. HTML, CSS, JS, photos, videos) are served from CDNs. Some CDNs like AWS CloudFront supports serving dynamic content. | . | The website’s DNS resolution tells clients which CDN server to contact. | . | Advantages Improved performance because users receive content from data centers close to them. | Reduced load because your servers do not have to serve requests that the CDN fulfills. | . | . Push CDNs . * Receive new content whenever changes occur on your server. * Content is only uploaded when it is new or changed, minimizing traffic but maximizing storage. * You take full responsibility for providing content, uploading directly to the CDN, and rewriting URLs to point to the CDN. * You can configure when content expires and when it is updated using TTLs. * Push CDNs work well for sites with small amounts of traffic or content that isn&#39;t often updated. * Content is pushed to the CDNs when needed, instead of being re-pulled at regular intervals. * For lots of updates, pushing content to the Push CDN places load on the server. * For heavy traffic, the Push CDN&#39;s cached content may not be sufficient and will place more load on the server to push content to the Push CDN. . Pull CDNs . * Grabs new content from your server when the first user requests the content. * You take full responsibility for providing content and rewriting URLs to point to the CDN. * This results in slower requests until content is cached on the CDN, as users need to pull from the server upon the first request. * A **time to live (TTL)** determines the life of the cached content, which you do not typically have control of. * Pull CDNs minimizing storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed. * Pull CDNs work well for sites with heavy traffic because the traffic spread out more evenly with only recently-requested content remaining on the CDN. * Older requested content is expired by the TTL, making space for new content. * For lots of updates, the Pull CDN is able to pull and cache the updated content when requested or old content is expired. * For heavy traffic, the Pull CDN can serve the most requested, cached content, only pulling from the server when for less requested content. . Disadvantages of CDNs . * CDN costs could be significant depending on traffic, although this should be compared against additional costs of not using a CDN. * Cached content might be stale if it is updated before the TTL expires it to be updated. * CDNs require changing URLs for static content to point to the CDN, e.g. directing `facebook.com` to `cdn-images.fb.com`. . Load Balancer . | . Reverse Proxy . Advantages | | . | Disadvantages | . Load Balancer vs Reverse Proxy . Application Layer . Microservices . Service Discovery . Disadvantages . Database . SQL/Relational Database Management System (RDBMS) ACID | Master-Slave Replication | Master-Master Replication | Disadvantages of Replication . | Federation . | Sharding . | Denormalization . | SQL Tuning | . | NoSQL BASE | Key-Value Store | Document Store | Wide-Column Store | Graph Database | . | SQL vs NoSQL | . Cache . Client Caching . | CDN Caching . | Web Server Caching . | Database Caching Query Caching | Object Caching | . | Application Caching . | Updating Cache Cache-Aside | Write-Through | Write-Behind/Write-Back | Refresh-Ahead | . | Cache Disadvantages | . Asynchronism . Message Queues | Task Queues | Back Pressure | Aynchronism Disadvantages | . Communication . Hypertext Transfer Protocol (HTTP) | Transmission Control Protocol (TCP) | User Datagram Protocol (UDP) | Remote Procedure Call (RPC) | Representational State Transfer (REST) | RPC vs REST Calls | . Security . References .",
            "url": "https://nhtsai.github.io/notes/system-design-overview",
            "relUrl": "/system-design-overview",
            "date": " • Sep 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Atomic Habits",
            "content": "Chapter 1: The Surprising Power of Atomic Habits . Small improvements aren’t notable or noticeable but are far more meaningful in the long run. A 1% improvement everyday results in 1.01365=37.781.01^{365} = 37.781.01365=37.78 change. | A 1% decline everyday results in 0.99365=0.030.99^{365} = 0.030.99365=0.03 change. | . | Habits are the compound interest of self improvement. The value of consistent, small changes are only apparent after years. Likewise, bad habits can accumulate in many missteps which compound to toxic results. | Success is the product of daily habits – not once-in-a-lifetime transformations. | . | Focus on the trajectory of habits. Outcomes are a lagging measure of your habits, built up from many small behavioral changes. | . | Time magnifies the margin between success and failure, in which effects are multiplied after a while. | . . Positive Compounds Negative Compounds . Productivity: Doing extra work builds up. | Stress: Little stresses compound into serious health issues. | . Knowledge: Lifelong learning is transformative. | Negative Thoughts: Bad thoughts can skew your interpretation of life. | . Relationships: Small interactions can build a network of strong connections. | Outrage: Small aggravations build up to mass outrage. | . Breakthrough moments are often the result of many previous actions. | Habits seem to make no difference until a critical threshold is crossed. This Valley of Disappointment is why many people give up on new habits. Compounding is not a linear improvement; the most powerful outcomes are delayed, think exponentially. | . | Habits need to persist long enough to break through the Plateau of Latent Potential in order to make meaningful difference. Change can take years – before it happens all at once. | . | Systems are about the process that lead to goals, the results you want to achieve. Goals are good for setting a direction, but systems are best for making progress. | You do not rise to the level of your goals. You fall to the level of your systems. | . | . . Problems with Goals . Winners and losers have the same goals: The difference in outcomes is due to systems of continuous improvements. | . Achieving a goal is only a momentary change: Systems tackle the underlying issues rather than temporarily achieving satisfactory results. | . Goals restrict happiness: You put off happiness until goals are met, rather than falling in love with the process. | . Goals are at-odds with long-term progress: Commitment to the process continuous improvement determines progress. | . An atomic habit is a little habit that is part of a larger system, building up to remarkable achievements through compound growth. | . Chapter 2: How Your Habits Shape Your Identity . Changing habits is challenging because you try to change the wrong thing. You should focus on building identity-based habits (3 -&gt; 2 -&gt; 1), rather than outcome-based habits (1 -&gt; 2 -&gt; 3). | . | . .   Layers of Behavioral Change   . 1. Outcomes | changing your results, setting goals | what you get | . 2. Process | changing your habits and systems | what you do | . 3. Identity | changing your beliefs, assumptions, and biases | what you believe | . Behavior that is incongruent with the self will not last. You need to change the underlying beliefs that led to past behavior in order to change existing habits. | You should change identities to more easily align with new behaviors and habits. | . | However, you should not be so attached to any specific identity as to prevent further behavioral improvements. Progress requires unlearning. | . | Repeated behaviors contribute more evidence supporting a gradual belief in a new identity and self-image. | . . Process of Changing your Identity . 1. Decide the type of person you want to be. What kind of qualities do they have? | . 2. Prove it to yourself with small wins. | . Your habits shape your identity, and your identity shapes your habits, working in a feedback loop. You should let values, principles, and identity drive this loop, rather than results. | . | Building habits is fundamentally about becoming someone, changing your beliefs about yourself. | . Chapter 3: How to Build Better Habits in 4 Simple Steps . Edward Thorndike’s studies on cat behavior shows that behaviors that result in favorable outcomes are more repeated than behaviors that result in unfavorable outcomes. | A habit is a behavior that has been repeated enough times to become automatic. Useful actions are reinforced into habits through a try, fail, learn, try differently feedback loop. | . | Brain activity decreases as it learns the cues that predict success, applying automatic solutions when the conditions are right. The conscious mind can only pay attention to one problem at a time. | Habits reduce cognitive load and free up mental capacity, so you can allocate your attention to other tasks. | Habits free your mind to focus on new challenges, allowing you to solve problems with as minimal energy and effort as possible. | . | . .   The Stages of Habit . 1. Cue (Problem Phase) | Triggers brain to initiate a behavior, predicts a reward. | . 2. Craving (Problem Phase) | Motivational force or desire to act, linked to a desired change of internal state. | . 3. Response (Solution Phase) | The habit (thought or action) you perform, depends on level of motivation and ability. | . 4. Reward (Solution Phase) | The end goal of every habit, satisfies cravings and teaches which actions are worth remembering. | . If behavior is insufficient at any of the four stages, the habit will not be formed. | Habit Loop is a neurological feedback loop that ultimately allows you to create automatic habits. The cue triggers a craving, which motivates a response, which provides a reward, which satisfies the craving, and, ultimately, becomes associated with a cue. | The problem phase (cue and craving) indicates when something needs to change. | The solution phase (response and reward) indicates when you take action and achieve the desired change. | . | The Four Laws of Behavior Change is a simple set of rules used to design good habits and eliminate bad ones. | . . Creating a Good Habit Breaking a Bad Habit . 1st Law (Cue): Make it obvious. | Inverse 1st Law (Cue): Make it invisible. | . 2nd Law (Craving): Make it attractive. | Inverse 2nd Law (Craving): Make it unattractive. | . 3rd Law (Response): Make it easy. | Inverse 3rd Law (Response): Make it difficult. | . 4th Law (Reward): Make it satisfying. | Inverse 4th Law (Reward): Make it unsatisfying. | . Chapter 4: The Man Who Didn’t Look Right . The brain is continuously analyzing information and your surroundings, picking up on important cues to predict certain outcomes subconciously. | You don’t need to be aware of the cue for a habit to begin. Habits are formed under the direction of your automatic and nonconscious mind. | . | Habits are hard to change because they are so mindless and automatic. | Pointing-and-Calling is the safety system of pointing and calling things out loud in order to reduce mistakes by 85%. It is effective because it raises a nonconscious habit to a more conscious level. | The Habits Scorecard is a similar exercise to maintain awareness of your habits and routines. For each action in a list of daily habits, you write a “+”, “-“, or “=” if it’s a good, bad, or neutral habit, respectively. | The rating is depending on your situation and your goals. “Good” refers to habits that have net positive outcomes and reinforce your desired identity. | . | The goal is to observe your thoughts and actions without any judgements to notice any nonconscious habits. Then you can point it out and say out loud the action and its outcome. | . | Behavior change starts with awareness, recognition of habits, and acknoledgement of the cues that trigger them | . Chapter 5: The Best Way to Start a New Habit . An implementation intention is a plan made beforehand about when and where to act, detailing how you intend to implement a habit. Implementation intentions leverage both time and location cues, in the form: “When situation X arises, I will perform response Y.” | . | People who make a specific plan for when and where they will perform a new habit are more likely to follow through. A lack of clarity, not motivation, is what prevents people from making improvements because it’s not always obvious when and where to take action. | Being specific about what you want and how you will achieve it helps you avoid any distractions or obstacles. | . | . . Implementation Intention I will [BEHAVIOR] at [TIME] in [LOCATION]. . Sleeping | I will sleep for 7 hours at 12AM in my bed. | . Studying | I will study math for 30 minutes at 6PM in my bedroom. | . Exercise | I will exercise for 1 hour at 5PM in my local gym. | . Relationship | I will talk to someone I know for 1 hour at 3PM in my living room. | . The Diderot Effect states that obtaining a new possession often creates a spiral of consumption that leads to additional purchases. Many human behaviors follow this principle, deciding on what to do next based on what you have just done; each action becomes a cue that triggers the next behavior. | . | Habit Stacking is a special implementation intention that pairs a new habit with a current habit, rather than a particular time and location. | . . Habit Stacking After [CURRENT HABIT], I will [NEW HABIT]. . Sleeping | After I brush my teeth, I will go to sleep for 7 hours. | . Studying | After I eat breakfast, I will study for 2 hours. | . Exercise | After I finish my work, I will exercise for 1 hour. | . Relationship | After I eat dinner, I will text a friend or family member to catch up. | . Habit Stacking allows you to create larger stacks by chaining small habits together, creating a natural momentum of behaviors. Example Morning Routine Stack: After making a cup of coffee, I will meditate for 60 seconds. | After meditating, I will write my day’s to-do list. | After writing my to-do list, I will immediately begin my first task. | | Example Evening Routine Stack: After finishing my dinner, I will clean the dirty dishes. | After cleaning the dishes, I will wipe down the counter. | After wiping down the counter, I will prepare my coffee mug for tomorrow morning. | | . | Habit Stacking works best when the cue if highly specific and immediately actionable. It’s important to select the right cue to kick things off. It should have the same frequency as your desired habit. | You can create a list of your daily habits and a list of daily occurrences to formulate the best layering of your daily habit stack. | . | The 1st Law of Behavior Change is make it obvious. Implementation Intentions and Habit Stacking help create obvious cues for you habits and design a clear plan for when and where to take action. | . | . Chapter 6: Motivation is Overrated; Environment Often Matters More . Anne Thorndike’s studies on environmental design shows that your habits change depending on your environment and the cues in front of you. “Every habit is context dependent.” | . | Kurt Lewin’s equation B=f(P,E)B=f(P,E)B=f(P,E) describes how Behavior is a function of the Person in their Environment. | Hawkins Stern’s Suggestion Impulse Buying describes how customers occasionally buy products due to presentation over desire. | Vision is the most important sensory ability, so small changes in contexts, or visual surroundings, can greatly impact your actions. | Obvious visual cues can trigger behaviors more often and better form habits. Creating multiple cues can increase the odds of triggering a behavior. | Environment design is altering living and work spaces to increase exposure of positive cues and decrease exposure to negative cues. | . | Over time, habits will become associated with the entire context surrounding the behavior, and the context becomes the cue. | Habits can be easier to change or form in a new environment, rather than building habits in the face of competing cues and old contexts. Create separate spaces (rooms or activity zones) for different activities. One space, one use. | . | Habits easily form in stable, predictable environments where everything has a place and a purpose. | . Chapter 7: The Secret to Self-Control . The Vietnam War Studies showed that soldiers addicted to heroin got rid of their addiction overnight after returning to an environment devoid of the cues triggering heroin abuse. This finding challenged the conventional association of unhealthy behavior as a moral weakness or lack of discipline. | . | You don’t need tremendous willpower and self-control if you don’t spend lots of time resisting temptations. Creating a more disciplined environment is important. | Habits encoded in the brain can arise again once the internalized cues are present. | . | To eliminate a bad habit, exposure to its cues need to be reduced. | Self-control is a short-term strategy that fails often when willpower is needed to resist temptations in a negative environment. | . Chapter 8: How to Make a Habit Irresistible . Niko Tinbergen’s experiments on North American sea birds showed that the brains are preloaded with certain rules that activate stronger than usual when exaggerated cues, or supernormal stimuli, are present. An example of supernormal stimuli is junk food for humans who have developed a craving for salt, sugar, and fat over years of evolution. | Nowadays, this craving is no longer advantageous to our health. | . | Food science studies how to make food more attractive to consumers Orosensation is how a product feels in your mouth. | Dynamic contrast refers to the combination of sensations that can make foods more interesting to eat. | . | The more attractive an opportunity is, the more likely it is to become habit-forming. | James Olds and Peter Milner showed how dopamine controls craving and desire. Habits are dopamine-driven feedback loops. | Dopamine is released both when anticipating pleasure and experiencing pleasure. | The anticipation (“wanting”) of a reward is what motivates action to fulfillment (“liking”) of a reward. | The greater the anticipation, the greater the dopamine spike. | . | Temptation bundling makes habits more attractive by linking an action you want to do with an action you need to do. Over time, the reward gets associated with the cue, and the habit becomes more attractive. | . | Premack’s Principle states that more profitable behaviors will reinforce less probable behaviors. You become conditioned to do undesirable tasks if it means you also get to do a rewarding task. | . | . . Habit Stacking + Temptation Bundling . After [CURRENT HABIT], I will [HABIT I NEED]. | . After [HABIT I NEED], I will [HABIT I WANT]. | . Chapter 9: The Role of Family and Friends in Shaping Your Habits . Laszlo Polgar showed how deliberate practice and good habits can overcome innate talent. | The culture you live in determins which behaviors are attractive to you. Humans desire to belong, so earliest habits are imitated and heavily influenced by surrounding culture and family. | . | Humans mainly imitate the close, the many, and the powerful. The Close (Friends &amp; Family) | Proximity greatly impacts behavior and habits, and family and friends provide an invisible peer pressure. | Join a culture where your desired behavior is the normal behavior and you already have something in common with the group, transforming your individual goal to a shared goal. The Many (The Tribe) | | Solomon Asch showed how the behavior of the many can often override the behavior of the individual. | Humans desire to fit in, so matching the tribe to the behavior makes change very attractive. The Powerful (Status and Prestige) | | Humans are attracted to behaviors that earn respect, approval, admiration, and status. | Humans are not attracted to behaviors that lower status or are negatively-viewed culturally. | . | . Chapter 10: How to Find and Fix the Causes of Your Bad Habits . Every behavior has a surface level craving and a deeper, underlying motive. A craving is a specific manifestation of a deeper underlying motive, a desire to change your internal state. | A desire is the difference between a current state and an ideal future state. | . | . . Craving Underlying Motive . Using Tinder | Find love and reproduce | . Browsing Facebook | Connect and bond with others | . Posting on Instagram | Win social acceptance and approval | . Searching on Google | Reduce uncertainty | . Playing video games | Achieve status and prestige | . There are many different ways to address the same underlying motive. | Habits are associations between an observed cue and a predicted response, heavily influencing behaviors. Feelings and emotions transform the cues and predictions into applicable signals. | The cause of habits is the preceding prediction, which leads to a feeling. | . | Reframing habits to highlight their benefits (positive) rather than their drawbacks (negative) can change your mindset and make a habit more attractive. A motivation ritual refers to the association of habits with something enjoyable, which can then later be used as a cue for motivation. | Eventually the associated habit becomes a cue to something enjoyable. | . | If you can reprogram your predictions, you can transform a hard habit into an attractive one. | . Chapter 11: Walk Slowly, but Never Backward . Taking action is better than being in motion, practice over planning. You want to delay failure, so you avoid taking action. | It’s better to take action and iteratively improve your results. | Preparation or planning can become a form of procrastination. | The key is to start with repetition, not perfection. | . | Habit formation is the process by which a behavior becomes progressively more automatic through repetition. Long-term Potentiation refers to the strengthening of connections between neurons in the brain based on recent patterns of activity. | Hebb’s Law is “Neurons that fire together wire together.” | . | Repetition is a form of change. To build a habit, you need to practice it. Repeating a habit leads to physical changes in the brain, different adaptations for different kinds of tasks. | Active practice and iteration is more effective than passive learning and theorizing. | Automaticity is the ability to perform a behavior without thinking about each step, which occurs when the nonconscious mind takes over after crossing the habit line. | . | Learning curves plot automaticity vs. time spent, revealing that habits form based on frequency, not time. This means the amount of time you have been performing a habit is less important than the number of times you have performed the habit successfully. | . | . Chapter 12: The Law of Least Effort . It is human nature to follow the Law of Least Effort, which states that when deciding between two similar options, the option that requires less work is more attractive. The most value for the least effort is most attractive. | The most common behaviors are extremely convenient and require little effort, e.g. checking social media or watching TV. | . | Habits are obstacles to desired outcomes. If good habits can be made more convenient, then they are more likely performed. | The idea is to make it as easy as possible in the moment to do things that payoff in the long run. | . | One way to reduce friction is environment design, making cues more obvious and optimizing for convenient actions. Addition by subtraction, or lean production, is the strategy of relentlessly eliminating points of friction that waste time or energy and optimizing to achieve more with less effort. | . | .",
            "url": "https://nhtsai.github.io/notes/atomic-habits",
            "relUrl": "/atomic-habits",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Making a Pull Request",
            "content": "Making a Pull Request . Fork the project you want to work on to get your own copy of the repository. . | Clone your copy of the repository using HTTPS or SSH. . | git clone forked_repo.git . Create a new branch to work on a separate branch. | git checkout origin/master -b fix_bug . Make any modifications to the code, e.g. bug fix, new feature, etc. . | Preview your changes. . | git diff . Check the state of the repository. Files should be modified but untracked. | git status . Track the modified files. | git add file.txt . Commit the changes. | git commit -m &quot;Fixed typo in file.txt&quot; . Check the state of the repository. Files should be modified and changes committed. | git status . Push your new branch up to your remote forked repository. | git push origin HEAD . HEAD is a a shortcut for your current checked-out branch, aka fix_typo. . Open a pull request from your fork. . | Read any contributing guidelines and rules of conduct. . | Review the changes in your pull request. . | Create a pull request if everything looks good. . | References . Anthony Explains #004 Video | .",
            "url": "https://nhtsai.github.io/notes/making-a-pull-request",
            "relUrl": "/making-a-pull-request",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to Apache Cassandra",
            "content": "Introduction to Apache Cassandra . Supplemental Video Lecture . . Relational Overview . Can RDBMS work for big data? . Replication makes ACID a lie Given a read-heavy workload, we want to create a replicated slave node from the master node. However, data is replicated asynchronously, which introduces replication lag. This means that if the client reads from the replicated slave node, old data will be returned and results are not consistent (ACID fails). | . | Third Normal Form doesn’t scale Unpredictable queries | . | Sharding is a nightmare Data is split all over the place, no more joins or aggregations. | Querying second indexes requires hitting every shard (non-performant), so we are forced to denormalize all things. | Keeping shards consistent, adding shards, or changing schemas requires custom scripts. | . | Low Availability Who is responsible for Master failovers? Whether manual or automatic, failovers need to be detected. | . | Multiple datacenter solutions are messy. | Downtime is frequent. | . | . Failure of RDBMS . Scaling is a mess | ACID naive at best (not consistent). | Re-sharding is a manual process. | Denormalize all 3rd normal form queries for performance. | High availability is complicated to achieve, requires additional operational overhead. | . Lessons Learned for a New Solution . Consistency is not practical –&gt; give up consistency. | Manual sharding and rebalancing is hard –&gt; build it into the cluster. | Every moving part makes systems more complex –&gt; simplify the architecture, no more master-slave. | Scaling up is expensive –&gt; only use commodity hardware, more affordable. | Scatter/Gather queries are not good –&gt; denormalize for real-time query performance, always hit 1 machine. | . Cassandra Overview . What is Apache Cassandra? . Fast Distributed Database: | High Availability: able to be accessed at all times | Linear Scalability: linear scale-up in performance | Predictable Performance: can guarantee service-level agreements (SLA) with very low latency | No single point of failure (SPOF): can withstand multiple points of failure | Multi-Datacenter: can be utilized across multiple datacenters out of the box | Commodity Hardware: can be implemented on affordable hardware when vertically scaling | Easy to manage operationally: can manage no matter the cluster size | Not a drop-in replacement for RDBMS: applications must be designed using Cassandra | . Hash Ring Analogy . No master/slave/replica sets | No config servers, zookeeper | Data is partitioned around the ring | Data is replicated to replication factor (RF=N) servers | All nodes hold data and can answer queries (both read &amp; write) | Location of data on ring is determined by partition key Partition key is a hash of the primary key. | . | . CAP Tradeoffs . CAP Theorem says that when a network partition failure happens, we can either: Cancel the operation and thus decrease the availability but ensure consistency | Proceed with the operation and thus provide availability but risk inconsistency | . | Latency between data centers makes consistency impractical, especially considering datacenters across the world. | Cassandra chooses availbility and partition tolerance over consistency. | . Replication . Data is replicated automatically and asynchronously. | Choose total number of servers to replicate data to, replication factor (RF), usually 3. Set when creating the keyspace, a group of Cassandra tables, like schema. | . | Data is always replicated to each replica. | If a machine is down during replication, missing data writes are replayed via hinted handoff. | . Consistency Levels . Consistency level is set on per query basis, can be configured for read or write. | ALL: all (100%) of replicas must confirm read/write to be considered successful. | QUORUM: majority (51%) of replicas must confirm read/write to be considered successful. | ONE: only 1 replica needs to confirm read/write to be considered successful, allows for fast read/write successes. | . Multiple Datacenters . Write to local datacenter (DC) and replicate asynchronously to other datacenters. Local Consistency Levels: QUORUM –&gt; LOCAL QUORUM | . | Replication factor per keyspace per datacenter. Can configure different RF for each datacenter. | . | Datacenters can be physical or logical. Can use one DC as OLTP for fast reads and one virtual DC for OLAP queries. | . | . Cassandra Internals and Choosing a Distribution . Write Path . Writes are written to any node in the cluster, which serves as the coordinator for the write request. | Writes are written to a commit log, an append-only, immutable data structure for durability. | Merges the write mutation to the memtable, an in-memory representation of the table. | Now Cassandra can respond to client about successful write. This simplicity in write path is why Cassandra is fast. | Every write includes a timestamp. | . | When memory is full, the memtable is serialized into an immutable SSTable and flushed to disk periodically. | New memtable is created in memory. | Cassandra never does any updates or deletes in-place. | Deletes are a special write case known as tombstone, a marker with a timestamp to say that there is no data at that location at that time. | . What is an SSTable? . An SSTable is an immutable data file for row storage. | Every write includes a timestamp of when it was written. | Partition is spread across multiple SSTables. | Same column can be in multiple SSTables. | Merged through compaction, taking small SSTables and merging them into bigger ones. Last write wins means for a row with many changes, only latest timestamp is kept so only the latest version of the row is saved. | . | Deletes are written as tombstones. | Easy backups since SSTables can be copied off to another server. | . Read Path . Any server/node may be queried, and it acts as the coordinator. | The coordinator then contacts nodes with the requested key in the read query. | On each node, data is pulled from SSTable(s) on disk and merged using the latest timestamp, like compaction. The disk type (SSD or hard drive) has a large impact on the speed and performance of Cassandra. | . | Consistency levels under ALL performs read repair in the background Sometimes nodes can disagree about the value of a given piece of data since Cassandra is eventually consistent. | read_repair_chance specifies the chance that Cassandra will update/sync information on all other replicas, default is around 10% chance of all reads. | . | . Open Source Distribution . Latest, bleeding edge features, perfect for hacking. | File JIRAs issues and bug reports for support. | Support via mailing list &amp; IRC. | . DataStax Enterprise Distribution . Open source Apache Cassandra at its core. | Focused on stable releases for enterprise. | Integrated Multi-Datacenter Search (for OLTP) | Integrated Spark or Hadoop for Analytics (for OLAP). | Free Startup Program (&lt; 3MM revenue, &lt; 30M funding). | Extended support, additional QA. | .",
            "url": "https://nhtsai.github.io/notes/cassandra-intro",
            "relUrl": "/cassandra-intro",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Learning Object-Specific Distance From a Monocular Image",
            "content": "Learning Object-Specific Distance From a Monocular Image . Research paper by Jing Zhu, Yi Fang, Husam Abu-Haimed, Kuo-Chin Lien, Dongdong Fu, and Junli Gu 1. . Abstract . TODO . Introduction . In computer vision research for self-driving cars, researchers have not recognized the importance of environment perception in favor of more popular tasks, like object classification, detection, and segmentation. Object-specific distance estimation is important for car safety and collision avoidance, and the lack of deep learning applications is likely due to the lack of datasets with distance measures for each object in the scene. . Current self-driving systems predict object distance using the traditional inverse perspective mapping (IPM) algorithm. This method first locates a point on the object (usually on the lower edge of the bounding box), then projects the located point onto a bird’s-eye view coordinate map using camera parameters, and finally estimates the object distance using the constructed bird’s-eye view coordinate map. This simple method can provide reasonable distance estimates for objects close and in front of the camera, but it performs poorly when objects are located to the sides of the camera or curved roads and when objects are over 40 meters away. . The authors first sought “to develop an end-to-end learning based approach that directly predicts distances for given objects in the RGB images.” End-to-end meaning that object detection and distance estimation parameters are trained jointly. The base model extracts features from RGB images, then utilizes region of interest (ROI) pooling to generate a fixed-size feature vector for each object, and feeds the ROI feature vectors into a distance regressor to predict a distance (Z) for each object. However, this method was not sufficiently precise for self-driving. . The authors then created an enhanced model with a keypoint regressor to predict (X,Y) of the 3D keypoint coordinates (X,Y,Z). Leveraging the camera projection matrix, the authors defined a projection loss between the projected 3D point (X,Y,Z) and the ground truth keypoint (X*,Y*,Z*). The keypoint regressor and projection loss are used for training only. During inference, the trained model takes in an image with bounding boxes and outputs the object-specific distance, without any camera parameters invervention. . The authors constructed an extended dataset from the KITTI object detection dataset and the nuScenes mini dataset by “computing the distance for each object using its corresponding LiDAR point cloud and camera parameters.” . Enhanced model performs better at distance estimation, compared to the traditional IPM algorithm and the support vector regressor, is also more precise than the base model, and is twice as fast during inference than IPM algorithm. . Summary . Base model: use deep learning to predict distance from given objects on RGB image without camera parameter intervention | Enhanced model: base model with keypoint regressor and new projection loss | Dataset: extended KITTI and nuScenes mini object-specific distance datasets | . Related Work . Distance Estimation . Inverse Perspective Mapping (IPM): convert a point or a bounding box in the image to the corresponding bird’s-eye view coordinate | Support Vector Regressor: predict object-specific distance given width and height of a bounding box | DistNet: use YOLO for bounding boxes prediction instead of image features learning for distance estimation, the distance regressor studied geometric mapping from bounding box with certain width and height to distance value In contrast, this paper directly predicts distances from learned image features. | . | Marker-based Methods: use auxiliary information, create segmented markers in the image and estimate distance using the marker area and camera parameters | Calibration Patterns: predict physical distance based on rectangular pattern, where 4 image points are needed to compute camera calibration | . 2D Visual Perception . R-CNNs: boost accuracy, decrease processing time for object detection, classfiication, dsegmentation | SSD and YOLO: end-to-end frameworks to detect and classify objects in RGB images | Monocular Depth Estimation: predict dense depth maps for given monocular color images | . Methods . The authors propose a base model that predicts physical, object-specific distance from given RGB images and object bounding boxes and an enhanced model with keypoint regressor for improved distance estimation. . Base Method . . Feature Extractor . Extract feature map for entire RGB image using image feature learning network. | Use existing architecture (e.g. vgg16, resnet50) as feature extractors. | Output of last layer is max-pooled and extracted as feature map for input RGB image. | . Distance Regressor and Classifier . Feed feature map and object bounding boxes into ROI pooling layer to generate fixed-size feature vector Fi boldsymbol{F_i}Fi​ for each object in the image. | Pass pooled object feature vector into distance regressor for predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) and object classifier for predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Distance Regressor: 3 fully-connected (FC) layers, {2048, 512, 1} for vgg16 and {1024, 512, 1} for resnet50. Softplus activation layer applied on output of last FC layer to ensure predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) is positive. | . | Distance Classifier: A FC layer with (number of categories in dataset) neurons, then softmax function to get predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Loss Functions Regressor Loss: . Ldist=1N∑i=1NL1;smooth(D(Fi),di∗)L_{dist}= frac{1}{N} sum_{i=1}^{N} L_{ text{1;smooth}}(D( boldsymbol{F_i}), d^{*}_{i})Ldist​=N1​i=1∑N​L1;smooth​(D(Fi​),di∗​) L1;smooth={0.5(xi−yi)2/β,if ∣xi−yi∣&lt;β∣xi−yi∣−0.5∗β,otherwiseL_{ text{1;smooth}} = begin{cases}0.5(x_{i} - y_{i})^2/ beta, &amp; text{if $|x_{i}-y_{i}|&lt; beta$} |x_{i}-y_{i}|-0.5* beta, &amp; text{otherwise} end{cases}L1;smooth​={0.5(xi​−yi​)2/β,∣xi​−yi​∣−0.5∗β,​if ∣xi​−yi​∣&lt;βotherwise​ | Classifier Loss: . Lcla=1N∑i=1NLcross-entropy(C(Fi),yi∗)L_{cla}= frac{1}{N} sum_{i=1}^{N} L_{ text{cross-entropy}}(C( boldsymbol{F_i}), y^{*}_{i})Lcla​=N1​i=1∑N​Lcross-entropy​(C(Fi​),yi∗​) Lcross-entropy=−∑i=1Myi∗∗log(C(Fi))L_{ text{cross-entropy}}=- sum_{i=1}^{M} y^{*}_{i} * log(C( boldsymbol{F_i}))Lcross-entropy​=−i=1∑M​yi∗​∗log(C(Fi​)) | NNN: number of objects in the image | MMM: number of categories | β betaβ: smooth loss scaling factor, usually 1.01.01.0 | D(Fi)D( boldsymbol{F_i})D(Fi​): predicted distance | C(Fi)C( boldsymbol{F_i})C(Fi​): predicted category label | di∗d^{*}_{i}di∗​: ground truth distance of the iii-th object | yi∗y^{*}_{i}yi∗​: ground truth category label of the iii-th object | . | . Model Learning and Inference . Train feature extractor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}minLbase​=Lcla​+λ1​Ldist​ Set λ1=1.0 lambda_{1}=1.0λ1​=1.0 during training. | . | Use ADAM optimizer with beta value β=0.5 beta=0.5β=0.5, learning rate of 0.001, exponentially decayed after 10 epochs. | Classifier encourages model to learn features used in estimating more accurate distances, only used during training. | After training, base model can predict object-specific distances given RGB images and object bounding boxes as input. | . Enhanced Method . . Add keypoint regressor to optimize base model by introducing projection constraint for better distance prediction. . Feature Extractor . Same architecture as base model: vgg16 or resnet50 into ROI pooling layer for object specific features Fi boldsymbol{F_i}Fi​. | . Keypoint Regressor . Keypoint regressor KKK learns to predict approximate keypoint position in 3D camera coordinate system. | Distance regressor predicts Z coordinate, while keypoint regressor predicts (X, Y). | Keypoint Regressor: 3 fully-connected (FC) layers, {2048, 512, 2} for vgg16 and {1024, 512, 2} for resnet50. | Since there is no ground truth of 3D keypoint available, project the generated 3D point (X,Y,Z)=([K(Fi),D(Fi)])(X,Y,Z)=([K( boldsymbol{F_i}), D( boldsymbol{F_i})])(X,Y,Z)=([K(Fi​),D(Fi​)]) back to image plane using camera projection matrix PPP. | Compute errors between ground truth 2D keypoint ki∗k^{*}_{i}ki∗​ and projected point P⋅([K(Fi),D(Fi)])P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})])P⋅([K(Fi​),D(Fi​)]). | Keypoint Function . L3Dpoint=1N∑i=1N1di∗∥P⋅([K(Fi),D(Fi)])−ki∗∥2L_{3Dpoint}= frac{1}{N} sum_{i=1}^{N} frac{1}{d^{*}_{i}} | P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})]) - k^{*}_{i} |_{2}L3Dpoint​=N1​i=1∑N​di∗​1​∥P⋅([K(Fi​),D(Fi​)])−ki∗​∥2​ Use weight with regard to ground truth distance to encourage better predictions for closer objects. | . | . Distance Regressor and Classifier . Same architecture as base model and training losses LdistL_{dist}Ldist​ and LclaL_{cla}Lcla​. | Distance regressor parameters also optimized by projection loss L3DpointL_{3Dpoint}L3Dpoint​. | . Model Learning and Inference . Train feature extractor, keypoint regressor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist+λ2L3Dpoint text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}+ lambda_{2}L_{3Dpoint}minLbase​=Lcla​+λ1​Ldist​+λ2​L3Dpoint​ Distance loss weight constant: lambda1=10.0lambda_{1}=10.0lambda1​=10.0 | Keypoint loss weight constant: lambda2=0.05lambda_{2}=0.05lambda2​=0.05 | . | Use same optimizer, beta, and learning rate as the base model. | Training Only: use camera projection matrix PPP, keypoint regressor, and object classifier. | Testing: Given RGB image and bounding boxes, directly predicts object-specific distances without any camera parameter intervention. | Both models trained for 20 epochs with batch size of 1 on the training subset, augmented with horizontally-flipped training images. | After training, input RGB image with bounding boxes into trained model to get the output of the distance regressor as the estimated object-specific distance in the validation subset. | . Dataset Construction . KITTI and nuScenes mini both provide RGB images, 2D/3D bounding boxes, category labels for objects in the images, and the corresponding velodyne point cloud for each image. . Object Distance Ground Truth Generation . Use 3D bounding boxes to segment velodyne point cloud for each object. | Sort the segmented points based on depth values. | Extract the n-th depth value as object-specific ground truth distance, where n=0.1∗(number of segmented points)n=0.1*( text{number of segmented points})n=0.1∗(number of segmented points) to avoid extracing depth values from noise points. | Project velodyne points to corresponding RGB image planes and get their image coordinates as keypoint ground truth distance. | Append both ground truths to the object detection dataset labels. | KITTI . Split KITTI into training (3,712 RGB images, 23,841 objects) and validation sets (3,768 RGB images, 25,052 objects) using 1:1 split ratio. | All KITTI objects are categorized into 9 classes, i.e. Car, Cyclist, Pedestrian, Misc, Person_sitting, Tram, Truck, Van, DontCare. | Generated KITTI ground truth distances should be between [0, 80] meters. | . nuScenes mini . Split nuScenes mini into training (200 images, 1,549 objects) and validation sets (199 images, 1,457 objects). | All nuScenes mini objects are categorized into 8 classes, i.e. Car, Bicycle, Pedestrian, Motorcycle, Bus, Trailer, Truck, Construction_vehicle. | Generated nuScenes mini ground truth distances should be between [2, 105] meters. | . Evaluation . Evaluation Metrics . Use the same metrics as depth prediction. Let di∗d^{*}_{i}di∗​ and did_{i}di​ denote the ground truth distance and the predicted distance, respectively. . Threshold . % of di s.t. max(di/di∗,di∗/di)=δ&lt;threshold % text{ of } d_i text{ s.t. max}(d_i/d^{*}_{i},d^{*}_{i}/d_i)= delta&lt; text{threshold}% of di​ s.t. max(di​/di∗​,di∗​/di​)=δ&lt;threshold | Absolute Relative Difference . Abs Rel=1N∑i=1N∣di−di∗∣/di∗ text{Abs Rel}= frac{1}{N} sum_{i=1}^{N}|d_{i}-d^{*}_{i}|/d^{*}_{i}Abs Rel=N1​i=1∑N​∣di​−di∗​∣/di∗​ | Squared Relative Difference . Squa Rel=1N∑i=1N∥di−di∗∥2/di∗ text{Squa Rel}= frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}/d^{*}_{i}Squa Rel=N1​i=1∑N​∥di​−di∗​∥2/di∗​ | RMSE (linear) . RMSElinear=1N∑i=1N∥di−di∗∥2 text{RMSE}_{ text{linear}}= sqrt{ frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}}RMSElinear​=N1​i=1∑N​∥di​−di∗​∥2​ | RMSE (log) . RMSElog=1N∑i=1N∥log⁡di−log⁡di∗∥2 text{RMSE}_{ text{log}}= sqrt{ frac{1}{N} sum_{i=1}^{N} | log{d_{i}}- log{d^{*}_{i}} |^{2}}RMSElog​=N1​i=1∑N​∥logdi​−logdi∗​∥2​ | . Note: don’t include distances predicted for DontCare objects when calculating errors. . Compared Approaches . Inverse Perpective Mapping Algorithm (IPM): predicts object-specific distance by approximating a transformation matrix between a normal RGB image and its bird’s-eye view image using camera parameters Use IPM from MATLAB’s computer vision toolkit to get transformation matrices for RGB images from the validation subset. | Project middle points of the lower edges of the object bounding boxes into bird’s-eye view coordinates using the transformation matrices. | Take values along forward direction as estimated distances. | . | Support Vector Regressor (SVR): predicts object-specific distance given width and height of a bounding box Compute width and height of each bounding box in the training subset. | Train SVR with the ground truth distance. | Input widths and heights of each bounding box in the validation set to get estimated object-specific distances. | . | . Results . Both proposed models have much lower relative errors and higher accuracies, compared to IPM and SVR, on KITTI. | Enhanced model performs the best, implying effectiveness of keypoint regressor and projection constraint, on both KITTI and nuScenes mini. | . References . J. Zhu, Y. Fang, H. Abu-Haimed, K. Lien, D. Fu, and J. Gu. Learning Object-Specific Distance from a Monocular Image. arXiv:1909.04182, 2019. &#8617; . |",
            "url": "https://nhtsai.github.io/notes/distance-estimation",
            "relUrl": "/distance-estimation",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a university graduate interested in data science and machine learning. Currently looking for internship and job opportunities. .",
          "url": "https://nhtsai.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nhtsai.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}