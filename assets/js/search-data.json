{
  
    
        "post0": {
            "title": "JSON",
            "content": "JSON . Video | W3 Tutorial | . JSON (JavaScript Object Notation) . * lightweight data representation used to send data between computers * Commonly used for APIS and configs * Lightweight and Easy to Read/Write * Integrates with Javascript (JSON is a superset of Javascript) and most languages . JSON Types . Strings: “Hello World”, “I”, double quotes only | Numbers: integers, floating point numbers, scientific notation, 10, 1.5, -30, 1.2e10 | Booleans: true, false | null: null | Arrays: [1, 2, 3], [“Hello”, “World”], access index using brackets | Objects: {key: value}, keys must be strings, value can be any JSON type, get values using {}.key, {“Age”: 30} | Cannot use a function, a date, undefined | . Syntax . Derived from JS object notation, but JSON format is text only | Originally specified by Douglas Crockford | Usually an array or object as top-level of JSON file | . { &quot;name&quot;: &quot;Nathan&quot;, &quot;number&quot;: 3, &quot;isProgrammer&quot;: true, &quot;hobbies&quot;: [&quot;Weight Lifting&quot;, &quot;Community Service&quot;], &quot;friends&quot;: [ { &quot;name&quot;: &quot;Matthew&quot;, &quot;number&quot;: 22, &quot;isProgrammer&quot;: true, &quot;hobbies&quot;: [&quot;Biking&quot;, &quot;Relaxing&quot;], &quot;friends&quot;: null } ] } . JSON vs XML . Similarities “self-describing”, human-readable | hierarchical | parsed by many programming languages | fetched using XMLHttpRequest | . | Differences JSON doesn’t use closing tags | JSON is more succinct | JSON is quicker to read and write | JSON can use arrays | JSON can be parsed by standard JS function, while XML has to be parsed with XML parser | . | Advantages of JSON Faster and easier than XML | Parsed into ready to use JS object using JSON.parse | . | . { &quot;employees&quot;: [ { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Doe&quot; }, { &quot;firstName&quot;: &quot;Anna&quot;, &quot;lastName&quot;: &quot;Smith&quot; }, { &quot;firstName&quot;: &quot;Peter&quot;, &quot;lastName&quot;: &quot;Jones&quot; } ] } . &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; .",
            "url": "https://nhtsai.github.io/notes/json",
            "relUrl": "/json",
            "date": " • Sep 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Probability",
            "content": "Harvard STAT 110: Introduction to Probability, Fall 2011 . Course Resources . Professor Joe Blitzstein | Course Lectures | Course Website | Course Syllabus | Course EdX | Course Textbook: Introduction to Probability, 2 ed. by Blitzstein &amp; Hwang | Course focuses on using probability as a language and set of tools for understanding statistics, science, risk, and randomness. | Topics: sample spaces/events, conditional probability, Bayes’ Theorem, univariate distributions, multivariate distributions, limit laws, Markov chains | . 1. Probability, Counting . Probability has its roots in gambling and is an important part of many disciplines today . | Sample Space: set of all possible outcomes of an experiment Experiment: anything with certain possible outcomes that are unknown beforehand | Event: a subset of the sample space | Example: The experiment of rolling 2 dice has 36 total outcomes in the sample space, and an event could be the subset of all outcomes that sum up to a number | . | Naive Definition of Probability P(A)=# favorable outcomes# possible outcomesP(A)= frac{ text{ # favorable outcomes}}{ text{ # possible outcomes}}P(A)=# possible outcomes# favorable outcomes​ | Example: Flipping a fair coin twice, what is probability that both flips are heads? 1 favorable outcome: $HH$ | 4 total outcomes: $HH, HT, TH, TT$ | Therefore, $P( text{both heads})= frac{1}{4}$ | . | Assumes that all outcomes are equally likely and a finite sample space If the coin was not fair, the outcomes would not be equally likely | If the sample space is infinite, the probability definition doesn’t make sense | . | . . Multiplication Rule If we have an experiment with $n_1$ possible outcomes, and for each outcome of $1 text{st}$ experiment, there are $n_2$ possible outcomes for the $2 text{nd}$ experiment, $ ldots$, and for each outcome of $(r-1) text{th}$ experiment, there are $n_r$ possible outcomes for the $r text{th}$ experiment, then there are $n_1 * n_2 * ldots * n_r$ overall possible outcomes. | The total number of possible outcomes of a series of experiments is the product of each experiment’s outcomes. | . | Example: 3 flavors of ice cream and 2 types of cones Ice cream flavor experiment has 3 outcomes (chocolate, vanilla, strawberry) | Waffle cone experiment has 2 outcomes (cake, waffle) | There are $6 text{ possible outcomes}= 2 times 3 = 3 times 2 $ , order of cone/ice cream choice does not matter | . | . . Binomial Coefficient (nk)=n!(n−k)!k!,0 if k&gt;n binom{n}{k}= frac{n!}{(n-k)!k!}, text{0 if k&gt;n}(kn​)=(n−k)!k!n!​,0 if k&gt;n Number of subsets of size k given n elements, where order does not matter | Pronounced “n choose k”, choosing k elements of n possible elements, where order does not matter | Intuition from multiplication rule Choose K elements: $n(n-1)(n-2) ldots (n-k+1)$ | In any order: divide by $k!$ because we over-counted by that factor | Therefore, $ frac{n(n-1)(n-2) ldots (n-k+1)}{k!}= frac{n!}{(n-k)!k!}$ | . | . | Example: Find probability of full house in poker, 5 card hand A full house is 3 cards of 1 rank and 2 cards of another rank, e.g. 3 sevens and 2 tens | Assume deck is randomly shuffled so that all sets of 5 cards are equally likely Because we assume cards are evenly shuffled, we can justify using naive definition | . | Number of possible hands: $ binom{52}{5}$ , “52 choose 5” | Favorable hands (full house): $ binom{13}{1} binom{4}{3} times binom{12}{1} binom{4}{2}$ Of 13 ranks choose 1 and of 4 cards need 3, then of 12 remaining ranks choose 1 and of 4 cards we need 2 | . | Therefore, $ frac{ binom{13}{1} binom{4}{3} times binom{12}{1} binom{4}{2}}{ binom{52}{5}}$ | . | . . Sampling Table: choose $k$ elements out of $n$ total elements We want to know how many ways there are of doing this, which should be immediate from the multiplication rule | We can choose to sample with replacement (we can choose the same element again) or without replacement (we cannot choose the same element again) . | With replacement, order matters Have $k$ experiments, each with $n$ outcomes and order matters, so ${1, 2} neq {2, 1}$ | . | . n1×n2×…×nk=nkn_1 times n_2 times ldots times n_k = n^kn1​×n2​×…×nk​=nk Without replacement, order matters Have $k$ experiments, with $n$ outcomes without replacement and order matters, so ${1, 2} neq {2, 1}$ | . | . n×(n−1)×…×(n−k+1)n times (n-1) times ldots times (n-k+1)n×(n−1)×…×(n−k+1) Without replacement, order does not matter Have $k$ experiments, with $n$ outcomes without replacement and order does not matter, so ${1, 2} = {2, 1}$ | Want 1 combination of $k$ elements to represent all $k!$ permutations with the same $k$ elements, so need to divide by $k!$ | Think of grouping all possible permutations with the same $k$ elements into $k!$ groups For the $k=3$ choice $(1,2,3)$, there are $3!$ orderings that we want to group into 1 representation ${1,2,3}$ | To do this grouping for all different choices, we divide by $k!$ so that we’re left with the number of unique combinations of $k=3$ elements | . | . | . n×(n−1)×…×(n−k+1)k×(k−1)×…×1=n!(n−k)!×k!=(nk) frac{n times (n-1) times ldots times (n-k+1)}{k times (k-1) times ldots times 1} = frac{n!}{(n-k)! times k!} = binom{n}{k}k×(k−1)×…×1n×(n−1)×…×(n−k+1)​=(n−k)!×k!n!​=(kn​) With replacement, order does not matter Hardest to understand, not obvious from multiplication rule | Have $k$ experiments, with $n$ outcomes and order does not matter, so ${1, 2} = {2, 1}$ | . | . (n+k−1)!(n−1)!×k!=(n+k−1n−1)=(n+k−1k) frac{(n+k-1)!}{(n-1)! times k!} = binom{n+k-1}{n-1} = binom{n+k-1}{k}(n−1)!×k!(n+k−1)!​=(n−1n+k−1​)=(kn+k−1​)   Order Matters Order Doesn’t Matter . With Replacement | $n^k$ | $ binom{n+k-1}{k}$ | . Without Replacement | $nPk = frac{n!}{(n-k)!}$ | $ binom{n}{k} = frac{n!}{(n-k)!k!}$ | . | . 2. Story Proofs, Axioms of Probability . General Advice for Homework Don’t lose common sense | Do check answers, especially by doing simple and extreme cases | Label people, objects, etc. (If we have n people, label them 1, 2, …, n) | . | . . Pick k times from a set of n objects, where order does not matter and with replacement Prove the answer is $ binom{n+k-1}{k}$ ways | Example: $k=0, binom{n-1}{0} = 1$ Only 1 way to choose 0 elements from n-1 elements, choose none! | . | Example: $k=1, binom{n}{1} = n$ When only picking 1 element, no difference if with/without replacement or if order matters or does not matter | . | Example: $n=2, binom{k+1}{k} = binom{k+1}{1} = k+1$ They are equal because: If we choose $k$ of $k+1$ elements, then the remaining is the 1. If we choose 1, then the remaining is the $k+1$ | Choosing k elements from 2 total elements with replacement and order doesn’t matter If we get the first element $x$ times, we know we got the second element $k-x$ times | Number of possible first elements we get is from ${0, ldots, k}$ , so there are $k+1$ possibilities of choosing $k$ from $n=2$ elements | . | Can think of this as flipping a fair coin twice | . | Equivalent: how many ways are there to put k indistinguishable elements into n distinguishable groups? Say $n=4,k=6$, a possible way is (***),(),(**),(*), or *** |   | ** | * | . | There must be $k$ stars and $n-1$ separators, we just need to choose $k$ positions for stars in $n+k-1$ total positions, or $ binom{n+k-1}{k} = binom{n+k-1}{n-1}$ | . | Bose-Einstein Bose said flipping a fair coin twice had 3 equally likely outcomes ${HH, TT, 1H+1T}$ | Bose thought of the two coin flips as indistinguishable, rather than ${HT, TH}$ | Thinking of coins as indistinguishable led to the discovery Bose-Einstein condensate | . | . | . . Story Proof: proof by interpretation, rather than by algebra or calculus Example: $ binom{n}{k} = binom{n}{n-k}$ Think about what this means in terms of choosing k elements rather than calculating factorials | . | Example: $n binom{n-1}{k-1} = k binom{n}{k}$ Imagine we pick k of n people, with one of them designated as the president | How many ways can we do this? | Approach 1: choose the president, then choose k-1 of n-1 people to form the rest of the group (left side) | Approach 2: choose $k$ of $n$ people, then select $1$ of the $k$ people to be the president (right side) | Both approaches count the same thing in different ways | . | Example: $ binom{m+n}{k}= Sigma_{j=0}^{k} binom{m}{j} binom{n}{k-j}$ (Vandermonde’s Identity) Imagine a group of $m$ people and a group of $n$ people, and we want to select $k$ people total from both groups | One way is we choose $j$ from group $m$ and $k-j$ from group $n$ | How many ways are there to choose $j$ from group $m$ and $k-j$ from group $n$? $ binom{m}{j} binom{n}{k-j}$ ways, using multiplication rule to get total number of ways for this $j$ | . | Then, we add those disjoint cases up to get the total number of ways! | . | . | . . Non-Naive Definition of Probability Probability Space: $S$ and $P$, where S is sample space, or the set of all possible outcomes of an experiment | P is a function that maps Input (Domain): any event A, which is a subset $A subseteq S$ | Output (Range): $P(A) in [0,1]$ a number between 0 and 1 | . | such that $P( emptyset)=0, P(S)=1$ Probability of empty set is 0 and probability of full set is 1 | What does it mean for the empty set to occur? If we get an outcome $S_0$ , if $S_0 in A$, then we say A occurred, else $A$ did not occur | If the empty set occurred, then $S_0 in emptyset$ , but nothing is in the empty set, so it cannot occur. | We want impossible events to be impossible $P( emptyset) = 0$ | . | Why is an outcome in S a certainty? $S_0$ must be in $S$ because $S$ represents the universe of all outcomes | . | . | $P( cup_{n=1}^{ infty} A_n) = Sigma_{n=1}^{ infty}P(A_n), text{ if } A_1,A_2, ldots text{ are disjoint/non-overlapping}$ Probability of the countably infinitely many union equals the sum of the probabilities if the events $A_1,A_2, ldots$ are disjoint/non-overlapping | . | . | . | Everything in probability basically derives from this definition with the two Axioms of Probability | . | . 3. Birthday Problem, Properties of Probability . * . 4. Conditional Probability I . 5. Conditional Probability II, Law of Total Probability . 6. Monty Hall, Simpson’s Paradox . 7. Gambler’s Ruin, Random Variables . 8. Random Variables and Their Distributions . 9. Expectation I, Indicator Random Variables, Linearity . 10. Expectation II . 11. Poisson Distribution . 12. Discrete vs. Continuous, Uniform Distribution . 13. Normal Distribution . 14. Location, Scale, LOTUS . 15. Midterm Review . 16. Exponential Distribution . 17. Moment Generating Functions I . 18. Moment Generating Functions II . 19. Joint, Conditional, Marginal Distributions . 20. Multinomial, Cauchy Distributions . 21. Covariance, Correlation . 22. Transformations, Convolutions . 23. Beta Distribution . 24. Gamma Distribution, Poisson Process . 25. Order Statistics, Conditional Expectation I . 26. Conditional Expectation II . 27. Conditional Expectation Given R.V. . 28. Inequalities . 29. Law of Large Numbers, Central Limit Theorem . 30. Chi-Square, Student-T, Multivariate Normal . 31. Markov Chains I . 32. Markov Chains II . 33. Markov Chains III . 34. A Look Ahead . References .",
            "url": "https://nhtsai.github.io/notes/probability",
            "relUrl": "/probability",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Database System Concepts",
            "content": "Database System Concepts by Silberschatz, Korth &amp; Sudarshan . Book Website | Online Copy | Student-friendly Source | . 1. Introduction to Database Systems . Database: collection of data that contains information relevant to an enterprise | Database-management system (DBMS): collection of interrelated data and set of programs to access those data Primary goal is to provide a convenient and efficient way to store and retrieve database information | Designed to manage large bodies of information, define structures for storage, provide ways to manipulate information, ensure safety of information in failures or breaches, avoid possible anomalous results | . | Database-System Applications Used to manage collections of data that are highly valuable, relatively large, and accessed by multiple users and applications, often simultaneously | Early database applications had simple, structured data, but modern database applications can use complex data with complex relationships | Databases are essential to every enterprise Sales: customer, product, purchase information | HR: employee information, salaries, payroll taxes, benefits | Banking: customer information, accounts, loans, transactions | Universities: student information, course registration, grades | . | Two modes of database usage Online transaction processing: large number of users retrieve small amounts of data and perform small updates | Data analytics: processing of data to draw conclusions and infer rules or decision procedures to drive business decisions Creating predictive models that take in attributes to output predictions | Data Mining combines knowledge discovery techniques from AI and statistics to implement efficient database operations | . | . | . | Purpose of Database Systems Created to manage commercial data through software | File-processing system: early form of database system in which files stored permanent records and used different applications to manipulate records to appropriate files | Issues with file-processing systems Data redundancy: copies of the same data appear in multiple files | Data inconsistency: copies of same data may no longer agree | Data access: difficulty of parsing entire files or writing custom applications for retrieving data | Data isolation: difficulty of working with data in various files in different formats | Data integrity: data values must satisfy consistency constraints | Data atomicity: upon failure, data should be restored to consistent state that existed prior to failure, either changes occur or not at all | Data concurrency-access: simultaneous manipulation of data without supervision may create inconsistent or invalid data | Data security: data should only be accessed by appropriate users with permission | . | . | View of Data Database system provides abstract view of the data, hiding implementation details of how data are stored and maintained | Levels of Data Abstraction, from lowest to highest Physical level: describes how the data are actually stored in complex, low-level data structures | Logical level: describes what data are stored in database and relationships among data in simple structures Physical data independence: logical level abstraction, or abstracting the complex, physical-level structures as relatively simple, logical level structures | . | View level: describes only part of entire database, simplifying the database to only the parts users are interested in Though logical level abstraction uses simpler structures, complexity comes from variety of information stored in a large database | System may provide many views for the same database | . | . | Example: Consider Instructor(id, name dept_name, salary), Department(dept_name, building, budget), Course(course_id, title, dept_name, credits), and Student(id, name, dept_name, total_credits) Physical level: records are stored in blocks of consecutive storage locations These details are hidden to programmers but important for database administrators | . | Logical level: each record has type definitions and relations to other records | View level: parts of the database are presented without logical level details in applications to users, who have the correct permissions | . | Instances and Schemas Instance: collection of information stored in database at a particular moment | Schema: overall design of the database, not changed or changed infrequently | Physical Schema: database design at the physical level, can be changed without affecting applications | Logical Schema: database design at the logical level, most important to programmers | Subschema: schemas at the view level that describe different views of the database | Physical data independence: not dependent on physical schema, no rewrites needed if physical schema changes | . | Data Models Data Model: collection of conceptual tools for describing data, relationships, semantics, and consistency constraints Describes design of database at physical, logical, and view levels | . | There are four categories of data models | Relational Model: uses a collection of tables, or relations, to represent both data and relationships among those data Each relation contains records with fields or attributes determined by its particular type | . | Entity-Relationship (E-R) Model: uses a collection of basic objects, or entities, and relationships among these objects, important for database design | Object-Based Data Model: uses object-oriented concepts of encapsulation, functions/methods, and object identity to extend entity-relationship model Object-Relational model combines object-oriented data model and relational data model | . | Semi-structured Data Model: allows data items of the same type to have different sets of attributes, often represented in Extensible Markup Language (XML) | Other obsolete data models include network data model and hierarchical data model | . | . | Database Languages Data-Manipulation Language (DML): language used to access or manipulate data as organized by appropriate data model, includes retrieval, insertion, deletion, modification Procedural DML: user specifies what data are needed and how to get those data | Non-Procedural or Declarative DML: user specifies what data are needed but not how to get those data | Query: statement requesting information retrieval, using the query language of a DML | . | Data-Definition Language (DDL): language used to specify database schema and additional properties of the data Data Storage and Definition Language: language used to specify storage structure and access methods used by database system | Consistency Constraints: constraints of data values stored in database, e.g. account balance must never be negative Usually checked every time a record is updated | . | Consistency constraints are implemented using integrity constraints that can be tested with minimal overhead | Domain Constraints: attribute must be of a certain domain (data type) | Referential Integrity: value that appears in one relation for a set of attributes also appears in another relation for the same set of attributes E.g. the dept_name of a course in the Course table must exist in the dept_name column of the Department table | Violations of referential integrity are caused by database manipulations, usually handled by rejecting the manipulation action that caused the violation | . | Assertions: any condition the database must always satisfy, every manipulation of the database is tested by the system for assertion validity | Authorization: expresses differentiation of users and their access permissions on various data values Read Authorization: allows reading but not modification | Insert Authorization: allows insertion but not modification | Update Authorization: allows modification but not deletion | Delete Authorization: allow deletion of data | Users can be assigned any combination of these permissions | . | DDL output is placed a data dictionary that contains metadata, or data about data Data dictionary: special type of table only internally accessible by database system, consulted before reading or modifying actual data | . | . | . | Relational Databases Relational Database: based on relational model, uses collection of tables to represent both data and relationships among those data, includes DML and DDL | Table/Record: multiple columns, each with a unique column name Record based model: database is structured in fixed-format records of several types Each table contains records of a particular type | Each record type defines a fixed number of attributes | Columns of table correspond to attributes of record type | . | Most basic table: CSV file However, unnecessarily duplicated information is a problem | . | . | SQL: non-procedural query language that includes DML and DDL | . -- DML for query SELECT Instructor.id, Department.dept_name FROM Instructor, Department WHERE Instructor.dept_name = Department.dept_name AND Department.budget &gt; 95000; -- DDL for schema CREATE TABLE department ( dept_name char (20), building char (15), budget numeric (12, 2) ); . Application Program: program that interacts with databases through using host language with embedded SQL queries To execute DML queries from host language, can either: Use API to send queries to database | Extend host language syntax to embed DML calls within host language program, uses DML precompiler for conversion | . | DML precompiler: converts DML statements to normal procedure calls in the host language | . | . | Database Design Conceptual-Design Phase: what attributes database should have and how to group these attributes to form the various fields Characterize fully the data needs of prospective database users to produce specification of user requirements | Choose data model, apply concepts of chosen data model to translate user requirements into a conceptual schema of the database Focus on describing data and their relationships, instead of worrying about low-level, physical implementation details | Can form tables using either E-R model or normalization algorithms | . | Review conceptual schema satisfies data requirements without conflicts or redundant features and satisfies functional requirements of project | Specification of Functional Requirements: users describe the kinds of operations/transactions that will be performed on the data | . | Logical-Design Phase: map high-level conceptual schema onto implementation data model of database system that will be used | Physical-Design Phase: specify physical features, including file organization and internal storage structures . | Entity-Relationship Model: collection of entities and their relationships Entity: distinguishable objects that are described by a set of attributes, e.g. Department(dept_name, building, budget). Entity Set: set of all entities of the same type | . | Relationship: association among several entities Relationship Set: set of all relationships of the same type | . | Overall logical structure, or schema, of a data base can be expressed graphically by an Entity-Relationship (E-R) Diagram | E-R diagrams are usually drawn using the Unified Modeling Language (UML) Entity sets are rectangular box with entity set name header and attributes listed below | Relationship sets are diamond with relationship name connecting a pair of related entity sets | Mapping Cardinalities: a constraint that expresses the number of entities to which another entity can be associated via a relationship set E.g. an instructor must be associated with only a single department, but a department can have many instructors | . | . | . | Normalization: process of generating a set of relation schemas to store information without unnecessary redundancy and to easily retrieve information Design schemas in an appropriate normal form using functional dependencies to determine desirable forms | Avoids repetition of information Moving repeated data across multiple rows into a single row of a different table and adding a relationship | . | Avoids inability to represent certain information Use null values to indicate unknown or missing values if inserting rows with incomplete information. | . | . | . | Database Engine Components of database system: storage manager, query processor, transaction management | Storage Manager: provides interface between low-level storage and queries from application programs Authorization and Integrity Manager: tests satisfaction of integrity constraints and checks authority of users who access the data | Transaction Manager: ensures database remains in consistent or correct state despite system failures and concurrent transaction executions proceed without conflicts, consists of concurrency-control manager and recovery manager Allows developers to treat a sequence of database accesses as if they were a single unit that either happens in its entirety or not at all | Abstracts away lower level details of concurrent access and system failures | . | File Manager: manages allocation of space on disk storage and data structures used to represent information on disk | Buffer Manager: fetches data from disk storage into main memory, decides what data to cache in main memory Enables database to handle data sizes that are larger than main memory size | . | . | Data structures implemented by storage manager Data Files: stores database itself | Data Dictionary: stores metadata about structure of the database, its schema | Indices: provides fast access to data items | . | Query Processor: helps simplify and facilitate access to data, translating non-procedural queries at the logical level to an efficient sequence of operations at the physical level DDL Interpreter: interprets DDL statements and records definitions in data dictionary | DML Compiler: translates DML statements in query language into evaluation plan of low-level instructions for query evaluation engine Query can be translated into many different evaluation plans with same result | Query Optimization: chooses the lowest cost evaluation plan for the query | . | Query Evaluation Engine: executes low-level instructions generated by DML compiler | . | Transaction Management Atomicity: operations that form a single logical unit of work happen in its entirety or not at all E.g. funds transfer by removing from A and adding to B should either happen together or not at all | . | Consistency: operations should preserve correctness of values E.g. the sums of balances between A and B should be preserved | . | Durability: changes as a result of successful operations should be persistent despite system failure E.g. new balances after funds transfer should persist even if the application crashes | . | Transaction: collection of operations that performs a single logical function in a database application A transaction is a unit of both atomicity and consistency, so it cannot violate any consistency constraints, though transaction execution can create temporary inconsistency | Programmer should properly define transactions that preserve consistency | E.g. a funds transfer should consist of debiting account A then crediting account B, both operations together form a transaction and should not be separate | . | Recovery Manager: ensures atomicity and durability properties Atomicity is achieved if no failures and all transactions complete successfully | But if transaction is incomplete due to failure, it should have no effect on the database | Failure Recovery: detects system failures and restores database to state prior to failure | . | Concurrency-Control Manager: controls interaction among concurrent transactions to ensure consistency | . | . | Database and Application Architecture | . . Shared-Memory architectures have multiple CPUs and exploit parallel processing using a common shared memory | Parallel databases are designed to run on a cluster consisting of multiple machines | Distributed databases allow data storage and query processing across multiple geographically separated machines | Two-Tier Architecture: application in client machine and queries database system in server machine | Three-Tier Architecture: client machine acts as front-end and does not contain direct database calls, communicates with application server to query database system Business Logic: specifies which actions to carry out under what conditions, embedded in central application server to handle multiple client front-ends | Better security and performance than two-tier architecture | . | Database Users and Administrators Naive User Uses predefined user interfaces, usually a forms interface where users can fill out fields of the form | Views reports generated from database | E.g. student registers for courses using registration website, application verifies user identity and provides form that can query the database to enroll | . | Application Programmer Computer professional who writes application programs | Chooses from many tools to develop user interfaces | . | Sophisticated User Interacts with system without writing programs | Forms requests using database query language or data software | E.g. analysts submit queries to explore data | . | Database Administrator (DBA): central control over DBMS, or data and programs that access those data Schema definition: defines original schema using DDL statements | Storage structure and access-method definition: specifies parameters for the organization of data and indices | Schema and physical-organization modification: changes schema and physical organization of data to reflect changing needs or improve performance | Granting of authorization for data access: regulates access of database by changing authorization information kept in an internal, special structure of the DBMS | Routine maintenance: periodic backups to remote servers, ensuring enough free disk space or upgrading disk space, monitoring performance across workload | . | . | History of Database Systems 1950s to early 1960s Magnetic tapes allowed sequential storage of data | Data processed by reading from the master tape and writing to another, creating a new master tape | . | Late 1960s to early 1970s Hard disks allowed direct access to any position in data | Network and hierarchical data models developed | Codd (1970) defines relational model and non-procedural querying | . | Late 1970s and 1980s IBM’s System R prototype leads to efficient relational DBMSs IBM’s SQL/DS, UC Berkeley’s Ingres, and first version of Oracle | Relational models eventually overtake other data models | Research on parallel, distributed, and object-oriented databases | . | 1990s Creation of SQL language for query-intensive workloads | Web interfaces for databases | High transaction-processing rates and high reliability | . | 2000s Semi-structured, graph, and spatial databases | XML and JSON data-exchange formats | Open-source PostgreSQL and MySQL | Auto-admin features for automatic reconfiguration | Column-stores for data analytics and data mining, | Map-reduce parallelism programming framework | NoSQL systems for lightweight data management for new types of data and eventual consistency allowed for greater scalability and availability | . | 2010s NoSQL systems develop stricter consistency and higher levels of abstractions | Cloud services allowed outsourcing of data storage and applications, software as a service | Data privacy regulations introduced | . | . | . 2. Introduction to the Relational Model .",
            "url": "https://nhtsai.github.io/notes/database-system-concepts",
            "relUrl": "/database-system-concepts",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Linear Algebra",
            "content": "MIT 18.06SC: Linear Algebra, Fall 2011 . Course Resources . Professor Gilbert Strang | Course Lectures | Course Website | Course Syllabus | . 1. The Geometry of Linear Equations . Lecture Summary | . The fundamental problem of linear algebra is to solve a system of equations. This is represented by Ax=bA boldsymbol{x} = boldsymbol{b}Ax=b. . 2-variable example: . 2x−y=0−x+2y=3 begin{aligned} 2x-y &amp;= 0 -x+2y &amp;= 3 end{aligned}2x−y−x+2y​=0=3​ . In the Row Picture method, we view the system of equations as separate lines on the xy-plane. The solution is (1,2)(1, 2)(1,2), the point where the two lines intersect. . In the Column Picture method, we view the system of equations as a linear combination of column vectors. The solution is x=1,y=2x=1,y=2x=1,y=2, the coefficients of column vectors that linearly combine to create b boldsymbol{b}b. . x[2−1]+y[−12]=[03]x begin{bmatrix} 2 -1 end{bmatrix} + y begin{bmatrix} -1 2 end{bmatrix} = begin{bmatrix} 0 3 end{bmatrix}x[2−1​]+y[−12​]=[03​] . In the Matrix Form method, we view the system of equations as a matrix multiplication. . Ax=bA boldsymbol{x} = boldsymbol{b}Ax=b [2−1−12][xy]=[03] begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix} begin{bmatrix} x y end{bmatrix} = begin{bmatrix} 0 3 end{bmatrix}[2−1​−12​][xy​]=[03​] . where A=[2−1−12]A = begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix}A=[2−1​−12​] is the coefficient matrix, x=[xy] boldsymbol{x}= begin{bmatrix} x y end{bmatrix}x=[xy​], and b=[03] boldsymbol{b}= begin{bmatrix} 0 3 end{bmatrix}b=[03​] . 3-variable example: . {2x−y=0−x+2y=−1−3y+4z=4 left { begin{array}{rl} 2x-y &amp;= 0 -x+2y &amp;= -1 -3y+4z &amp;= 4 end{array} right.⎩⎪⎨⎪⎧​2x−y−x+2y−3y+4z​=0=−1=4​ . 2. Elimination with Matrices . References .",
            "url": "https://nhtsai.github.io/notes/linear-algebra",
            "relUrl": "/linear-algebra",
            "date": " • Sep 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Azure Fundamentals",
            "content": "AZ-900: Microsoft Azure Fundamentals Certification Course . Course Video | Content Outline 15-25%: Cloud Concepts | 30-35%: Azure Core Services | 25-30%: Security, Privacy, Compliance, and Trust | 20-25%: Pricing and Support | . | Need to get around 700/1000 or 70% to pass. | Exam has 40-60 questions, can get 12-18 questions wrong. | Question types: multiple choice, multiple answer, drag and drop, hot area (multiple drop-down menus) | Exam time is 60 min, seat time is 90 min. | Valid for 24 months/2 years before re-certification. | . Cloud Concepts . Cloud Computing: practice of using a network of remote servers hosted on Internet to store, manage, and process data, rather than a local server or a personal computer On-Premise: you own the server, manage the infrastructure, own all the risk | Cloud Providers: servers are owned and managed by other parties | . | Dedicated Servers: one physical machine dedicated to a single business Runs a single website or application | Very expensive, high maintenance, high security | . | Virtual Private Server: one physical machine dedicated to a single business virtualized into sub-machines to run multiple websites or applications | . | Shared Hosting: one physical machine shared by hundreds of businesses relies on most tenants under-utilizing their resources | very cheap, very limited | . | Cloud Hosting: multiple physical machines that act as one system abstracted into multiple cloud services | flexible, scalable, secure, cost-effective, high configurability | . | Most Common Infrastructure as a Service (IaaS) Cloud Services Compute: a virtual machine that can run applications, programs, and code | Storage: a virtual hard-drive that can store files | Networking: a virtual network that can define internet connections or network isolations | Databases: a virtual database for storing reporting data or general purpose application use | . | Benefits of Cloud Computing Cost Effective: pay for what you consume, no upfront cost | Global: launch workloads anywhere in the world | Secure: cloud services can be secure by default, handled by provider | Reliable: data backup, disaster recover, replication, fault tolerance | Scalable: adapt resources and services based on demand | Elastic: automate scaling | Current: underlying software is upgraded by provider | . | Types of Cloud Computing Software as a Service (SaaS): product that is run and managed by a service provider, for consumers Salesforce, Gmail, Office 365, other software in the cloud | . | Platform as a Service (PaaS): focus on deployment and management of your apps, for developers AWS Elastic Beanstalk, Heroku, Google App Engine | . | Infrastructure as a Service (IaaS): cloud IT, provides access to networking features, computers, and data storage space, for administrators Microsoft Azure, AWS, Oracle Cloud | . | . | Types of Cloud Computing Responsibilities Customer is responsible | . On-Premise IaaS PaaS SaaS . Applications | Applications | **Applications** | Applications | . Data | Data | Data | Data | . Runtime | Runtime | Runtime | Runtime | . Middleware | Middleware | Middleware | Middleware | . OS | OS | OS | OS | . Virtualization | Virtualization | Virtualization | Virtualization | . Servers | Servers | Servers | Servers | . Storage | Storage | Storage | Storage | . For on-premise, customer is responsible for everything! | . | . Microsoft Azure . Tech company based in Redmond, Washington that makes software, devices, cloud services, search engines. Most known for Windows Operating System | . | Azure is the cloud service provider (CSP) from Microsoft Means “bright blue color of the cloudless sky” | . | Azure Deployment Models .   Cost Security Configuration Technical Knowledge   . Public Cloud | everything built on cloud provider, cloud native | most cost-effective | security controls by default, may not meet your security requirements | configuration limited by provider | no in-depth knowledge needed | . Private Cloud | everything built on company’s data centers, on-premise | most-expensive | no security guarantee, can meet your security requirements | configurable anyway | in-depth knowledge needed to configure private infrastructure | . Hybrid Cloud | using both on-premise and cloud-native from cloud service provider | depends on cloud usage | need to secure, can meet your security requirements | best of both words | in-depth knowledge needed to configure private infrastructure and of cloud services | . Cross Cloud | using multiple cloud providers, multi-cloud | security controls spread across providers | configurations limited across providers | in-depth knowledge needed of multiple cloud providers |   | . | Total Cost of Ownership (TCO) On-premise (CAPEX): software license fees, implementation, configuration, training, physical security, hardware, IT personnel, maintenance | Azure/Cloud (OPEX): subscription fees, implementation, configuration, training | Usually save 75% by moving to cloud services | . | Capital vs. Operational Expenditure Capital Expenditure (CAPEX): spending money upfront on physical infrastructure, deduct that expense from tax bill over time, on-premise business model Server costs | Storage costs | Network costs | Backup and Archive costs | Disaster recovery costs | Data center costs | Technical personnel costs | Need to guess upfront what you plan to spend | . | Operational Expenditure (OPEX): physical costs are shifted to cloud service provider, customer handles non-physical costs Software lease and customization costs | Cloud training costs | Cloud support costs | Cloud metric billing costs (compute, storage) | Can try a product or service without investing in equipment | . | . | Availability: ability to ensure service remains available High Availability: ensure no single point of failure and/or ensure a certain level of performance | Having multiple data centers in multiple availability zones (AZs) ensures availability | Load Balancer: evenly distributes traffic to multiple servers in a data center, rerouting traffic in case of server failure | Azure Load Balancer | . | Scalability: ability to grow rapidly or unimpeded High Scalability: increase capacity based on increased demand of traffic, memory, and compute power | Vertical Scaling: upgrading a single machine, scaling up | Horizontal Scaling: adding additional servers of the same size, scaling out | . | Elasticity: ability to shrink and grow to meet demand High Elasticity: automatically change capacity based on current demand of traffic, memory, and compute power | Use horizontal scaling to scale in or scale out machines | Generally, high elasticity is hard to accomplish with vertical scaling | Azure VM Scale Sets, SQL Server Stretch Database | . | Fault Tolerance: ability to prevent or handle failures | Disaster Recovery: ability to recover from failures, high durability High Durability: ability to recover from a disaster and prevent loss of data | Backup archives, restoring backups, backup functionality, ensure live data is not corrupt | . | . Evolution of Computing . Dedicated: physical server wholly utilized by a single customer Need to guess total capacity needed when buying a server, might be underutilized | Upgrading is slow and expensive | Limited by operating system | Multiple apps can fight over shared resources | Guaranteed security, privacy, and full utility of underlying resources | . | Virtual Machines (VMs): multiple virtual machines run on one machine Hypervisor: software layer to run virtual machines | Physical server is shared by multiple customers | Need to guess total capacity, might be underutilized, pay for a fraction of the server | Limited by guest operating system | Multiple apps can fight over shared resources if run on a single VM | . | Containers: multiple containers run on one virtual machine Docker Daemon: software layer that lets you run multiple containers | Can maximize capacity used by application to be more cost-effective | Containers share same underlying OS, but each container can run a different OS for more flexibility | Multiple apps can run side by side in containers without fighting over shared resources | . | Functions: managed VMs running managed containers, apps divided up into functions, serverless compute Upload code and choose amount of memory and duration, only responsibility for code | Very cost-effective, only pay for code execution time, VMs only execute when needed | Cold Starts: overhead time needed to start a server affects execution time | . | . Global Infrastructure . Region: grouping of multiple data centers, or availability zones | Geography: discrete market of 2+ regions that preserves data residency and compliance boundaries United States, US Government, Canada, Brazil, Mexico | Used to guarantee data will remain within a country’s boundaries | . | Paired Regions: each region is paired with another region 300 miles away only one region is updated a a time to ensure no outages, also used for disaster recovery | Azure Geo-Redundant Storage: replicates data to a secondary region automatically, ensuring data is durable even in the event that the primary region isn’t recoverable. | . | Not all Azure cloud services are available in every region. | Recommended Region: a region that provides the broadest range of service capabilities and is designed to support availability zones now or in the future | Alternate Region: a region that extends Azure’s footprint within a data residency boundary where a recommended region also exists, not designed to support availability regions. | General Availability (GA): when a service is considered ready to be used publicly by everyone | Azure Cloud services are grouped into 3 categories by when services are available Foundational: available immediately or within 12 months in recommended and alternate regions | Mainstream: available immediately or within 12 months in recommended regions, available in alternate regions based on customer demand | Specialized: available in recommended or alternate regions based on customer demand | . | Special Regions: regions to meet compliance or legal reason US Government | Chinese Government via partnership with 21Vianet for data centers | . | Availability Zones (AZs): physical location made up of 1+ data centers Data Center: secured building that contains hundreds of thousands of computers | A region will generally contain 3 AZs | Data centers within a region are isolated from each other but will be close enough for low-latency | Common practice to run workloads in 3 AZs to ensures high availability | . | AZ Supported Regions: not every region has support for availability zones, i.e. Alternate or Other Recommended regions have at least 3 AZs | . | Fault and Update Domains An AZ in an Azure region is a combination of a fault domain and an update domain | Fault Domain: logical grouping of hardware to avoid a single point of failure within an AZ Group of VMs that share a common power source and network switch | If part of data center fails, other servers won’t be taken down. | . | Update Domain: ensure resources don’t go offline when underlying hardware and software is updated | . | Availability Set: logical grouping that ensures VMs are in different fault and update domains to avoid downtime Each VM in an availability set is assigned a fault domain and an update domain. | Fault Domain e.g. server rack | Update Domain e.g. a group of specific servers across racks | Higher number of domains means more isolated VMs. Update domain must be 1 when fault domain is 1. | . | . | . Azure Services Overview . Computing Services Azure Virtual Machines: Windows or Linux VMs, most common type of compute You choose OS, Memory, CPU, and Storage, share hardware with other customers | . | Azure Container Instances: run containerized apps on Azure without provisioning servers or VMs, Docker as a Service | Azure Kubernetes Service (AKS): easy to deploy, manage, and scale containerized applications, Kubernetes (K8) as a Service | Azure Service Fabric: distributed systems platform for easy to package, deploy, and manage scalable and reliable microservices, Tier-1 Enterprise Containers as a Service | Azure Functions: event-driven, serverless compute (functions) run code without provisioning or managing servers, pay for compute time, Functions as a Service | Azure Batch: Plans, schedules, and executes batch compute workloads across running 100+ jobs in parallel, can use Spot VMs to save money (using low-priority VMs to save money) | . | Storage Services Azure Blob Storage: store very large files and large amounts of unstructured files, pay for what you store, unlimited storage, no-resizing volumes, filesystem protocols, Object Serverless Storage | Azure Disk Storage: a virtual SSD or HDD volume, encryption by default, attached to VMs | Azure File Storage: shared volume that you can access and manage like a file server | Azure Queue Storage: data store for queueing and reliably delivering messages between applications, Messaging Queue | Azure Table Storage: Wide-column NoSQL database that hosts unstructured data independent of any schema | Azure Databox (Heavy): rugged briefcase computer and storage designed to move TB or PB data, ship data in a physical device | Azure Archive Storage: long-term cold storage for when you need to hold onto files for years | Azure Data Lake Storage: centralized repository to store structured and unstructured data at any scale | . | Database Services Azure Cosmos DB: fully managed NoSQL database designed for scale with 99.999% availability | Azure SQL DB: fully managed MS SQL database with auto-scale, integral intelligence, and robust security | Azure Database: fully managed and scalable MySQL, PostgreSQL, MariaDB database with high availability and security | SQL Server on VMs: host enterprise SQL Server apps in cloud, convert MS SQL on-premise to Azure Cloud | Azure Synapse Analytics: fully managed data warehouse with integral security at every level of scale at no extra cost | Azure Database Migration Service: migrates databases to cloud with no application code changes | Azure Cache for Redis: caches frequently used and static data to reduce data and application latency | Azure Table Storage: wide-column NoSQL database that hosts unstructured data independent of any schema | . | Application Integration Services Azure Notifications Hub: send push notifications to any platform from any backend, Pub/Sub | Azure API Apps: quickly build and consume APIs in the cloud, API Gateway to Azure Services | Azure Service Bus: reliable cloud messaging as a service and simple hybrid integration | Azure Stream Analytics: serverless real-time analytics, from cloud to edge | Azure Logic Apps: schedule, automate, and orchestrate tasks, business processes, and workflows, integrate with Enterprise SaaS and Enterprise applications | Azure API Management: hybrid, multi-cloud management platform for for APIs across all environments, put in front of existing APIs for additional functionality | Azure Queue Storage: data store for queueing and reliably delivering messages between applications, Messaging Queue | . | Developer and Mobile Tools Azure SignalR Service: easily add real-time web functionality to applications, Real-Time Messaging, like Pusher for Azure | Azure App Service: easy to use service for deploying and scaling web applications, like Heroku for Azure | Visual Studio: IDE designed for creating powerful, scalable applications for Azure | Xamarin: create powerful and scalable native mobile apps with .NET and Azure, Mobile-App Framework | . | DevOps Services Azure Boards: proven agile tools to plan, track, and discuss work across teams, Kanban Boards or Github Projects | Azure Pipelines: build, test, and deploy with CI/CD that works with any language, platform, and cloud | Azure Repos: unlimited, cloud-hosted, private Git repos to collaborate and build code with pull requests and advanced file management | Azure Test Plans: test and ship with confidence using manual and exploratory testing tools | Azure Artifacts: create, host, and share packages with your team, add artifacts to CI/CD pipelines | Azure DevTest Labs: fast, easy, and lean dev-test environments | Azure Resource Manager (ARM): programmatically create Azure resources via JSON template Infrastructure as Code (IaC): process of managing and provisioning data centers through scripts, rather than physical hardware configuration or interactive configuration tools | . | Azure Quickstart Templates: library of pre-made ARM templates to quickly launch new projects, use scripts to quickly set up a project E.g. Deploy a Django App, Deploy Ubuntu VM with Docker Engine, Web App on Linux with PostgreSQL, etc. | . | . | Cloud-Native Networking Services Azure DNS: ultra-fast DNS responses and ultra-high domain availability | Azure Virtual Network (vNet): logically isolated section of Azure network where you launch your Azure resources, choose range of IP addresses using CIDR Range Create a virtual network within an Azure network with public and private subnets | CIDR Range of 10.0.0.0/16 = 65,536 IP Addresses lower number (/16) means more IP addresses | . | Subnets: logical partition of IP network, breaking up vNet IP range into smaller CIDR ranges, i.e. subnet CIDR Range 10.0.0.0/24 = 256 IP Addresses Public Subnet: can reach the internet, e.g. web app | Private Subnet: cannot reach internet, e.g. database | . | . | Azure Load Balancer: OSI Level 4 (Transport, lower-level) load balancer | Azure Application Gateway: OSI Level 7 (HTTP, app-level) load balancer, can apply Web Application Firewall service | Networking Security Groups: virtual firewall at subnet level to secure ports | . | Enterprise/Hybrid Networking Services Azure Front Door: scalable, secure entry point for fast delivery of global applications | Azure Express Route: connection between on-premise to Azure Cloud, 50 MB/s to 10 GB/s | Virtual WAN: networking service that brings networking, security, and routing functionalities together to provide single operational interface | Azure Connection: VPN connection securely connecting two Azure local networks via IPSec | Virtual Network Gateway: site-to-site VPN connection between Azure virtual network and local network. | . | Azure Traffic Manager: operates at DNS layer to direct incoming DNS requests based on routing method of your choice, i.e. weighted, performance, priority, geographic, multivalue, subnet Route traffic to servers geographically near to reduce latency | Fail-over to redundant systems in case primary systems failure | Route to random VM to simulate A/B testing | . | Azure DNS: host domain names on Azure, create DNS Zones, manage DNS records . | Azure Load Balancer: evenly distribute incoming network traffic across a group of backend resources or servers, operates on OSI Layer 4 (Transport), does not understand HTTP requests Public Load Balancer: routes incoming traffic from internet to public-facing servers (Public IPs) | Private Load Balancer: routes incoming traffic from internet to private-facing servers (Private IPs) | . | Scale Sets: group identical VMs and automatically increase or decrease amount of servers based on: change in CPU, memory, disk, network performance | on a predefined schedule | used for Elasticity | . | IoT Services Internet of Things (IoT): network of internet connected objects able to collect and exchange data | IoT Central: connects IoT devices to cloud | IoT Hub: secure and reliable communication between IoT app and managed devices | IoT Edge: fully managed IoT Hub service, data processing and analysis nearest the IoT devices Edge Computing: offload compute from cloud to local computing hardware, IoT devices | . | Windows 10 IoT Core Services: subscription for long-term OS support and services for Windows 10 IoT Core devices | . | Big Data and Analytics Services Big Data: massive volumes of structured and unstructured data that are difficult to move and process using traditional techniques | Azure Synapse Analytics: enterprise data warehousing and big data analytics, run SQL queries against large databases for reporting | HDInsight: open-source analytics software for Hadoop, Kafka, Spark | Azure Databricks: Spark-based analytics platform on Azure cloud | Data Lake Analytics: on-demand analytics job service that simplifies big data Data Lake: storage repo that holds large amounts of raw data | . | . | AI/ML Services Artificial Intelligence (AI): machines perform jobs that mimic human behavior | Machine Learning (ML): machines that get better at a task without explicit programming | Deep Learning (DL): machines have artificial neural network to solve complex problems | Azure Machine Learning Service: service to run AI/ML workloads to build flexible pipelines to automate workflows | Personalizer: personalized experiences for every user | Translator: real-time translation | Anomaly Detector: detect anomalies to quickly identify and troubleshoot issues | Azure Bot Service: serverless bot services that scales on demand | Form Recogniser: automate extraction text, key/value pairs, and tables | Computer Vision: customize computer vision models | Language Understanding: build natural language understanding into apps | QnA Maker: create conversational QnA bot from existing content | Text Analytics: extract sentiment, key phrases, named entities, language from text | Content Moderator: moderate text and images for safer user experience | Face: detect and identify people and emotions in images | Ink Recogniser: recognize digital handwritten content | . | Serverless Services Serverless: underlying servers, infrastructure, and OS fully managed by service provider, highly available, scalable, cost-effective Event Driven Scale: serverless function triggered by or trigger other events to compose complex, scaling applications | Abstraction of Servers: code described as functions, running on different compute instances | Micro-Billing: bill in micro-seconds of compute time | . | Azure Functions: run small amounts of code, serverless functions | Blog Storage: serverless object storage | Logic Apps: build serverless workflows, state machines for serverless compute | Event Grid: use Pub/Sub messaging system to react and trigger events using Azure Cloud | . | . Management Tools . Azure Portal: web-based, unified console to access Azure Cloud services | Azure Preview Portal: portal to utilize new features that are not fully released | Azure Powershell: manage Azure resources directly from Powershell CLI Powershell: command-line shell and scripting language for task automation and configuration management framework, built on .NET Common Language Runtime (CLR) | . | Visual Studio Code: source-code editor that can be run on Azure Cloud | Azure Cloud Shell: interactive, authenticated, browser-accessible shell for managing Azure resources using PowerShell or Bash | Azure CLI: type az followed by other commands to manage Azure resources Command Line Interface (CLI): process commands to computer program using lines of text, implemented using a shell or terminal | . | . Security . Azure Trust Center: website portal to provide information on privacy, security, and regulatory compliance on Azure | Compliance Programs: enterprise companies may specify compliance programs required for applications built on Azure | Azure Active Directory (Azure AD): identity and access management service, helps sign-in and access internal and external resources, Single-Sign On (SSO) | Multi-Factor Authentication (MFA): security control that uses a second device to confirm user logins, protect against stolen passwords | Azure Security Center: unified infrastructure security management system to evaluate security of data centers | Key Vault: safeguard cryptographic keys by cloud apps and services Secrets Management: control access to tokens, passwords, certificates, API keys, and other secrets | Key Management: control encryption keys | Certificate Management: manage public and private SSL certificates | Hardware Security Module (HSM): piece of hardware designed to store encryption keys, data stored in memory not disk and deleted upon device failure Multi-tenant (multiple customers virtually isolated on single HSM) HSMs are FIPS 140-2 Compliant | Single-tenant (single customer on dedicated HSM) HSMs are FIPS 140-3 Compliant | . | Azure DDoS Protection: free basic protection and paid standard protection ($3k/month) with metrics, alerts, reporting, support, and SLAs Distributed Denial of Service (DDoS) Attack: malicious attack by flooding website with large amounts of fake traffic | . | Azure Firewall: managed network security service to protect Azure virtual network resources create, enforce, and log application and network connectivity policies across subscriptions and virtual networks | static public IP address for virtual network resources to allow outside firewalls to identify traffic originating from your virtual network | high availability, no additional load balancers are required | configure deployment to span multiple AZs for increased availability | no additional cost for firewall in Availability Zone (AZ) | additional costs for inbound and outbound data transfers in AZs | . | Azure Information Protection (AIP): protect sensitive information with encryption restricted access and rights, and integrated security in Office apps | Azure Application Gateway: web-traffic load balancer (OSI Layer 7 HTTP) to reroute traffic based on set of rules Web Application Firewall (WAF) can be attached for additional protection on OSI Layer 7 | . | Azure Advanced Threat Protection (ATP): cloud-based security that leverages on-premises Active Directory to identify, detect, and investigate advanced threats, compromised identities, and malicious insider actions Intrusion Detection System (IDS)/Intrusion Protection System (IPS): device or application that monitors network or systems for malicious activity or policy violations | . | Microsoft Security Development Lifecycle (SDL): software security assurance process Training, Requirements, Design, Implementation, Verification, Release, Response | . | Azure Policy: create, assign, and manage policies to enforce or control properties of a resource evaluates resources by comparing resource properties to Policy Definitions, or business rules in JSON | . | Azure Role-Based Access Control (RBAC): manage access to Azure resources and permissions using role assignments Role Assignments: security principal, role definition, scope | Security Principal: access identities User: individual profile in Azure Active Directory | Group: set of users in Azure Active Directory | Service Principal: security identity used by applications or services to access specific Azure resources | Managed Identity: identity automatically managed by Azure | . | Role Definition: collection of permissions (e.g. read, write, delete), can be high-level or specific Use built-in roles (Owner, Contributor, Reader, User Access Administrator) or custom roles | . | Scope: resources available to be accessed by role, controls resource access at Management, Subscription, or Resource Group level | . | Lock Resources: prevent other users from accidentally deleting or modifying critical resources of a subscription, resource group, or resource CanNotDelete: users can read and modify but not delete resource | ReadOnly: users can read but not modify or delete resource | . | Management Groups: manage multiple subscriptions/accounts in hierarchical structure Each group has top level root and subscriptions inherit conditions of their management group | . | Azure Monitor: collect, analyze, and act on telemetry from cloud and on-premises environments | Azure Service Health: information about current and upcoming issues, e.g. service impacting events, planned maintenance, other availability changes Azure Status: service outage information in Azure | Azure Service Health: personalized view of Azure service and region health | Azure Resource Health: health of individual cloud resources | . | Azure Advisor: personalized cloud consultant for best practices on Azure provides recommendations in High Availability, Security, Performance, Cost, Operational Excellence | . | . | . Billing and Pricing . Service Level Agreement (SLA): Azure’s commitments for uptime and connectivity individualized per Azure service | performance targets in uptime and connectivity are in percentages 99% (two nines), 99.9% (three nines), …, 99.9999999% (nine nines) | . | not available for Free Tier or shared tiers | . | Service Credits: discount applied as compensation for under-performing Azure product or service based on SLA | Composite SLA: combined SLAs across different service offerings, improved with fallback systems Web App + SQL Database = 99.95% * 99.99% = 99.94% composite | Web App + (SQL Database or Queue Fallback) = 99.95% * (99.99999%) = 99.95% composite | . | TCO Calculator: estimate cost savings of migrating to Azure | Azure Marketplace: apps and services from third-party publishers categorized as Free, Free-Trial, Pay-As-You-Go, or Bring-Your-Own-License (BYOL) | Azure Support Basic: email support for billing and account, Azure Advisor, Health Status, Community Support, Documentation | Developer: Basic support, email tech support, third-party software support, minimal business impact response time &lt; 8 hrs, Architecture General Guidance | Standard: Developer support, moderate business impact response time &lt; 4 hrs, critical business impact response time &lt; 1 hr | Professional Direct: Standard support, minimal business impact response time &lt; 4 hrs, moderate business impact response time &lt; 2 hrs, Architecture, Operational Support, Proactive Guidance from ProDirect delivery managers, Webinars by Azure engineers | Enterprise: Professional Direct support, more things | . | Azure Licensing Azure Hybrid Use Benefit (HUB): Repurpose investment of Window Server licenses to use Azure virtual machines (Windows Servers, SQL Servers), Bring your own license (BYOL) | . | Azure Subscriptions Student: no credit card required, $100 USD credits for 12 months | Free: credit card required, $200 USD credits for 30 days, certain Azure products free for 12 months | Pay-as-you-go (PAYG): credit card required, charged monthly based on resource consumption | Enterprise Agreement: receive discounted price for licenses and cloud services | . | Azure Pricing Calculator: configure and estimate costs for Azure products | Azure Cost Management perform cost-analysis, visualize spending on Azure cloud services | create budgets, set budget threshold and alerts | . | . References .",
            "url": "https://nhtsai.github.io/notes/azure-fundamentals",
            "relUrl": "/azure-fundamentals",
            "date": " • Sep 19, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Building a Web Server",
            "content": "Web Servers . Introduction . A webserver is a networking server that waits for a client to send a request and generates a response to send back to the client. | The communication between a client and server uses HTTP protocol. | The client can be a browser or any other software that uses HTTP. | HTTP is just text that follows a certain pattern. Specify which resource and its headers | Separate head and body with blank space | Specify body message | . | A socket is an abstraction that allows you to send and receive bytes through a network. . | Example code in python # Python3.7+ import socket # define socket host and port HOST, PORT = &#39;&#39;, 8888 # Create socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((HOST, PORT)) server_socket.listen(1) print(f&#39;Serving HTTP on port {PORT} ...&#39;) while True: # Wait for client connections client_connection, client_address = server_socket.accept() # Get client request request_data = client_connection.recv(1024) print(request_data.decode(&#39;utf-8&#39;)) # Send HTTP response http_response = b&quot;HTTP/1.1 200 OK n nHello, World!&quot; client_connection.sendall(http_response) client_connection.close() server_socket.close() . We first define server socket’s host and port. | Then we create a server socket and set it to AF_INET for IPv4 address family and SOCK_STREAM for TCP. | This webserver creates a socket and binds it to port 8888 of any available host (localhost) to listen for connections. | The server socket accepts a connection and gets a client socket to receive information and the address of the client socket on the other side of the connection. | The server receives 1,024 bytes of data from the client socket. | The server then sends a message in bytes to the client socket. | Finally, the client closes the client socket connection. | . | URL http://localhost:8888/hello http designates HTTP protocol | localhost is the host name | 8888 is the port number | /hello is the path, or page to connect | . | Before the client can send HTTP requests, a TCP connection with the web server must be established. | Then the client sends an HTTP request and receives a response from the web server. . | HTTP Request GET /hello HTTP/1.1 GET is the HTTP method | /hello is the path, or page to connect | HTTP/1.1 is the protocol version | . | HTTP Response . HTTP/1.1 200 OK Hello, World! . HTTP/1.1 is the protocol version | 200 OK is the HTTP status code | Hello, World! is the HTTP response body | . | By default the browser requests the root of a server, e.g. GET / HTTP/1.0, but we should return the index.html page instead. . while True: # Wait for client connections client_connection, client_address = server_socket.accept() # Get client request request_data = client_connection.recv(1024) print(request_data.decode(&#39;utf-8&#39;)) # Get the contents of htdocs/index.html fin = open(&#39;htdocs/index.html&#39;) content = fin.read() fin.close() # Send HTTP response with content from index.html # Encode entire message to bytes http_response = &quot;HTTP/1.1 200 OK n n&quot; + content client_connection.sendall(http_response.encode()) client_connection.close() . | . References . Let’s Build a Web Server | Python Webserver | .",
            "url": "https://nhtsai.github.io/notes/web-server",
            "relUrl": "/web-server",
            "date": " • Sep 13, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Introduction to Database Systems",
            "content": "CMU 15-445: Introduction to Database Systems, Fall 2019 . Course Resources . Professor Andy Pavlo | Course Lectures | Course Website | Course Syllabus | Course Textbook: Database System Concepts, 6 or 7 ed. by Silberschatz, Korth &amp; Sudarshan | Course focuses on design and implementation of disk-oriented database management systems. | Topics: Relational Databases, Storage, Execution, Concurrency Control, Recovery, Distributed Databases, Potpourri | . 1. Relational Model &amp; Relational Algebra . Lecture Summary . | Database: organized collection of inter-related data that models some aspect of the real-world. Databases are the core component of most computer applications. | E.g. a digital music store database that keeps track of artists and albums needs to store artist biographies and albums those artists released. | . | Flat File: simplest database stored as CSV files that we manage in our own code. Use a separate file per entity, e.g. one file for artists and one file for albums | The application parses each file to read/update records. | E.g. Artist(name, year, country) and Album(name, artist, year) | If we wanted to get the year Ice Cube went solo, we could iterate through every line in the artists file and get year of row where name is Ice Cube. | . | Issues with Flat Files Data Integrity How do we ensure that the artist is same for each album entry, i.e. no typos or artist changes name? | How do we ensure the album year is a valid number and not an invalid string? | How do we store an album with multiple artists, i.e. check if parsed artist is single or multiple? | . | Implementation How do you find a particular record? | What if we want to use a new application with the same database, i.e. connecting logic between two languages? | What if two threads try to write to the same file at the same time, e.g. overwriting data? | . | Durability What if the machine crashes during updating a record? | What if we want to replicate database on multiple databases for high availability? | . | . | Database Management System (DBMS): software that allows applications to store and analyze information in a database Allows definition, creation, querying, update, and administration of databases that can be used with multiple applications. | Handles all the complex underlying logic for applications. | Focus on databases on disk, but there are other databases, e.g. in-memory. | . | Early DBMSs Issues Database applications were difficult to build and maintain | Tight coupling between logical and physical layers | Need to know what queries applications needed before deploying the database | . | Early 50s, mathematician Ted Codd (Edgar F. Codd) of IBM noticed people spent lots of time rewriting database applications, i.e. adjusting APIs depending on storing data as tree or hash table. Ted proposed the relational data model in A Relational Model of Data for Large Shared Data Banks in 1970. | . | The relational data model database abstraction to avoid maintenance. Store database in simple data structures, i.e. as relations aka tables. | Access data through high-level language, i.e. write code describing desired result aka declarative code instead of procedural code. Software has to create query plan to get desired result, similar to compilers creating machine code from high-level languages. | . | Physical storage left up to implementation, e.g.data on disk or in-memory, storing relations as trees or hash tables, etc. | . | . | Data Model: collection of concepts for describing the data in a database. Schema: description of a particular collection of data, using a given data model. | Relational data model is most common. The “best data model” because “9 times out of 10 that’s probably what you need.” | Can be used to model anything! | . | NoSQL Systems Key/Value data model | Graph data model | Document data model | Column-Family data model | . | Array/Matrix data model used in machine learning | . | Hierarchical or Network data models are obsolete/rare. | . | Relational Model Structure: definition of relations and their contents | Integrity: ensure database’s contents satisfy constraints | Manipulation: how to access and modify a database’s contents | Relation (Table): unordered set that contains relationship of attributes that represent entities N-ary relation: a table with N columns | . | Tuple (Record): set of attribute values (aka its domain) for an instance of an entity in the relation Values are normally atomic/scalar (no arrays or strings), originally. Now, anything can be stored. | The special value NULL, aka value is unknown, is a member of every domain. How NULL values are stored is implementation-specific. | . | . | Primary Key: attribute or set of attributes that can uniquely identify a single tuple, usually as an id field. Some DBMSs automatically create an internal primary key if not defined, e.g. SEQUENCE in SQL 2003 or AUTO_INCREMENT in MySQL. | . | Foreign Key: specifies that an attribute from one relation has to map to a tuple in another relation, usually as an id of a tuple in another relation. Can have another table with foreign keys relating Artist IDs to Album IDs to store albums with many artists. | DBMSs can also check IDs exists when inserting as foreign keys. | . | . | Data Manipulation Languages (DML): how to store and retrieve information from a database Procedural: the query specifies the high-level strategy the DBMS should use to find the desired result. E.g. relational algebra | . | Non-Procedural/Declarative: the query specifies only what data is wanted and not how to find it, so the DBMS needs to come up with a strategy. E.g. relational calculus | . | . | Relational Algebra: procedural language proposed by Ted Codd for querying relational data models. Fundamental operations, based on set algebra, to retrieve and manipulate tuples in a relation. | Each operator takes one or more relations as its inputs and outputs a new relation | Select $( sigma)$ : choose a subset of tuples from a relation that satisfies a selection predicate. Predicate is a filter to only select tuples that satisfy its requirement. | Can combine multiple predicates using conjunctions or disjunctions | Syntax: $ sigma _{ text{predicate}}(R)$ | Example: $ sigma _{ text{a textunderscore id=’a2’}}(R)$ | Select corresponds to WHERE in SQL. | . | Projection $( Pi)$ : generate a relation with tuples that contains only the specified attributes. Can rearrange attributes’ ordering | Can manipulate the values | Syntax: $ Pi _{ text{attributes}}(R)$ | Example: $ Pi _{ text{b textunderscore id - 100, a textunderscore id}}( sigma _{ text{a textunderscore id=’a2’}}(R))$ | Projection corresponds to SELECT in SQL. | . | Union $( cup)$ : generate a relation that contains all tuples that appear in either only one or both input relations. Relations must have same attributes with same types. | Syntax: $R cup S$ | Example: $R cup S$ | Union corresponds to UNION ALL in SQL. | . | Intersection $( cap)$ : generate a relation that contains only the tuples that appear in both of the input relations. Relations must have same attributes with same types. | Syntax: $R cap S$ | Example: $R cap S$ | Intersection corresponds to INTERSECT in SQL. | . | Difference $(-)$ : generate a relation that contains only the tuples that appear in first and not the second of input relations. Relations must have same attributes with same types. | Syntax: $R - S$ | Example: $R - S$ | Difference corresponds to EXCEPT in SQL. | . | Product $( times)$ : generate a relation that contains all possible combinations of tuples from the input relations. Syntax: $R times S$ | Example: $R times S$ | Product corresponds to CROSS JOIN in SQL. | . | Join $( bowtie)$ : generate a relation that contains all tuples that are a combination of two tuples (one from each input relation) with a common value(s) for one or more attributes. Syntax: $R bowtie S$ | Example: $R bowtie S$ | Join corresponds to NATURAL JOIN in SQL. | . | Other operators developed over the years. Rename $( rho)$ | Assignment $(R leftarrow S)$ | Duplicate elimination $( delta)$ | Aggregation $( gamma)$ | Sorting $( tau)$ | Division $(R div S)$ | . | Relational algebra defines the high-level steps of how to compute a query, instead of only stating the desired result. E.g. $ sigma _{ text{b textunderscore id=102}}(R bowtie S)$ vs. $(R bowtie ( sigma _{ text{b textunderscore id=102}}(S))$ | The first query natural joins R and S, then performs a filter after. | The second query filters S, then natural joins the output relation to R. | The queries look similar, but the efficiency could differ greatly if S and R have a billion tuples and only one tuple in S has b_id=102. The first query joins two huge relations, but the second one can find the one tuple where b_id=102 before joining. | . | A better approach is to state the high-level answer that you want the DBMS to compute. E.g. Retrieve the joined tuples from R and S where b_id equals 102. | If relations change in sizes, there no need to change the application code manually. | . | . | Queries The relational model is independent of any query language implementation. | SQL is the de facto standard. | The DBMS will take the high-level query and generate the underlying execution plan to produce the desired result. | The DBMS will adapt and improve itself without us having to change the application. | . | Conclusion Databases are ubiquitous, important, and everywhere. | Relational algebra defines the primitives for processing queries on a relational database. Will come up again in query optimization and query execution. | . | IMPORTANT: The original 9 of the 36 Chambers are the RZA, the GZA, Inspectah Deck, Ghostface Killah, Masta Killa, U-God, Method Man, Ol’ Dirty Bastard, and Raekwon. Cappadonna was an original member of the clan but was in jail at the time, thus could not be on the 36 Chambers. | . | . 2. Advanced SQL . Lecture Summary . | SQL History Originally “SEQUEL” from IBM’s System R prototype “Structured English Query Language” | Adopted by Oracle in the 1970s | Not defined by Ted Codd, the creator of relational algebra, who later on developed language Alpha | . | IBM released DB2 in 1983 IBM’s first commercial relational database using SQL | . | ANSI Standard in 1986, ISO in 1987 “Structured Query Language” | . | Current SQL standard is SQL:2016 (added JSON, polymorphic tables) Every new specification/standard adds new features, which are pushed by major bodies that develop own proprietary features | . | Most DBMSs at least support SQL-92 standard No one follows an SQL standard exactly | . | . | Relational Languages The goal is for the user to specify the desired answer at a high level, not how to compute it. | The DBMS is responsible for efficient evaluation of the query Query Optimizer: reorders operations and generates query plan | . | SQL is a collection of languages Data Manipulation Language (DML): commands that manipulate data | Data Definition Language (DDL): methods of creating tables and schemas | Data Control Language (DCL): security authorizations to control data access | View Definition | Integrity &amp; Referential Constraints | Transactions | . | IMPORTANT: SQL is based on bag algebra and relational algebra is based on set algebra List: a collection of elements with a defined order, can have duplicates | Bag: a collection of elements with no defined order, can have duplicates | Set: a collection of elements with no defined order, cannot have duplicates | If we want to define order or ensure no duplicates, the DBMS needs to do extra work to provide this. The database will ignore these requests, making queries more efficient. | . | . | . | Example Database Student(sid, name, login, gpa) | Enrolled(sid, cid, grade) | Course(cid, name) | . | Aggregates Aggregate function: functions that return a single value by applying a function on a bag of tuples. AVG(col): Return average col value | MIN(col): Return minimum col value | MAX(col): Return maximum col value | SUM(col): Return sum of values in col | COUNT(col): Return number of values for col | . | Aggregate functions can only be used in SELECT output clause. Example: Count number of students that have an “@cs” login. | . -- login field doesn&#39;t matter for COUNT SELECT COUNT(login) AS cnt FROM Student WHERE login LIKE &#39;%@cs%&#39; -- can just count every row using * SELECT COUNT(*) AS cnt FROM Student WHERE login LIKE &#39;%@cs%&#39; -- or count 1 for every row SELECT COUNT(1) AS cnt FROM Student WHERE login LIKE &#39;%@cs%&#39; . | Multiple aggregates Example: Get number of students that have an “@cs” login and their average GPA. | . SELECT COUNT(sid), AVG(gpa) FROM Student WHERE login LIKE &#39;%@cs%&#39; . | Distinct aggregates Example: Count number of unique students that have an “@cs” login. | . SELECT COUNT(DISTINCT login) FROM Student WHERE login &#39;%@cs%&#39; . | GROUP BY: project tuples into subsets and calculate aggregates against each subset To aggregate a columns and return a non-aggregated column, we need to use GROUP BY. | Non-aggregated columns in the SELECT output clause must be in GROUP BY clause. | Example: Get average GPA of students enrolled in each course. | . SELECT AVG(s.gpa) AS avg_gpa, e.cid FROM Enrolled AS e, Student AS s WHERE e.sid = s.sid GROUP BY e.cid . | HAVING: filters results based on aggregation computation. We cannot filter aggregations in the WHERE clause because the aggregation is not computed yet. | To filter aggregations, we must use HAVING, which is like a WHERE clause for GROUP BY. | Example: Get average GPA of students enrolled in each course with &gt; 3.9 GPA. | . SELECT AVG(s.gpa) AS avg_gpa, e.cid FROM Enrolled AS e, Student AS s WHERE e.sid = s.sid -- cannot use: WHERE e.sid = s.sid AND avg_gpa &gt; 3.9 GROUP BY e.cid HAVING avg_gpa &gt; 3.9 . Knowing the HAVING clause can allow DBMS to optimize query. In a declarative language, we can optimize because we know the desired result ahead of time, compared to a procedural language, we don’t. | Example: Get courses with 10 students or less | . SELECT COUNT(s.sid) AS cnt, e.cid FROM Enrolled AS e, Student AS s WHERE e.sid = s.sid GROUP BY e.cid HAVING cnt &lt;= 10 . While counting the number of students for a course, if more than 10 students, DBMS can stop counting early for that course. | . | . | . | Strings Standard says all string fields are case-sensitive and declared with only single quotes. | Exceptions MySQL is case-insensitive and can use double quotes Standard needs to match case to match: WHERE UPPER(name) = UPPER(&#39;KaNyE&#39;) | MySQL is case-insensitive and allows double quotes: WHERE name = &quot;KaNyE&quot; | . | SQLite is case-sensitive and can use double quotes | . | LIKE: used for string matching % matches any substring, including empty strings | _ matches any one character | . WHERE e.cid LIKE &#39;15-%&#39; WHERE s.login LIKE `%@c_` . | String functions Defined in standard, can be used in either output (SELECT) or predicates (WHERE, HAVING) | . -- output SELECT SUBSTRING(name, 0, 5) FROM Student -- predicate SELECT * FROM Student WHERE UPPER(s.name) LIKE &#39;CHRIS%&#39; . | String concatenation Standard says to use || operator to concatenate 2+ strings together | Exceptions: MSSQL use +, MySQL use CONCAT() | . | . | Date/Time Operations Manipulate and modify DATE/TIME attributes, can be used in either output or predicates | Syntax and support varies wildly | Example: Get current time | . -- PostgreSQL (SQL standard) SELECT NOW(); SELECT CURRENT_TIMESTAMP; -- MySQL SELECT NOW(); SELECT CURRENT_TIMESTAMP(); SELECT CURRENT_TIMESTAMP; -- SQLite SELECT CURRENT_TIMESTAMP; . Example: Extract day from a date and getting number of days since beginning of year | . -- PostgreSQL (SQL standard) SELECT EXTRACT(DAY FROM DATE(&#39;2021-09-21&#39;)); SELECT DATE(&#39;2021-09-21&#39;) - DATE(&#39;2021-01-01&#39;) AS days; -- MySQL SELECT EXTRACT(DAY FROM DATE(&#39;2021-09-21&#39;)); SELECT DATEDIFF(DATE(&#39;2021-09-21&#39;), DATE(&#39;2021-01-01&#39;)) AS days; -- SQLite -- cannot extract, cannot subtract dates, no datediff function -- method: cast date strings into julian days, subtract and cast to int -- Why is SQLite the most widely-used database? It&#39;s free and public domain (no copyright). . Simple things are difficult to do. | . | Output Redirection: store query results in another table What if we want to use query results in subsequent queries? | We can redirect query results into a new table. Table must not already be defined | Table will have same number of columns and same data types as input | . | . -- SQL standard SELECT DISTINCT cid INTO CourseIDs FROM Enrolled; -- MySQL CREATE TABLE CourseIds ( SELECT DISTINCT cid FROM Enrolled ); . We can redirect query results into an existing table. Inner SELECT must generate same number of columns and data types as target table | Syntax and options vary on dealing with inserting duplicate tuples Target table has primary key constraint must be unique and will handle insertion of duplicate primary key values differently. | . | . | . -- assuming CourseIDs table already exists INSERT INTO CourseIDs ( SELECT DISTINCT cid FROM Enrolled ); . | Output Control ORDER BY: order output tuples by values in one or more of their columns Use ORDER BY &lt;col&gt; [ASC|DESC], default is ascending order | Can also use any complex expression, e.g. ORDER BY 1+1 | . | Unlike GROUP BY, columns in ORDER BY clause don’t need to be in SELECT clause | Example: Get IDs of students enrolled in 15-445, sorted by descending grade then ascending student ID. | . SELECT sid FROM Enrolled WHERE cid = &#39;15-445&#39; ORDER BY grade DESC, sid ASC . LIMIT: limit the number of tuples returned in output, can offset to return a range Use LIMIT &lt;count&gt; [OFFSET &lt;count&gt;] | Use ORDER BY to make results consistent | . | Example: Get 20 student ID and names, skip first 10. | . SELECT sid, name FROM Student LIMIT 20 OFFSET 10 . | Nested Queries: queries containing other queries, difficult to optimize Inner queries can appear (almost) anywhere in a query | Example: Get names of all students enrolled in 15-445. Naive execution: nested loops, for each student in Student, loop over sids in Enrolled | Better execution: produce set of enrolled student IDs once, loop over sids in Student to check if in enrolled set | Optimized execution: inner join tables on s.sid = e.sid | . | . -- nested query using IN SELECT name FROM Student WHERE sid IN ( SELECT sid FROM Enrolled WHERE cid = &#39;15-445&#39; ); -- join SELECT s.name FROM Student AS s, Enrolled AS e WHERE s.sid = e.sid AND e.cid = &#39;15-445&#39; . To construct a nested query, think about what is the answer you want to produce from the outer query. | Then figure out what you need from the inner query. . | ALL: must satisfy expression for all rows in sub-query | ANY: must satisfy expression for at least one row in sub-query | IN: equivalent to =ANY() | EXISTS: at least one row is returned | . -- nested query using ANY SELECT name FROM Student WHERE sid = ANY( SELECT sid FROM Enrolled WHERE cid = &#39;15-445&#39; ); . Can use subqueries in SELECT clause for every single tuple in enrolled table where cid = 15-445, do a match up in Student table where student IDs are the same | essentially doing a join inside SELECT clause, reversing order of evaluating tables | this reversal is important for optimization because it might be faster to go through Enrolled table first | . | . -- nested query in output SELECT (SELECT s.name FROM Student AS s WHERE s.id = e.sid) AS sname FROM Enrolled AS e WHERE e.cid = &#39;15-445&#39; . Example: find student record with highest ID that is enrolled in at least one course | . SELECT sid, name FROM Student WHERE sid &gt;= ALL( SELECT sid FROM Enrolled ) SELECT sid, name FROM Student WHERE sid IN ( SELECT MAX(sid) FROM Enrolled ) SELECT sid, name FROM Student WHERE sid IN ( SELECT sid FROM Enrolled ORDER BY sid DESC LIMIT 1 ) . Example: find all courses with no enrolled students Inner query can reference outer query, but outer cannot reference inner. | . | . SELECT * FROM Course WHERE NOT EXISTS ( SELECT * FROM Enrolled WHERE Course.cid = Enrolled.cid ) . | Window Functions Performs a calculation across a set of tuple that relate to a single row | Like an aggregation, but tuples are not grouped into single output tuples | Basic Syntax FUNC-NAME: aggregation functions, special functions | OVER: how to ‘slice’ up data, can also sort | . | . SELECT ... FUNC-NAME(...) OVER (...) FROM tableName . Special window functions ROW_NUMBER(): number of current row | RANK(): sort order position of current row, used with ORDER BY | . | . SELECT *, ROW_NUMBER() OVER () AS row_num FROM Enrolled . OVER: specifies how to group together tuples when computing the window function | Use PARTITION BY to specify group, like GROUP BY clause | Without PARTITION BY, window group is the whole table | . SELECT cid, sid, ROW_NUMBER() OVER (PARTITION BY cid) FROM Enrolled ORDER BY cid . Use ORDER BY to sort entries in each group | . SELECT cid, sid, ROW_NUMBER() OVER (ORDER BY cid) FROM Enrolled ORDER BY cid . Example: find student with highest grade for each course Window groups tuples by course ID and orders each group by letter grade | . | . SELECT * FROM ( SELECT *, RANK() OVER (PARTITION BY cid ORDER BY grade ASC) AS rank FROM Enrolled ) AS ranking WHERE ranking.rank = 1 . | Common Table Expressions (CTE) Provides a way to write auxiliary statements for use in a larger query | Like a temp table just for one query | Alternative to nested queries and views | Basic Syntax | . WITH cteName AS ( SELECT 1 ) SELECT * FROM cteName . Can bind output columns to names before the AS keyword two named columns, return 1 row with 2 values (1, 2) | . | . WITH cteName (col1, col2) AS ( SELECT 1, 2 ) SELECT col1 + col2 FROM cteName . Example: find student with highest id that is enrolled in at least one course Query references the CTE cteSource | . | . WITH cteSource (maxId) AS ( SELECT MAX(sid) FROM Enrolled ) SELECT name FROM Student, cteSource WHERE Student.sid = cteSource.maxId . Difference between CTE and nested queries: CTEs can use recursion! . | Example: print sequence of numbers from 1 to 10 . | . WITH RECURSIVE cteSource (counter) AS ( (SELECT 1) -- base case UNION ALL -- recursive call: (SELECT counter + 1 -- get current value + 1 FROM cteSource WHERE counter &lt; 10) -- specified limit of recursion ) SELECT * FROM cteSource . | Conclusion SQL is not a dead language. | You should (almost) always strive to compute your answer as a single SQL statement. | . | . 3. Database Storage I . Lecture Summary . | Disk-Oriented Architecture DBMS assumes primary storage location of the database is on non-volatile disk | DBMS manages movement of data between volatile and non-volatile storage | . | Storage Hierarchy CPU Registers &gt; CPU Caches &gt; DRAM &gt; SSD &gt; HDD &gt; Network Storage (e.g. AWS EBS/S3) | Ordered from faster, smaller, expensive to slower, larger, cheaper | Volatile not persistent without power, needs power to maintain memory | random access: can quickly jump around randomly in memory, same performance in any order of jumping | byte-addressable: can read exactly 64 bits at a location | volatile is DRAM and above storages, called memory | . | Non-Volatile persistent without power | sequential access: more efficient to read contiguous blocks of storage Think of a HDD like a record player, where random access requires expensive movement of needle to another location | . | block-addressable: need to get a whole 4KB block or page to read the desired 64 bits at a location | non-volatile is SSD and below storage, called disk | . | Exception: non-volatile memory (Intel Octane) Like DRAM in that sits in DIMM slot and is byte-addressable | Like SSD in that it is persistent without power | The future of computers! | . | Access Times | . Access Time Storage Comparison . 0.5 ns | L1 Cache Ref | 0.5 sec | . 7 ns | L2 Cache Ref | 7 sec | . 100 ns | DRAM | 100 sec | . 150,000 ns | SSD | 1.7 days | . 10,000,000 ns | HDD | 16.5 weeks | . ~30,000,000 ns | Network Storage | 11.4 months | . 1,000,000,000 ns | Tape Archives | 31.7 years | . | System Design Goals Allow DBMS to manage databases that exceed the amount of memory available | Reading/writing to disk is expensive, these calls must be managed carefully to avoid large stalls and performance degradation Use concurrent queries, caching, precomputing, etc. | . | . | Disk-Oriented DBMS At the lowest layer, we have a disk that holds database files Database file consists of a directory and pages, or blocks | . | In the next layer, we have the memory that holds the buffer pool, which manages movement of data between disk and memory | At the highest layer, we have the execution engine, which executes queries | Example: The execution engine needs Page 2, but it is not in memory We need to use the directory to locate Page 2 | Fetch Page 2 from disk | The buffer pool will bring Page 2 into memory | Then we return a pointer to Page 2 to the execution engine to interpret the layout of Page 2 | The buffer pool manager ensures the page is there while the execution engine is operating on that memory | . | . | Why not use the OS? Why does the DBMS need to ‘fake’ more memory than available and manage memory in this way when the OS can already do this? Similar to virtual memory, where there is a large address space (virtual) and a place for OS to bring in pages from disk (physical) | The OS is already responsible for moving the files’ pages in and out of memory | . | Under the hood, the OS uses memory mapping (mmap) to store contents of a file into a process’ address space mmap file: takes a file on disk and maps the files’ pages into the address space of our process | Now we can read and write to those memory locations | If the file is not in memory the OS, a page fault occurs | OS needs to bring files into memory before any operations are performed | . | Example We have a bunch of pages in a file on-disk, a 4-slot virtual memory page table, and 2-slot physical memory | Application wants to read Page 1 We look in virtual memory and see Page 1 is not backed by physical memory, get a page fault | Then we fetch Page 1 from disk and back it into physical memory slot | Then we update our virtual page table to point to Page 1 in physical memory | . | Application wants to read Page 3, and the same process occurs and Page 3 is fetched into physical memory | Application wants to read Page 2, but both physical memory slots are taken We need to decide which page (Page 1 or Page 3) to remove, load Page 2 into physical memory, and return a pointer in the virtual memory to the fetched data in physical memory | While this is happening, the thread for Page 2 is stalled while the disk scheduler for the OS goes to fetch Page 2 An optimized approach is to hand off the stalled request task to another thread to allow non-stalled tasks to continue executing without delay | . | . | . | The OS does not understand what the DBMS wants to do; it just sees a bunch of reads and writes to pages and does not understand the high-level semantics of queries and what data queries want to read By relying on the OS, virtual memory, and mmap, we don’t take advantage of our knowledge of the DBMS and give up control of memory management to the blind OS | . | What if we allow multiple threads to access the mmap files to hide page fault stalls? This works good enough for read-only access because stalled requests that are being fetched can be passed to other threads to keep executing non-stalled requests | It is complicated for multiple writing threads because the OS does not know that certain pages need to be flushed out to disk before other pages do The OS just writes data out to disk without knowing if it is okay or not | . | . | Some solutions to issues of using mmap madvise: tell OS how you expect to read certain pages (sequential or random access) | mlock: tell OS that memory ranges cannot be paged out | msync: tell OS to flush memory ranges out to disk | . | mmap files and relying on the OS for memory management sound good but will create performance bottlenecks and correctness issues None of the major databases use mmap completely | . | DBMS (almost) always wants to control things itself and can do a better job at it knows what queries want to do and the workload | flushing dirty pages to disk in correct order | specialized pre-fetching | buffer replacement policy | thread/process scheduling | OS is not your friend | . | . | Database Storage Problem 1: How the DBMS represents database in files on disk | Problem 2: How the DBMS manages its memory and moves data back-and-forth from disk | . | File Storage DBMS stores database as one or more files on disk E.g. SQLite stores databases in 1 file, most other systems store in multiple files due to file-size limitations | OS does not know anything about the contents of these files, but the formats of these files are typically specific to the DBMS | . | Early systems in 1980s used custom file systems on raw storage, rather than formatting the hard drive to a common file format like NTFS Some ‘enterprise’ DBMSs still support this, e.g. Oracle DB2 | Most newer DBMSs do not do this | . | . | Storage Manager: responsible for maintaining a database’s files on disk Some DBMS do own scheduling for reads/writes to improve spatial and temporal locality of pages, i.e. combining queries that closely-located blocks | A file itself is organized as collection of pages storage manager tracks data read/written to pages | storage manager tracks available space to store new data in the pages | . | Page: fixed-size block of data can contain anything: tuples, meta-data, indexes, log records, … | most systems do no mix page types, e.g. a page with only tuples and a page with only index information | some systems require a page to be self-contained, in which all the information needed to comprehend the contents of a page must be stored within the page itself | Example: a table with 10 columns of different types Consider page’s metadata about the table (schema, layout) was stored in one page and all the tuples of the table stored in another | An issue arises if the metadata page is lost in a system failure, then the tuples page is difficult to comprehend or interpret | By keeping metadata in each page, we trade overhead and storage for better disaster recovery | . | . | Each page is given a unique identifier, generated by the DBMS The DBMS uses an indirection layer to map page IDs to physical location in a file at some offset | We want to do this to move pages around (compacting disk, setting up another disk) and still know where pages are physically located because page IDs are unchanged | . | 3 notions of pages in DBMS Hardware Page (usually 4KB): the page access level from the storage device itself, what the HDD or SSD exposes The lowest level we can do atomic writes to the storage device, usually in 4KB writes | The level the device can guarantee a “failsafe write”, or a write and flush to the disk is atomic | Example: if we need to write 16KB and we write 8KB, crash, and write 8KB There is a “torn write” where we only see the first half but not the second half | The hardware can only guarantee 4KB writes at a time | . | . | OS Page (usually 4KB): the page access level when moving data from disk into memory | Database Page (512B - 4KB SQLite - 8KB PostgreSQL - 16KB MySQL): the page access level the DBMS operates on Why use more than 4KB pages? | Internally, we use a table to map pages to locations on disk. Using large sized pages, we can reduce that page mapping table size to represent more data with fewer page ID mappings | Tradeoff is handling 4KB writes very carefully | . | . | . | Page Storage Architecture Different DBMSs manage pages in files on disk differently, how pages are organized within files Heap File Organization | Sequential/Sorted File Organization | Hashing File Organization | . | At this lowest level of hierarchy, we don’t care what data is stored in the pages (tuples, indexes, etc.) | . | Database Heap Heap File: unordered collection of pages where tuples are stored in random order Relational model does not have any order, insertion order is not guaranteed | API functions: Create, Get, Write, Delete page | Must support iterating over all pages for scanning entire table | . | Metadata to keep track of what pages exist and which ones have free space for inserting new data | Representations of heap files Doubly-Linked List (naive) . . maintain a header page at beginning of file that stores two pointers HEAD of free page list, pages with available space | HEAD of data page list, pages completely full | . | each page keeps track of number of free slots in itself to insert new data, we look through pages in the free page list | . | to find a particular page, we iterate over the entire data page list | if the pages were sorted in a unified linked list, we would have faster page searching but need to iterate over all pages to find free space | . | Page Directory (optimal) . . maintain a directory page that tracks location to data pages in the database files, similar to a hash table | directory also records number of free slots per page | DBMS has to make sure that directory pages are in sync with the data pages the directory holds metadata about the pages themselves that need to be in sync, but this cannot be guaranteed | the hardware cannot guarantee (over 4KB writes) that we can write to two pages at the same time | Example: if we deleted a bunch of data in a page and freed up space, the page directory should update the number of free slots for that page | But if the system crashes before the page directory is updated, the DBMS may think the page is full when it is not | . | . | . | . | Page Layout Page Header: metadata about the page’s contents Page Size, Checksum (used to determine torn writes/crashes), DBMS Version, Transaction Visibility, Compression Information | Some systems require pages to be self-contained | . | How do we store data within a page (assuming only storing tuples)? Tuple-Oriented | Log-Structured | . | Tuple Storage How to store tuples in a page? | Simple Append: strawman idea Header keep tracks of the number of tuples in the page | Append new tuples to end (determined by offset calculated from number of tuples) | If we delete a tuple, there is a gap where the removed tuple was. | If tuples are fixed length, we can just insert a new one in the gap, but we need to sequentially scan for gaps | But if not fixed-length, then the gap may be too big or too small | . | Slotted Pages: most common layout scheme . . Header keeps track of number of used slots and the offset of the starting location of the last slot used | Slot array maps ‘slots’ to the tuples’ starting position offsets | Tuples are stored at the end of the page, and the offset of start of the tuple is written to a slot array Tuples can be fixed or variable length | The slot array grows from the beginning to end of page | The tuple storage grows from end to the beginning of page | Page is full when two section meet or gap too small to store anything | . | Now we can move tuples around the page without affecting upper levels Locating a record (record Id) is just page ID and slot number | To move a tuple, just update its slot number | To delete a tuple, some systems use compaction to remove gap or some systems can mark slot value as available or some systems just append and deal with the gap later | . | . | . | . | Tuple Layout Tuple: essentially a sequence of bytes | The DBMS needs to interpret the bytes into attribute types and values | Prefixed with header that contains metadata visibility info (concurrency control) | bitmap for NULL values | . | Do not need to store metadata about tuple in itself because it is stored in the page or another structure Need to store metadata in JSON/Schema-less databases like MongoDB because every single document can be different, so you need metadata in itself. | . | Attributes are typically stored in the order specified when the table is created Not necessary in relational model, but easier for software engineering | . | Denormalized Tuple Data Can physically denormalize (e.g. pre-join) related tuples and store them together in the same page This is when data from different tables are stored within the same page | Technically allowed, but now have to keep track of which tuple is from which table | . | Potentially reduces amount of I/O for common workload patterns | Can make updates more expensive | Normalization: how we split up data across different tables Naturally happens with foreign keys | Sometimes, we want embed tables inside another table to avoid joins | Packing tuples inside of a tuple value | . | Not a new idea IBM System R did this in 1970s, abandoned in DB2 | Several NoSQL DBMSs (CloudSpanner, MongoDB, etc.) do this without calling it physical denormalization | . | . | . | Conclusion Database is organized in pages. | Different ways to track pages. | Different ways to store pages. | Different ways to store tuples. | . | . 4. Database Storage II . 5. Buffer Pools &amp; Memory Management . 6. Hash Tables . 7. Tree Indexes I . 8. Tree Indexes II . 9. Multi-Threaded Index Concurrency Control . 10. Sorting &amp; Aggregations . 11. Join Algorithms . 12. Query Execution I . 13. Query Execution II . 14. Query Planning &amp; Optimization I . 15. Query Planning &amp; Optimization II . 16. Concurrency Control Theory . 17. Two-Phase Locking Concurrency Control . 18. Timestamp Ordering Concurrency Control . 19. Multi-Version Concurrency Control . 20. Database Logging Schemes . 21. ARIES Database Recovery . 22. Introduction to Distributed Databases . 23. Distributed OLTP Databases . 24. Distributed OLAP Databases . 25. Shasank Chavan (Oracle In-Memory Databases) . 26. Systems Potpourri . References .",
            "url": "https://nhtsai.github.io/notes/database-systems-intro",
            "relUrl": "/database-systems-intro",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "SQL Overview",
            "content": "SQL Overview . Mode SQL | Select Star SQL | W3 Schools SQL | Window Functions Video | . SQL . SQL, or Structured Query Language, is a declarative language used to access and manipulate databases. | SQL can execute queries, retrieve data, insert records, update records, and delete records. | SQL can create new databases, tables, stored procedures, views. | SQL can set permissions on tables, procedures, and views. | . RDBMS . RDBMS, or Relational Database Management System, is the basis for SQL and all modern database systems (MS SQL Server, MySQL, etc.). | A database is an organized collection of data tables. | A schema is an overview of all tables in a database. | A table is a collection of related data entries, formatted in columns and rows. Every table is broken up into smaller entities called fields. | Reference a table using database.table. | . | A field is a table column (vertical entity) designed to maintain specific information about every record in the table. | A record is a table row (horizontal entity) that holds specific information about each individual entry that exists in a table. | . SQL Syntax . SQL keywords are not case-sensitive. | Semicolons separate each SQL statement, often used to execute more than one statement in the same query call. | Use -- comment for single line comments, /* comment */ for multi-line comments | . SELECT . Extracts data from a database | Use double-quotes for column names if column name is a keyword. . -- Selecting all columns and rows SELECT * FROM table_name -- Using double quotes for column name (since max is a keyword) SELECT &quot;Max&quot; FROM table_name . | . SELECT TOP/LIMIT . Specify the number of records to return | Use LIMIT for MySQL, other databases have different ones . -- SQL Server/MS Access SELECT TOP 10 col1, col2 FROM table_name -- MySQL/PostgreSQL SELECT col1, col2 FROM table_name LIMIT 10 ORDER BY col1 DESC -- use ORDER BY to get meaningful row order . | . FROM . Denotes the database and table to query . -- Selecting all rows of col1 SELECT col1 FROM table_name . | . WHERE . Filters the data retrieved from the database based on a specified condition . -- Filtering for rows where col1 is positive SELECT col1 FROM table_name WHERE col1 &gt; 0 . | . Comparison Operators Symbol . Equal To | = | . Not Equal To | &lt;&gt;, != | . Greater Than | &gt; | . Greater Than or Equal To | &gt;= | . Less Than | &lt; | . Less Than or Equal To | &lt;= | . Arithmetic Operators Symbol . Addition | + | . Subtraction | - | . Multiplication | * | . Division | / | . Use operators to compare across columns in the same row. Need aggregate functions to compare across rows. | . | . Logical Operators Symbol Example . Match similar values | LIKE, % is a multiple wildcard character, _ is an individual wildcard character, [ab] is any single character, [^ab] is any single character not in brackets, [a-b] is any single character in a range of characters | WHERE col LIKE &#39;n%&#39; | . Match similar values, case insensitive | ILIKE | WHERE col LIKE &#39;n%&#39; | . Match on list of values, or query result | IN | WHERE col IN (1, 2, 3) | . Match on a range, inclusive | BETWEEN ... AND ... | WHERE col BETWEEN 5 AND 10 | . Match null values | IS NULL | WHERE col IS NULL | . Match non-null values | IS NOT NULL | WHERE col IS NOT NULL | . Match on 2 conditions | AND | WHERE col &gt;= 1 AND col &lt;= 2 | . Match on either of 2 conditions | OR | WHERE col = 1 OR (col = 2 AND col = 3) | . Match on not a condition | NOT | WHERE col NOT LIKE &#39;%n%&#39; WHERE col NOT BETWEEN 2 AND 3 | . Match on any values meeting condition | ANY | WHERE id = ANY(SELECT id FROM users WHERE score = 10) | . Match on all values meeting condition | ALL | WHERE id != ALL(SELECT id FROM users WHERE score &lt; 90) | . Exists if subquery returns 1+ rows | EXISTS | WHERE EXISTS (SELECT id FROM users WHERE age &lt; 18) | . ORDER BY . Sorts data based on 1+ columns | Ascending sort by default, use DESC for descending sort | Can also use #s in place of column names, corresponding to (1-indexed) order of columns in SELECT . -- Ordering using column names SELECT col1, col2 FROM table_name ORDER BY col1 DESC, col2 -- Ordering using column numbers SELECT col1, col2 FROM table_name ORDER BY 1 DESC, 2 . | . Aggregate Functions . Aggregation Command Example . Gets count of all non-null values | COUNT | SELECT COUNT(col1) | . Gets sum of all values, null values are 0 | SUM | SELECT SUM(col1) | . Gets average of all values, null values are 0 | AVG | SELECT AVG(col1) | . Gets minimum of all values | MIN | SELECT MIN(col1) | . Gets maximum of all values | MAX | SELECT MAX(col1) | . AS (Aliasing) . Renames a table or column . SELECT col1 AS &quot;Sales&quot; FROM table_name AS T . | . GROUP BY . Aggregates across smaller groupings instead of the whole table | Required for non-aggregated columns when performing aggregation | Can also reference columns by 1-indexed order in SELECT | Order of column names in grouping does not matter . SELECT A, B, COUNT(C) FROM table_name GROUP BY A, B -- Equivalent query SELECT A, B, COUNT(C) FROM table_name GROUP BY 2, 1 . | . HAVING . Filters the result of aggregate columns | Like WHERE but for aggregated columns . SELECT A, MAX(B) -- max aggregation FROM table_name WHERE A &gt;= 4 -- filter before aggregation GROUP BY A -- group by non-aggregated cols HAVING MAX(B) &gt; 10 -- filter after aggregation ORDER BY 2 DESC -- descending sort by MAX(B) LIMIT 100 -- return only first 100 rows . | . CASE WHEN . Used to handle if/else logic | Always goes in the SELECT clause | Default value is NULL if no ELSE clause . SELECT CASE WHEN condition THEN value WHEN condition THEN value ELSE value -- optional, default is NULL END AS col_name FROM table_name . | Use with aggregate functions to create different aggregation groups can use CASE WHEN alias in GROUP BY or copy and paste the whole CASE WHEN | . -- vertical table example SELECT CASE -- processes all c values into 5 groups WHEN col=1 THEN &#39;Freshman&#39; WHEN col=2 THEN &#39;Sophomore&#39; WHEN col=3 THEN &#39;Junior&#39; WHEN col=4 THEN &#39;Senior&#39; ELSE &#39;None&#39; END AS grade, COUNT(1) AS count FROM table_name GROUP BY grade -- groups by grade -- horizontal table example SELECT COUNT(CASE WHEN col=1 THEN 1 ELSE NULL END) AS fr_count, COUNT(CASE WHEN col=2 THEN 1 ELSE NULL END) AS so_count, COUNT(CASE WHEN col=3 THEN 1 ELSE NULL END) AS jr_count, COUNT(CASE WHEN col=4 THEN 1 ELSE NULL END) AS sr_count FROM table_name . | . DISTINCT . Selects unique values of 1+ columns | Only need to include DISTINCT once | Slow performance, especially in aggregations . SELECT DISTINCT year, month FROM table_name -- use with aggregate SELECT COUNT(DISTINCT month) AS unique_months FROM table . | . Joins . Uses common identifiers to join related tables Rows without matching common identifiers will default to NULL | . | Use aliases for tables when joining Aliases distinguish identical column names when joining two tables | . | Can use aliases in SELECT clause, like SELECT alias.col_name | . JOIN/INNER JOIN . Joins two tables, returns only matched rows from both tables | Unmatched rows are not included . -- Keeps matched rows SELECT A.*, B.col2 FROM table1 A JOIN table2 B ON A.col=B.col . | . LEFT JOIN/LEFT OUTER JOIN . Joins two tables, returns matched rows and unmatched rows from left table. . -- Keeps all A rows and matched B rows SELECT A.*, B.col2 FROM table1 A LEFT JOIN table2 B ON A.col=B.col . | . RIGHT JOIN/RIGHT OUTER JOIN . Joins two tables, returns matched rows and unmatched rows from right table. | Usually use flip the table order and use LEFT JOIN. . -- Keeps match A rows and all B rows SELECT A.*, B.col2 FROM table1 A RIGHT JOIN table2 B ON A.col=B.col . | . FULL OUTER JOIN . Joins two tables, returns all matched and unmatched rows. | Commonly used in aggregations to find overlap between two tables . -- Keeps all matched/unmatched rows SELECT A.*, B.col2 FROM table1 A JOIN table2 B ON A.col=B.col -- Example SELECT COUNT( CASE WHEN A.a IS NOT NULL AND B.b IS NULL THEN A.a -- must use A.a otherwise NULL and COUNT will be 0 ELSE NULL END ) AS A_only, COUNT( CASE WHEN A.a IS NOT NULL AND B.b IS NOT NULL THEN A.a -- can keep either A.a or B.b ELSE NULL END ) AS both, COUNT( CASE WHEN A.a IS NULL AND B.b IS NOT NULL THEN B.b -- must use B.b otherwise NULL and COUNT will be 0 ELSE NULL END ) AS B_only FROM table1 A JOIN table2 B ON A.a = B.b . | . Joins with Conditions . Can put conditions either in the WHERE or ON clauses . -- Finds rows that fit the condition rather -- than joining all rows then filtering SELECT A.*, B.* FROM table1 A JOIN table2 B ON A.id = B.id AND A.year &gt; B.year + 5 . | Conditioning WHERE vs. ON clauses WHERE joins all rows that match, then filters the rows based on that condition May be less efficient when joining large tables | . | ON only joins rows that match the conditions listed | For INNER JOINs, the two methods effectively produce the same results because unmatched (NULL) rows are discarded. | For OUTER JOINs, this is a slight difference. | . | Example documents table | . id name . 1 | doc1 | . 2 | doc2 | . 3 | doc3 | . 4 | doc4 | . 5 | doc5 | . downloads table | . id doc_id user . 1 | 1 | sandeep | . 2 | 1 | simi | . 3 | 2 | sandeep | . 4 | 2 | reya | . 5 | 3 | simi | . | WHERE conditioning vs. ON conditioning . SELECT documents.name, downloads.id FROM documents LEFT OUTER JOIN downloads ON documents.id=downloads.doc_id WHERE user=&#39;sandeep&#39; -- WHERE condition SELECT documents.name, downloads.id FROM documents LEFT OUTER JOIN downloads ON documents.id=downloads.doc_id AND user=&#39;sandeep&#39; -- ON condition . | Intermediate JOIN table for WHERE conditioning Matches every row using key, filters after the join. | . documents.id name downloads.id doc_id user . 1 | doc1 | 1 | 1 | sandeep | . 1 | doc1 | 2 | 1 | simi | . 2 | doc2 | 3 | 2 | sandeep | . 2 | doc2 | 4 | 2 | reya | . 3 | doc3 | 5 | 3 | simi | . 4 | doc4 | NULL | NULL | NULL | . 5 | doc5 | NULL | NULL | NULL | . | Intermediate JOIN table for ON conditioning Does not match rows that are not user=&#39;sandeep&#39;, filters one table before the join. | . documents.id name downloads.id doc_id user . 1 | doc1 | 1 | 1 | sandeep | . 2 | doc2 | 3 | 2 | sandeep | . 3 | doc3 | NULL | NULL | NULL | . 4 | doc4 | NULL | NULL | NULL | . 5 | doc5 | NULL | NULL | NULL | . | Result of WHERE conditioning . name downloads.id . doc1 | 1 | . doc2 | 3 | . | Result of ON conditioning . name downloads.id . doc1 | 1 | . doc2 | 3 | . doc3 | NULL | . doc3 | NULL | . doc4 | NULL | . doc5 | NULL | . | Because the WHERE condition filters after the LEFT OUTER JOIN, THE NULL rows of the user column are removed. You can see that because the ON condition filters before the LEFT OUTER JOIN, the NULL rows are kept in the final result. | For INNER JOINs, put join conditions in the ON clause and put where conditions in the WHERE clause. | For LEFT OUTER JOINs, put join conditions in the ON clause and put where conditions that reference the right table in the ON clause also. Referencing the right table after the LEFT JOIN in the WHERE clause converts the LEFT JOIN into an INNER JOIN. | Exception: reference the right table in the WHERE clause when looking for records not in the table, e.g. WHERE t2.id IS NULL. | Think of conditions in the ON clause as “right table WHERE clause filters” that are applied prior to joining. | . | . Joins with Multiple Foreign Keys . Accuracy of joining is improved | Performances: SQL uses indexes to speed up queries Using multiple join keys can speed up performance for large datasets | . SELECT A.col1, B.col1 FROM table1 A JOIN table2 B ON A.id = B.id AND A.col = B.col . | . Self Joins . Joins a table with itself | Self joins can help compare different records of the same table . -- WHERE: Find pairs from the same city SELECT A.name, B.name, A.city FROM users A, users B WHERE A.id &lt;&gt; B.id -- avoid matching users to themselves AND A.city = B.city -- find pairs from the same city -- ON: Find all pairs SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id != B.id -- prevents identical pairs, e.g. (5, 5) -- ON: Find all unique pairs SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id &lt; B.id -- prevents identical pairs and permutations, e.g. (1, 5) but not (5, 1) -- ON: Find unique pairs of duplicates SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id &lt; B.id -- gets different pairs AND A.last = B.last -- with the same value, eg. different people with same last name . | . UNION . Stacks two tables vertically | Only appends DISTINCT values, identical rows are dropped | Use UNION ALL to append all values, including identical rows | Can write 2 SELECT queries, and put the results on top of each other to create one table Result set’s column names usually same as first SELECT clause | . | Every SELECT statement within UNION must have same number of columns The columns must have similar data types | The columns in every SELECT statement must also be in the same order | . -- UNION Example SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; -- Union Example SELECT &#39;Customer&#39; AS Type, ContactName, City, Country FROM Customers UNION SELECT &#39;Supplier&#39;, ContactName, City, Country FROM Suppliers; -- UNION ALL Example SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; ORDER BY City . | . Data Types . Differences vary across various versions of SQL | . Imported As Stored As . String | VARCHAR(1024) | . Date/Time | DATE, DATETIME, TIMESTAMP, INTERVAL, YEAR | . Number | Exact: INTEGER, SMALLINT, DECIMAL, NUMERIC Approx: FLOAT, REAL, DOUBLE PRECISION | . Boolean | BOOLEAN | . CAST . In MySQL, changes data type of a column | In PostgreSQL, can also use expression::type casting syntax . SELECT CAST(col AS Integer) AS int_col FROM table_name WHERE CAST(col AS Integer) &gt; 10 . | . Dates . DATE, DATETIME, TIMESTAMP are date types used in SQL versions | YYYY-MM-DD is used for easy ordering, identical orders for dates and strings | Date ranges are stored as INTERVAL Intervals are defined in English terms, e.g. 10 seconds, 5 days, or 2 months | . -- Using intervals SELECT (CAST(table_name.date AS Timestamp) + INTERVAL &#39;1 week&#39;) AS week_after FROM table_name . | EXTRACT(field FROM date_col): helps deconstruct dates field includes: YEAR, MONTH, DAY, HOUR, SECOND, DECADE, DOW (day of week) | . SELECT EXTRACT(YEAR FROM date_col) AS year FROM table_name . | DATE_TRUNC(field, date_col): rounds date to a specified precision field includes: YEAR, MONTH, DAY, HOUR, SECOND, DECADE | . SELECT DATE_TRUNC(DAY, date_col) AS rounded_day FROM table_name . | System Datetime Variables CURRENT_DATE() | CURRENT_TIME() | CURRENT_TIMESTAMP() | LOCALTIME() | LOCALTIMESTAMP() | NOW() | To any above, can add AT TIME ZONE timezone_code to convert to a different timezone. | . SELECT NOW() SELECT CURRENT_DATE() AT TIME ZONE &#39;PST&#39; AS current_date_pst . | . Null Functions . Functions vary by SQL version | IFNULL(col, value): returns default value if expression is null | COALESCE(col, value): returns default value if expression is null . SELECT name, price * (inventory + COALESCE(ordered, 0)) FROM products . | . String Functions . LEFT(str, n): get n characters from left end of string | RIGHT(str, n): get n characters from right end of string | TRIM(type chars FROM str): trim characters from string TRIM(both &#39;()&#39; FROM col1): trim from both ends | TRIM(leading &#39;()&#39; FROM col1): trim from front | TRIM(trailing &#39;()&#39; FROM col1): trim from back | . | POSITION(substr IN str): get position of substring in string, case-sensitive | STRPOS(str, substr): get position of substring in string, case-sensitive | SUBSTR(str, start_idx, n): get n-length substring starting at index of string | CONCAT(str, str, ...): concatenate strings | str || str || ...: concatenate strings | UPPER(str): convert string to upper case | LOWER(str): convert string to lower case | . Subqueries . Subqueries are nested/inner queries to perform operations in multiple steps | Subqueries are required to have aliases . SELECT sub.* FROM ( SELECT * FROM students WHERE grade &gt; 80 ) sub WHERE sub.class_level = &#39;Senior&#39; . | Example: How many incidents happen, on average, on Fridays in December? . -- Count # of incidents that happen on each day. -- Average # of daily incidents over each day of week (5: Friday) in month (December). SELECT EXTRACT(MONTH FROM sub.incident_date) AS &quot;Month&quot;, sub.day_of_week, AVG(sub.daily_incidents) FROM ( SELECT EXTRACT(DOW FROM incident_date) AS day_of_week, incident_date, COUNT(incident_id) AS incidents FROM log_table GROUP BY 1, 2 ) sub WHERE sub.day_of_week = 5 AND EXTRACT(MONTH FROM sub.incident_date) = 12 . | Example: How many incidents happen for each category, averaged over all months? . -- Count # of incidents per category that happen on each month -- Average # of incidents per category over all months SELECT sub.category, AVG(sub.incidents) AS avg_incidents_per_month FROM ( SELECT EXTRACT(MONTH FROM incident_date) AS &quot;month&quot;, category, COUNT(incident_id) as incidents FROM log_table GROUP BY 1, 2 ) sub GROUP BY 1 . | Subqueries in Conditional Logic Don’t use aliases for subqueries in conditional statements . | Example: get all products that have min price . | . SELECT * FROM products WHERE price = (SELECT MIN(price) FROM products) -- subquery returns single cell . Example: get the products in the top 5 prices | . SELECT * FROM products WHERE price IN ( SELECT DISTINCT price FROM products ORDER BY price DESC LIMIT 5 ) -- subquery returns multiple cells (of one column) . | Joining Subqueries Using filtering in the ON clause can handle cases when subquery returns one or multiple rows | ON filtering is the same as WHERE filtering for INNER JOINS . | Example: get the products in the top 5 prices | . SELECT * FROM products JOIN ( SELECT DISTINCT price FROM products ORDER BY price DESC LIMIT 5 ) sub ON products.price = sub.price . | Joining Subqueries with Aggregation Functions . Example: Find the products from the top 3 categories with the least counts. | . -- subquery returns top 3 categories with least product counts -- query selects all products in the specified 3 categories -- result set is ordered by count and descending price -- (products with same category have same counts) SELECT products.*, sub.c AS counts FROM products JOIN ( SELECT category, COUNT(id) AS c FROM products GROUP BY category ORDER BY c LIMIT 3 ) sub ON products.category = sub.category ORDER BY sub.c, products.price DESC . Unlike using WHERE col IN (subquery), JOIN (subquery) ON table.col=sub.col allows aggregated values to be passed into the outer query because an IN clause is limited to one set of values. Additionally, MySQL does not allow LIMIT in subqueries in the WHERE clause. | . | . | Using Subqueries to Improve Performance Inefficient: FULL JOIN creates a huge intermediate join table | . -- Get counts of A and B entities per month SELECT COALESCE(A.month, B.month) AS month, -- fill null values of outer join COUNT(DISTINCT A.id) AS count_A, COUNT(DISTINCT B.id) AS count_B FROM A FULL OUTER JOIN B -- creates a large intermediate join table ON A.month = B.month GROUP BY 1 . Intermediate Join Table . A.id A.month B.month B.id . 1 | A | A | 5 | . 1 | A | A | 6 | . 2 | B | B | 7 | . 3 | B | B | 7 | . 4 | C | NULL | NULL | . NULL | NULL | D | 8 | . | Result . month count_A count_B . A | 1 | 2 | . B | 2 | 1 | . C | 1 | 0 | . D | 0 | 1 | . | Optimized: Use subqueries to divide the problem into smaller tables before joining . | . -- Get counts of A and B entities per month -- Pre-aggregate counts in A and B separately before joining SELECT COALESCE(A.month, B.month) as month, subA.count_A, subB.count_B FROM ( SELECT month, COUNT(DISTINCT id) AS count_A FROM A GROUP BY 1 ) subA FULL OUTER JOIN ( SELECT month, COUNT(DISTINCT id) AS count_B FROM B GROUP BY 1 ) subB ON subA.month = subB.month . Intermediate Join Tables . month count_A . A | 1 | . B | 2 | . C | 1 | . month count_B . A | 2 | . B | 1 | . D | 1 | . | Result . month count_A count_B . A | 1 | 2 | . B | 2 | 1 | . C | 1 | 0 | . D | 0 | 1 | . | . | The two methods get the same result, but the join using subqueries is faster because operating on smaller intermediate join tables is more efficient. Method 1 joins both tables then gets counts for A and B per month. | Method 2 gets counts for A and B per month separately, then joins the results together. | . | . Window Functions . A window is a group of related rows that are somehow related the current row, e.g. all rows with same month or same city | A window function performs calculation across a window, Unlike aggregation functions, window functions don’t group rows into a single output row No need to group by non-aggregated rows | . | Window functions are able to access more than just the current row of query result | A window function takes in a column/row and a window of related rows that includes the row | . | Aggregation functions can serve as window functions Inside OVER(), use ORDER BY to sort the window by a column | Cannot use GROUP BY with window functions | . SELECT SUM(col) OVER (...) SELECT COUNT(col) OVER (...) SELECT AVG(col) OVER (...) -- Example: Get a running total -- Using SUM as a window function -- ORDER BY to calculate chronologically SELECT duration, SUM(duration) OVER (ORDER BY start_time) AS running_total FROM rides . Window Result Durations are summed and ordered by start_time to create a running total | Without ORDER BY, each value would be sum of all seconds in its partition | . | . duration running_total . 1 | 1 | . 1 | 2 | . 2 | 4 | . 1 | 5 | . 4 | 9 | . 3 | 12 | . | Apply window function over individual groups Inside OVER(), use PARTITION BY to specify how the window function builds the window Each partition will be treated as one window, upon which the function will be applied | . | Without PARTITION BY, the whole table will be treated as one window | . -- using aggregation, cannot get individual information from all employees SELECT dept_name, MAX(salary) AS max_salary FROM employees GROUP BY dept_name -- better to use window function -- PARTITION BY specifies how to group rows to form a window SELECT E.*, MAX(salary) OVER(PARTITION BY dept_name) AS max_salary FROM employees E . The aggregation will return each department and the max salary of that department, without any other employee data. | The window function will return every employee data with the max salary of the department that employee works in The window function duplicates the department’s max salary to every row | Without PARTITION BY, the max salary of all departments will be duplicated for every row | . | . | ROW_NUMBER(): window function that returns number of row . | RANK(): window function that ranks values Like ROW_NUMBER() but gives same values when tied, e.g. 1, 2, 2, 4, 4, 4, 7 | . | DENSE_RANK(): window function that ranks values without skipping numbers Like RANK() but does not skip values, e.g. 1, 2, 2, 3, 3, 3, 4 | . SELECT team, score, ROW_NUMBER(score) OVER(PARTITION BY team ORDER BY score), RANK(score) OVER(PARTITION BY team ORDER BY score), DENSE_RANK(score) OVER(PARTITION BY team ORDER BY score) FROM table_name . Team Score ROW_NUMBER() RANK() DENSE_RANK() . A | 30 | 1 | 1 | 1 | . A | 40 | 2 | 2 | 2 | . A | 40 | 3 | 2 | 2 | . A | 55 | 4 | 4 | 3 | . B | 20 | 1 | 1 | 1 | . B | 20 | 2 | 1 | 1 | . B | 35 | 3 | 3 | 2 | . C | 20 | 1 | 1 | 1 | . -- Fetch the top 3 employees in each department by salary -- Using ROW_NUMBER() may miss out on other employees tied for top 3 salary values SELECT * FROM ( SELECT E.*, ROW_NUMBER() OVER (PARTITION BY dept_name ORDER BY salary DESC) AS salary_num FROM employee E ) sub WHERE sub.salary_num &lt; 4 -- Fetch the top employees in each department -- who earn the top 3 salaries in their department -- Returns every employee earning one of the top 3 salaries in their department SELECT * FROM ( SELECT E.*, RANK() OVER (PARTITION BY dept_name ORDER BY salary DESC) AS salary_rank FROM employee E ) sub WHERE sub.salary_rank &lt; 4 . | NTILE(n): window function that determines which bucket row falls in e.g. use n = 4 for quartile, or n = 100 for percentile | For small partition rows with less rows than tiles, the tiles will resemble a numerical ranking | . -- get quartile and percentile of student scores for each class SELECT class, score, NTILE(4) OVER (PARTITION BY class ORDER BY score DESC) AS quartile, NTILE(100) OVER (PARTITION BY class ORDER BY score DESC) AS percentile FROM table_name . | LAG(col, n): window function that creates a column that pulls from the previous n rows of col First n rows of column will be NULL because no previous rows to pull from | Some versions of SQL allow default values, i.e. LAG(col, n, default), to fill in NULL values | . | LEAD(col, n): window function that creates a column that pulls from next n rows of col Last n rows of column will be NULL because no previous rows to pull from | Some versions of SQL allow default values, i.e. LEAD(col, n, default), to fill in NULL values | . -- Example of lag and lead score, partitioned on team SELECT team, score, LAG(score, 1) OVER (PARTITION BY team ORDER BY score) AS lag_score, LEAD(score, 1) OVER (PARTITION BY team ORDER BY score) AS lead_score FROM table_name . team score lag_score lead_score . A | 20 | NULL | 30 | . A | 30 | 20 | 40 | . A | 40 | 30 | NULL | . B | 30 | NULL | 60 | . B | 60 | 30 | 70 | . B | 70 | 50 | 75 | . B | 75 | 70 | NULL | . -- Show if the salary of an employee is higher, lower, or equal to the previous employee. SELECT E.*, LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id) AS prev_emp_salary, CASE WHEN E.salary &gt; LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id THEN &#39;Higher&#39; WHEN E.salary &lt; LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id) THEN &#39;Lower&#39; ELSE &#39;Equal&#39; END AS salary_range FROM employee E . id name dept_name salary prev_emp_salary salary_range . 1 | Anna | Admin | 4000 | 0 | Higher | . 3 | Bert | Admin | 5000 | 4000 | Higher | . 4 | Dirk | Admin | 3000 | 5000 | Lower | . 6 | Cris | Admin | 6000 | 3000 | Higher | . 2 | Kurt | Finance | 4000 | 0 | Higher | . 5 | Fred | Finance | 5000 | 4000 | Higher | . 7 | Paul | Finance | 5000 | 5000 | Equal | . 8 | Evan | Finance | 2000 | 5000 | Lower | . -- Example of removing null rows using subquery SELECT sub.* FROM ( SELECT location, duration - LAG(duration, 1) OVER (PARTITION BY location ORDER BY duration) AS difference FROM rides ) sub WHERE sub.difference IS NOT NULL . | Window Aliases can name and re-use same window for several queries | should always come after the WHERE clause | . SELECT location, duration, NTILE(4) OVER ntile_window AS quartile, NTILE(100) OVER ntile_window AS percentile FROM rides WINDOW ntile_window AS (PARTITION BY location ORDER BY duration) -- window alias ORDER BY location, duration . | . SELECT INTO . Copies data from one table into a new table (in external database) . SELECT * INTO adults IN &#39;backup.db&#39; -- optional: [IN external_db] FROM users WHERE age &gt;= 18 . | . UPDATE . Modifies existing records in a table . UPDATE table_name SET col1 = val1, col2 = val2, ... WHERE condition -- without WHERE clause, all rows will be updated . | . DELETE . Deletes data in a database | . INSERT INTO . Inserts new data into a database . -- specifying which columns, values to insert INSERT INTO table_name (col1, col2, col3, ...) VALUES (val1, val2, val3, ...) -- don&#39;t need to specify if adding all columns INSERT INTO table_name VALUES (val1, val2, val3, val4, val5) . | . INSERT INTO SELECT . Copies data from one table and inserts it into another table | data types in source and destination tables must match . INSERT INTO all_profits SELECT * FROM current_profits WHERE company = &#39;Apple&#39; . | . Common Table Expressions (CTE) . A temporary named result set that can be referenced within SELECT, INSERT, UPDATE, or DELETE statements, also used in a CREATE to create a view | Advantage over subqueries: CTEs can be use multiple times in a query . -- Find average grades for every senior student WITH cte_student_grades (student, class_year, avg_grade) AS ( SELECT S.first_name + &#39; &#39; + S.last_name, YEAR(S.graduation_date), AVG(E.grade) FROM students S JOIN exams E ON S.id = E.student_id GROUP BY first_name + &#39; &#39; + last_name, YEAR(graduation_date) ) -- use CTE in a SELECT statement SELECT student, avg_grade, &#39;Senior&#39; AS class_level FROM cte_student_grades WHERE class_year = 2021 . | Use to perform multi-level aggregations, e.g. average minimum grade AVG(MIN(grade)) SQL does not allow subqueries or aggregate functions inside an aggregate function | SO you have to do MIN(grade) first in a CTE, then do AVG(min_grade) | . -- Find average min and average max exam grades across all subjects WITH min_max_grade AS ( SELECT SU.id, SU.subject_name, MIN(E.grade) AS min_grade, MAX(E.grade) AS max_grade FROM subjects SU JOIN exams E ON SU.id = E.subject_id GROUP BY SU.id, SU.subject_name ) SELECT AVG(min_grade) AS avg_min_grade, AVG(max_grade) AS avg_max_grade FROM min_max_grade; -- use CTE in a query -- Alternatively, subqueries are not as readable, reusable, -- and opposite in terms of thought process (AVG -&gt; MIN/MAX) SELECT AVG(min_grade) AS avg_min_grade, AVG(max_grade) AS avg_max_grade FROM ( SELECT SU.id, SU.subject_name, MIN(E.grade) AS min_grade MAX(E.grade) AS max_grade FROM subjects SU JOIN exams E ON SU.id = E.subject_id GROUP BY 1, 2 ) . | . Stored Procedures . A prepared SQL code that can be saved and reused, like a function . -- Storing procedure CREATE PROCEDURE SelectAllUsers AS SELECT * FROM Users GO -- Executing procedure EXEC SelectAllUsers . | A procedure can take in parameters . -- Storing procedure CREATE PROCEDURE SelectAllUsers @City nvarchar(30), @MinAge Int AS SELECT * FROM Users WHERE city = @City AND age &gt;= @MinAge GO -- Executing procedure EXEC SelectAllUsers @City = &#39;Los Angeles&#39;, @MinAge = 18 . | . SQL Database . CREATE DATABASE . Creates a new database: CREATE DATABASE testDB | . DROP DATABASE . Drop an existing database: DROP DATABASE testDB | . BACKUP DATABASE . Create full backup of existing database . BACKUP DATABASE testDB TO DISK = &#39;D: backups latest_backup.bak&#39; WITH DIFFERENTIAL -- include to only back up changes since last full backup . | . SQL Table . Creating a table . -- Create new table CREATE TABLE Users ( id int, name varchar(255), city varchar(255), age int, is_loyal boolean ) -- Create new table from another table CREATE TABLE NYC_Users AS SELECT id, name, city, age FROM Users WHERE city = &#39;New York City&#39; . | . SQL Constraints . Specifies rules for the data in the table | . Constraint Function . NOT NULL | column cannot have null values | . UNIQUE | column has all different values | . PRIMARY KEY | column is NOT NULL and UNIQUE, serves as row id | . FOREIGN KEY | prevents actions that would destroy links between tables | . CHECK | ensures column values specify a specific condition | . DEFAULT | set default value for column if no value specified | . CREATE INDEX | used to quickly create and retrieve data from database | . AUTO_INCREMENT | starting from 1, increments by 1 every time row inserted | . sql -- Create table with various constraints CREATE TABLE Users ( id int NOT NULL AUTO_INCREMENT, dept_id int NOT NULL, full_name varchar(255) NOT NULL, email varchar(255) UNIQUE, age int, city varchar(255) DEFAULT &#39;Seattle&#39;, CHECK (age &gt;= 18), PRIMARY KEY (id), FOREIGN KEY (dept_id) ) . Alter or add, delete, modify columns or various constraints of a table . -- Add column ALTER TABLE Users ADD email VARCHAR(255) -- Drop column ALTER TABLE Users DROP column is_loyal -- Change datatype ALTER TABLE Users MODIFY COLUMN city varchar(300) . | Remove a table . -- Delete a table DROP TABLE Users -- Truncate: delete data inside but not table itself TRUNCATE TABLE Users . | . SQL Index . An index is used to retrieve data more quickly, not seen by users . -- Create index CREATE INDEX idx_name ON Users (last_name) -- Create unique index CREATE UNIQUE INDEX idx_name ON Users (first_name, last_name) -- Drop index DROP INDEX idx_name ON Users . | . Views . A virtual table based on the result-set of an SQL statement . -- Create the view CREATE VIEW [Luxury Products] AS SELECT product_name, price FROM products WHERE price &gt; (SELECT AVG(price) FROM products) -- Query the view SELECT * FROM [Luxury Products] -- Update the view CREATE OR REPLACE VIEW [Luxury Products] AS SELECT product_name, price FROM products WHERE price &gt; (SELECT AVG(price) FROM products) -- Drop the View DROP VIEW [Luxury Products] . | . References .",
            "url": "https://nhtsai.github.io/notes/sql-overview",
            "relUrl": "/sql-overview",
            "date": " • Sep 6, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "System Design Overview",
            "content": "System Design . Scalability . Scalability for Dummies | A Word on Scalability | . Clones . Load balancers evenly distribute user requests to public web servers. | Rule #1: Every server contains exactly the same codebase and does not store any user-related data, like sessions or profile pictures, on local disc or memory. | Sessions need to stored in a centralized data store that is accessible by all application servers. The data store can be an external database or an external persistent cache (e.g. Redis), which will have better performance than an external database. | External means data store is somewhere in or near the data center of application servers, does not reside on application servers themselves. | . | Deployment ensures that a code change is sent to all servers without an outdated server still serving old code. | Clones are instances of an machine image based upon a “super-clone” that is created from one of your servers. Just do an initial deployment of your latest code to a new clone and everything is ready. | . | . Databases . Using cloning, you can now horizontally scale across multiple servers to handle lots of requests. | But one day, your application slows and breaks due to the MySQL database. | Path 1: Keep MySQL Apply active-passive replication strategy on the database. | Upgrade server components like RAM. | Consider sharding, denormalization, SQL tuning, etc. | Eventually the upkeep will become too expensive. | . | Path 2: NoSQL Denormalize right from the beginning. | Remove joins from any database query. | Use MySQL as a NoSQL database or switch to MongoDB. | If database requests still get too slow, consider a cache. | . | . Cache . An in-memory cache (e.g. Memcached, Redis) is a simple key-value store that resides between the application and data storage. | The application should try to read from the cache first before hitting the database because the cache is lightning fast. The cache holds every dataset in RAM and handles requests as fast as possible. | . | Cached Database Queries Whenevery a query on the database is run, store the result dataset in a cache. | Use a hashed version of query as cache key. | Expiration is a large issue. All cached results that include piece a data needed to be deleted when that piece of data changes. | . | . | Cached Objects See the data as an object or class. Let the class assemble the dataset from the database. | Store the complete instance of the class or assembled dataset into the cache. | . | Rather than storing results of multiple queries, we can aggregate the results as data for a class instance and store the instance in the cache. | . | If this ID is not present in the cache, load the data from DB, translate it and return to the client. You can cache this result by translating this data again, into the rawdata your cache has, and put it into the cache. This makes it easy to delete the object when a piece of data changes. | This makes asynchronous processing possible. Servers query the database to assemble the data in the class. | The application just serves the latest cached object and never touches the database. | . | Example Objects: user sessions, fully rendered blog articles, activity streams, user-friend relationships The blog object has multiple methods that query the database for data. | Instead of caching the result of these separate database calls, cache the entire blog object. | When something changes, the blog object queries the database for updated data. | The application only has to serve the latest cached blog post instead of querying the database. | . | . | . Asynchronism . Asynchronously doing work in advance and serving finished work with low request time. Used to turn dynamic content into static content. Think pre-rendering pages into static HTML files to be served quicker. | The rendering can be scripted to run every hour by a cronjob. | This pre-computing process helps make web applications scalable and performant | . | . | Referring unforseen, immediate requests to an asynchronous service. Upon receiving a computing intensive task, the job is added to the job queue. | The job is then sent to an asynchronous server to be processed in the background. | The results are returned once the server is done processing. | . | Basic idea: Have a queue of tasks or jobs that a worker can process. Backends become scalable and frontends become responsive. | Tools to implement async processing: RabbitMQ | . | . Performance vs Scalability . A service is scalable if it results in increased performance in a manner proportional to resources added. E.g. adding another server to handle requests speeds up the website response time | . | If the system is slow for a single user, there is a performance problem. | If the system is fast for a single user but slow under heavy load, there is a scalability problem. . | An always-on service is scalable if adding resources to facilitate redundancy does not result in loss of performance. E.g. adding multiple copies of a database for redundancy does not decrease the query response time | . | Scalability requires applications be designed with scaling in mind. | Scalability also has to handle heterogeneity. As new resources (hardware, software) come online, some nodes will be able to process faster or store more data than other nodes in a system. | . | . Latency vs Throughput . Latency is the time to perform some action or to produce some result. | Throughput is the number of such actions or results per unit of time. | Generally, you should aim for maximal throughup with acceptable latency. | . Availability vs Consistency . CAP Theorem . In a distributed system, you can only support 2 guarantees: Consistency: every read receives most recent write or an error | Availability: every request receives a response, without guarantee that it contains the most recent version of the data | Partition Tolerance: system continues to operate despite arbitrary partitioning due to network failures, e.g. a server crashes or goes offline | . | Networks aren’t reliable, so partition tolerance needs to be supported. | You need to make a software tradeoff between consistency and availability. | . Consistency and Partition Tolerance (CP) . System is consistent across servers and can handle network failures, but responses to requests are not always available. | Waiting for a response from the partitioned node might result in a timeout error. | Good choice if atomic reads and writes are required. Atomic refers to performing operations one at a time. | . | . Availability and Partition Tolerance (AP) . System is always available and can handle network failures, but the data is not always consistent or up to date across nodes. | Responses return the most readily available version of the data on any node, which might be outdated. | Writes might take some time to propagate when the partition/failure is resolved. | Good choice if eventual consistency is needed or when the system needs to continue working despite external errors. | . Consistency Patterns . With multiple copies of the same data (redundancy), how do we synchronize them across nodes (consistency) to provide all users the same view of the data? The CAP Theorem need to respond to every read with the most recent write or an error to be consistent. | . | . Weak Consistency . After a write, reads may or may not see it, and a best effort approach is taken. | Weak consistency works well for real-time use cases, such as VoIP, video chat, and realtime multiplayer video games. If you briefly lose reception during a phone call, you don’t really care or hear what was lost during connection loss. | . | Weak consistency is used in systems like memcached, where the result might or might not be there. | . Eventual Consistency . After a write, reads will eventually see it, typically within milliseconds. | Data is replicated asynchronously. | Eventual consistency works well in highly available systems. | Eventual consistency is used in systems like DNS and email. | . Strong Consistency . After a write, reads will see it. | Data is replicated synchronously. | Strong Consistency works well in systems that need transactions. | Strong consistency is used in systems like file systems and relational database management systems (RDBMSes). | . Transactions . An extended form of consistency across multiple operations. | E.g. transfering money from account A to account B Operation 1: subtract from A | Operation 2: add to B | What if something happens in between operations? E.g. Another transaction A or B, machine crashes | . | You want some kind of guarantee that the invariants will be maintained. Money subtracted from A will go back to A. | Money created will eventually be added to B. | . | . | Transactions are useful because… Correctness | Consistency | Enforce invariants | ACID: atomic, consistent, isolated, durable | . | . Availability Patterns . Fail-Over . Active-Passive Heartbeats are sent between the active server and the passive server on standby. Only the active server handles traffic. | If a heartbeat is interrupted, the passive server takes over the active server’s IP address and resumes service to maintain availability. | Downtime duration is determined by whether passive servier is already running in ‘hot’ standby or starting from ‘cold’ standby. | . | Active-Active Both servers are managing traffic, spreading load between them | If servers are public-facing, DNS needs to know about public IPs of both servers. | If servers are private-facing, application logic needs to know about both servers. | . | Disadvantages More hardware and additional complexity | Potential for loss of data if active system fails before any newly written data can be replicated to the passive | . | . Replication . Master-Slave One master node handles all writes, which are then replicated onto multiple slave nodes. | . | Master-Master Both master nodes handle all write requests, spreading load between them. The changes are then replicated onto multiple slave nodes. | . | . Availability in Numbers . Uptime or downtime is the percentage of time the service is available/not available. | Availability is generally measured in 9s, by which a service with 99.99% availability is described as having “four 9s”. . Acceptable Downtime Duration 99.9% Availability 99.99% Availability . Downtime per year | 8h 45m 57.0s | 52m 35.7s | . Downtime per month | 43m 49.7s | 4m 23.0s | . Downtime per week | 10m 04.8s | 1m 05.0s | . Downtime per day | 1m 26.4s | 08.6s | . | . Availability in Sequence . Overall availability decreases when two components with &lt; 100% availability are in sequence. | Availability(Total)=Availability(Foo)∗Availabiilty(Bar) text{Availability}( text{Total}) = text{Availability}( text{Foo}) * text{Availabiilty}( text{Bar})Availability(Total)=Availability(Foo)∗Availabiilty(Bar) | If both Foo and Bar have 99.9% availability each, their total availability in sequence would be 99.8%. | . Availability in Parallel . Overall availability increases when two componenets with &lt; 100% availabiilty are in parallel. | Availability(Total)=1−(1−Availability(Foo))∗(1−Availabiilty(Bar)) text{Availability}( text{Total}) = 1 - (1 - text{Availability}( text{Foo})) * (1 - text{Availabiilty}( text{Bar}))Availability(Total)=1−(1−Availability(Foo))∗(1−Availabiilty(Bar)) | If both Foo and Bar have 99.9% availability each, their total availability in parallel would be 99.9999%. | . Domain Name System (DNS) . A Domain Name System (DNS) translates a domain name, e.g. www.example.com, to an IP address, e.g. 8.8.8.8. DNS is hierarchical, with a few authoritative servers at the top level. | Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. | Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. | DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL). | . | A Name Server (NS) Record specifieds the DNS servers for your domain/subdomain. | A Mail Exchange (MX) Record specifies the mail servers for accepting messages. | An Address (A) Record points a name to an IP address. | A Canonical Name (CNAME) points a name to another name or to an A Record, e.g. pointing example.com to www.example.com. | Services that provide managed DNS services include: CloudFlare, Route 53, etc. | . DNS Traffic Routing Methods . Round Robin Pairs an incoming request to a specific machine by circling through a list of servers capaable of handling the request | May not result in a perfectly-balanced load distribution | . | Weighted Round Robin Each server machine is assigned a performance value, or weight, relative to the other servers in the pool, usually in an automated benchmark testing. This weight determines how many more or fewer requests are sent to that server, compared to other servers in the pool. | . | The result is a more even or equal load distribution. Prevents traffic from going to servers under maintenance. | Weights can help load balance between varying cluster sizes. | A/B Testing | . | . | Latency-Based Create latency records between servers in multiple regions. | When a request arrives, the DNS queries the NS, which looks at the most recent latency data. | The load balancer with the lowest latency is the one chosen to serve the user. | . | Geolocation-Based: Choosing servers to serve traffic based on the geographic location of users. E.g. routing all European traffic to a European load balancer | . | Can localize content and restrict content distribution based on region. | Can load balance predicatably so each user location is consistently routed to the same endpoint. | . | . Disadvantages of DNS . Accessing a DNS server introduces a slight delay, which can be mitigated by caching. | DNS server management is complex and generally managed by governments, ISPs, and large companies. | DNS services are susceptible to Distributed Denial of Service (DDoS) attacks, which prevent users from accessing websites without knowing Twitter’s IP address(es). | . Content Delivery Network (CDN) . A Content Delivery Network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files (e.g. HTML, CSS, JS, photos, videos) are served from CDNs. Some CDNs like AWS CloudFront supports serving dynamic content. | . | The website’s DNS resolution tells clients which CDN server to contact. | . | Advantages Improved performance because users receive content from data centers close to them. | Reduced load because your servers do not have to serve requests that the CDN fulfills. | . | . Push CDNs . Receive new content whenever changes occur on your server. Content is only uploaded when it is new or changed, minimizing traffic but maximizing storage. | You take full responsibility for providing content, uploading directly to the CDN, and rewriting URLs to point to the CDN. | . | You can configure when content expires and when it is updated using TTLs. | Push CDNs work well for sites with small amounts of traffic or content that isn’t often updated. Content is pushed to the CDNs when needed, instead of being re-pulled at regular intervals. | For lots of updates, pushing content to the Push CDN places load on the server. | For heavy traffic, the Push CDN’s cached content may not be sufficient and will place more load on the server to push content to the Push CDN. | . | . Pull CDNs . Grabs new content from your server when the first user requests the content. You take full responsibility for providing content and rewriting URLs to point to the CDN. | This results in slower requests until content is cached on the CDN, as users need to pull from the server upon the first request. | . | A time to live (TTL) determines the life of the cached content, which you do not typically have control of. | Pull CDNs minimizing storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed. | Pull CDNs work well for sites with heavy traffic because the traffic spread out more evenly with only recently-requested content remaining on the CDN. Older requested content is expired by the TTL, making space for new content. | For lots of updates, the Pull CDN is able to pull and cache the updated content when requested or old content is expired. | For heavy traffic, the Pull CDN can serve the most requested, cached content, only pulling from the server when for less requested content. | . | . Disadvantages of CDNs . CDN costs could be significant depending on traffic, although this should be compared against additional costs of not using a CDN. | Cached content might be stale if it is updated before the TTL expires it to be updated. | CDNs require changing URLs for static content to point to the CDN, e.g. directing facebook.com to cdn-images.fb.com. | . Load Balancer . | . Reverse Proxy . Advantages | | . | Disadvantages | . Load Balancer vs Reverse Proxy . Application Layer . Microservices . Service Discovery . Disadvantages . Database . SQL/Relational Database Management System (RDBMS) ACID | Master-Slave Replication | Master-Master Replication | Disadvantages of Replication . | Federation . | Sharding . | Denormalization . | SQL Tuning | . | NoSQL BASE | Key-Value Store | Document Store | Wide-Column Store | Graph Database | . | SQL vs NoSQL | . Cache . Client Caching . | CDN Caching . | Web Server Caching . | Database Caching Query Caching | Object Caching | . | Application Caching . | Updating Cache Cache-Aside | Write-Through | Write-Behind/Write-Back | Refresh-Ahead | . | Cache Disadvantages | . Asynchronism . Message Queues | Task Queues | Back Pressure | Aynchronism Disadvantages | . Communication . Hypertext Transfer Protocol (HTTP) | Transmission Control Protocol (TCP) | User Datagram Protocol (UDP) | Remote Procedure Call (RPC) | Representational State Transfer (REST) | RPC vs REST Calls | . Security . References .",
            "url": "https://nhtsai.github.io/notes/system-design-overview",
            "relUrl": "/system-design-overview",
            "date": " • Sep 5, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Atomic Habits",
            "content": "Atomic Habits by James Clear . 1. The Surprising Power of Atomic Habits . Small improvements aren’t notable or noticeable but are far more meaningful in the long run. A 1% improvement everyday results in 1.01365=37.781.01^{365} = 37.781.01365=37.78 change. | A 1% decline everyday results in 0.99365=0.030.99^{365} = 0.030.99365=0.03 change. | . | Habits are the compound interest of self improvement. The value of consistent, small changes are only apparent after years. Likewise, bad habits can accumulate in many missteps which compound to toxic results. | Success is the product of daily habits – not once-in-a-lifetime transformations. | . | Focus on the trajectory of habits. Outcomes are a lagging measure of your habits, built up from many small behavioral changes. | . | Time magnifies the margin between success and failure, in which effects are multiplied after a while. | . . Positive Compounds Negative Compounds . Productivity: Doing extra work builds up. | Stress: Little stresses compound into serious health issues. | . Knowledge: Lifelong learning is transformative. | Negative Thoughts: Bad thoughts can skew your interpretation of life. | . Relationships: Small interactions can build a network of strong connections. | Outrage: Small aggravations build up to mass outrage. | . Breakthrough moments are often the result of many previous actions. | Habits seem to make no difference until a critical threshold is crossed. This Valley of Disappointment is why many people give up on new habits. Compounding is not a linear improvement; the most powerful outcomes are delayed, think exponentially. | . | Habits need to persist long enough to break through the Plateau of Latent Potential in order to make meaningful difference. Change can take years – before it happens all at once. | . | Systems are about the process that lead to goals, the results you want to achieve. Goals are good for setting a direction, but systems are best for making progress. | You do not rise to the level of your goals. You fall to the level of your systems. | . | . . Problems with Goals . Winners and losers have the same goals: The difference in outcomes is due to systems of continuous improvements. | . Achieving a goal is only a momentary change: Systems tackle the underlying issues rather than temporarily achieving satisfactory results. | . Goals restrict happiness: You put off happiness until goals are met, rather than falling in love with the process. | . Goals are at-odds with long-term progress: Commitment to the process continuous improvement determines progress. | . An atomic habit is a little habit that is part of a larger system, building up to remarkable achievements through compound growth. | . 2. How Your Habits Shape Your Identity . Changing habits is challenging because you try to change the wrong thing. You should focus on building identity-based habits (3 -&gt; 2 -&gt; 1), rather than outcome-based habits (1 -&gt; 2 -&gt; 3). | . | . .   Layers of Behavioral Change   . 1. Outcomes | changing your results, setting goals | what you get | . 2. Process | changing your habits and systems | what you do | . 3. Identity | changing your beliefs, assumptions, and biases | what you believe | . Behavior that is incongruent with the self will not last. You need to change the underlying beliefs that led to past behavior in order to change existing habits. | You should change identities to more easily align with new behaviors and habits. | . | However, you should not be so attached to any specific identity as to prevent further behavioral improvements. Progress requires unlearning. | . | Repeated behaviors contribute more evidence supporting a gradual belief in a new identity and self-image. | . . Process of Changing your Identity . 1. Decide the type of person you want to be. What kind of qualities do they have? | . 2. Prove it to yourself with small wins. | . Your habits shape your identity, and your identity shapes your habits, working in a feedback loop. You should let values, principles, and identity drive this loop, rather than results. | . | Building habits is fundamentally about becoming someone, changing your beliefs about yourself. | . 3. How to Build Better Habits in 4 Simple Steps . Edward Thorndike’s studies on cat behavior shows that behaviors that result in favorable outcomes are more repeated than behaviors that result in unfavorable outcomes. | A habit is a behavior that has been repeated enough times to become automatic. Useful actions are reinforced into habits through a try, fail, learn, try differently feedback loop. | . | Brain activity decreases as it learns the cues that predict success, applying automatic solutions when the conditions are right. The conscious mind can only pay attention to one problem at a time. | Habits reduce cognitive load and free up mental capacity, so you can allocate your attention to other tasks. | Habits free your mind to focus on new challenges, allowing you to solve problems with as minimal energy and effort as possible. | . | . .   The Stages of Habit . 1. Cue (Problem Phase) | Triggers brain to initiate a behavior, predicts a reward. | . 2. Craving (Problem Phase) | Motivational force or desire to act, linked to a desired change of internal state. | . 3. Response (Solution Phase) | The habit (thought or action) you perform, depends on level of motivation and ability. | . 4. Reward (Solution Phase) | The end goal of every habit, satisfies cravings and teaches which actions are worth remembering. | . If behavior is insufficient at any of the four stages, the habit will not be formed. | Habit Loop is a neurological feedback loop that ultimately allows you to create automatic habits. The cue triggers a craving, which motivates a response, which provides a reward, which satisfies the craving, and, ultimately, becomes associated with a cue. | The problem phase (cue and craving) indicates when something needs to change. | The solution phase (response and reward) indicates when you take action and achieve the desired change. | . | The Four Laws of Behavior Change is a simple set of rules used to design good habits and eliminate bad ones. | . . Creating a Good Habit Breaking a Bad Habit . 1st Law (Cue): Make it obvious. | Inverse 1st Law (Cue): Make it invisible. | . 2nd Law (Craving): Make it attractive. | Inverse 2nd Law (Craving): Make it unattractive. | . 3rd Law (Response): Make it easy. | Inverse 3rd Law (Response): Make it difficult. | . 4th Law (Reward): Make it satisfying. | Inverse 4th Law (Reward): Make it unsatisfying. | . 4. The Man Who Didn’t Look Right . The brain is continuously analyzing information and your surroundings, picking up on important cues to predict certain outcomes subconciously. | You don’t need to be aware of the cue for a habit to begin. Habits are formed under the direction of your automatic and nonconscious mind. | . | Habits are hard to change because they are so mindless and automatic. | Pointing-and-Calling is the safety system of pointing and calling things out loud in order to reduce mistakes by 85%. It is effective because it raises a nonconscious habit to a more conscious level. | The Habits Scorecard is a similar exercise to maintain awareness of your habits and routines. For each action in a list of daily habits, you write a “+”, “-“, or “=” if it’s a good, bad, or neutral habit, respectively. | The rating is depending on your situation and your goals. “Good” refers to habits that have net positive outcomes and reinforce your desired identity. | . | The goal is to observe your thoughts and actions without any judgements to notice any nonconscious habits. Then you can point it out and say out loud the action and its outcome. | . | Behavior change starts with awareness, recognition of habits, and acknoledgement of the cues that trigger them | . 5. The Best Way to Start a New Habit . An implementation intention is a plan made beforehand about when and where to act, detailing how you intend to implement a habit. Implementation intentions leverage both time and location cues, in the form: “When situation X arises, I will perform response Y.” | . | People who make a specific plan for when and where they will perform a new habit are more likely to follow through. A lack of clarity, not motivation, is what prevents people from making improvements because it’s not always obvious when and where to take action. | Being specific about what you want and how you will achieve it helps you avoid any distractions or obstacles. | . | . . Implementation Intention I will [BEHAVIOR] at [TIME] in [LOCATION]. . Sleeping | I will sleep for 7 hours at 12AM in my bed. | . Studying | I will study math for 30 minutes at 6PM in my bedroom. | . Exercise | I will exercise for 1 hour at 5PM in my local gym. | . Relationship | I will talk to someone I know for 1 hour at 3PM in my living room. | . The Diderot Effect states that obtaining a new possession often creates a spiral of consumption that leads to additional purchases. Many human behaviors follow this principle, deciding on what to do next based on what you have just done; each action becomes a cue that triggers the next behavior. | . | Habit Stacking is a special implementation intention that pairs a new habit with a current habit, rather than a particular time and location. | . . Habit Stacking After [CURRENT HABIT], I will [NEW HABIT]. . Sleeping | After I brush my teeth, I will go to sleep for 7 hours. | . Studying | After I eat breakfast, I will study for 2 hours. | . Exercise | After I finish my work, I will exercise for 1 hour. | . Relationship | After I eat dinner, I will text a friend or family member to catch up. | . Habit Stacking allows you to create larger stacks by chaining small habits together, creating a natural momentum of behaviors. Example Morning Routine Stack: After making a cup of coffee, I will meditate for 60 seconds. | After meditating, I will write my day’s to-do list. | After writing my to-do list, I will immediately begin my first task. | | Example Evening Routine Stack: After finishing my dinner, I will clean the dirty dishes. | After cleaning the dishes, I will wipe down the counter. | After wiping down the counter, I will prepare my coffee mug for tomorrow morning. | | . | Habit Stacking works best when the cue if highly specific and immediately actionable. It’s important to select the right cue to kick things off. It should have the same frequency as your desired habit. | You can create a list of your daily habits and a list of daily occurrences to formulate the best layering of your daily habit stack. | . | The 1st Law of Behavior Change is make it obvious. Implementation Intentions and Habit Stacking help create obvious cues for you habits and design a clear plan for when and where to take action. | . | . 6. Motivation is Overrated; Environment Often Matters More . Anne Thorndike’s studies on environmental design shows that your habits change depending on your environment and the cues in front of you. “Every habit is context dependent.” | . | Kurt Lewin’s equation B=f(P,E)B=f(P,E)B=f(P,E) describes how Behavior is a function of the Person in their Environment. | Hawkins Stern’s Suggestion Impulse Buying describes how customers occasionally buy products due to presentation over desire. | Vision is the most important sensory ability, so small changes in contexts, or visual surroundings, can greatly impact your actions. | Obvious visual cues can trigger behaviors more often and better form habits. Creating multiple cues can increase the odds of triggering a behavior. | Environment design is altering living and work spaces to increase exposure of positive cues and decrease exposure to negative cues. | . | Over time, habits will become associated with the entire context surrounding the behavior, and the context becomes the cue. | Habits can be easier to change or form in a new environment, rather than building habits in the face of competing cues and old contexts. Create separate spaces (rooms or activity zones) for different activities. One space, one use. | . | Habits easily form in stable, predictable environments where everything has a place and a purpose. | . 7. The Secret to Self-Control . The Vietnam War Studies showed that soldiers addicted to heroin got rid of their addiction overnight after returning to an environment devoid of the cues triggering heroin abuse. This finding challenged the conventional association of unhealthy behavior as a moral weakness or lack of discipline. | . | You don’t need tremendous willpower and self-control if you don’t spend lots of time resisting temptations. Creating a more disciplined environment is important. | Habits encoded in the brain can arise again once the internalized cues are present. | . | To eliminate a bad habit, exposure to its cues need to be reduced. | Self-control is a short-term strategy that fails often when willpower is needed to resist temptations in a negative environment. | . 8. How to Make a Habit Irresistible . Niko Tinbergen’s experiments on North American sea birds showed that the brains are preloaded with certain rules that activate stronger than usual when exaggerated cues, or supernormal stimuli, are present. An example of supernormal stimuli is junk food for humans who have developed a craving for salt, sugar, and fat over years of evolution. | Nowadays, this craving is no longer advantageous to our health. | . | Food science studies how to make food more attractive to consumers Orosensation is how a product feels in your mouth. | Dynamic contrast refers to the combination of sensations that can make foods more interesting to eat. | . | The more attractive an opportunity is, the more likely it is to become habit-forming. | James Olds and Peter Milner showed how dopamine controls craving and desire. Habits are dopamine-driven feedback loops. | Dopamine is released both when anticipating pleasure and experiencing pleasure. | The anticipation (“wanting”) of a reward is what motivates action to fulfillment (“liking”) of a reward. | The greater the anticipation, the greater the dopamine spike. | . | Temptation bundling makes habits more attractive by linking an action you want to do with an action you need to do. Over time, the reward gets associated with the cue, and the habit becomes more attractive. | . | Premack’s Principle states that more profitable behaviors will reinforce less probable behaviors. You become conditioned to do undesirable tasks if it means you also get to do a rewarding task. | . | . . Habit Stacking + Temptation Bundling . After [CURRENT HABIT], I will [HABIT I NEED]. | . After [HABIT I NEED], I will [HABIT I WANT]. | . 9. The Role of Family and Friends in Shaping Your Habits . Laszlo Polgar showed how deliberate practice and good habits can overcome innate talent. | The culture you live in determins which behaviors are attractive to you. Humans desire to belong, so earliest habits are imitated and heavily influenced by surrounding culture and family. | . | Humans mainly imitate the close, the many, and the powerful. The Close (Friends &amp; Family) | Proximity greatly impacts behavior and habits, and family and friends provide an invisible peer pressure. | Join a culture where your desired behavior is the normal behavior and you already have something in common with the group, transforming your individual goal to a shared goal. The Many (The Tribe) | | Solomon Asch showed how the behavior of the many can often override the behavior of the individual. | Humans desire to fit in, so matching the tribe to the behavior makes change very attractive. The Powerful (Status and Prestige) | | Humans are attracted to behaviors that earn respect, approval, admiration, and status. | Humans are not attracted to behaviors that lower status or are negatively-viewed culturally. | . | . 10. How to Find and Fix the Causes of Your Bad Habits . Every behavior has a surface level craving and a deeper, underlying motive. A craving is a specific manifestation of a deeper underlying motive, a desire to change your internal state. | A desire is the difference between a current state and an ideal future state. | . | . . Craving Underlying Motive . Using Tinder | Find love and reproduce | . Browsing Facebook | Connect and bond with others | . Posting on Instagram | Win social acceptance and approval | . Searching on Google | Reduce uncertainty | . Playing video games | Achieve status and prestige | . There are many different ways to address the same underlying motive. | Habits are associations between an observed cue and a predicted response, heavily influencing behaviors. Feelings and emotions transform the cues and predictions into applicable signals. | The cause of habits is the preceding prediction, which leads to a feeling. | . | Reframing habits to highlight their benefits (positive) rather than their drawbacks (negative) can change your mindset and make a habit more attractive. A motivation ritual refers to the association of habits with something enjoyable, which can then later be used as a cue for motivation. | Eventually the associated habit becomes a cue to something enjoyable. | . | If you can reprogram your predictions, you can transform a hard habit into an attractive one. | . 11. Walk Slowly, but Never Backward . Taking action is better than being in motion, practice over planning. You want to delay failure, so you avoid taking action. | It’s better to take action and iteratively improve your results. | Preparation or planning can become a form of procrastination. | The key is to start with repetition, not perfection. | . | Habit formation is the process by which a behavior becomes progressively more automatic through repetition. Long-term Potentiation refers to the strengthening of connections between neurons in the brain based on recent patterns of activity. | Hebb’s Law is “Neurons that fire together wire together.” | . | Repetition is a form of change. To build a habit, you need to practice it. Repeating a habit leads to physical changes in the brain, different adaptations for different kinds of tasks. | Active practice and iteration is more effective than passive learning and theorizing. | Automaticity is the ability to perform a behavior without thinking about each step, which occurs when the nonconscious mind takes over after crossing the habit line. | . | Learning curves plot automaticity vs. time spent, revealing that habits form based on frequency, not time. This means the amount of time you have been performing a habit is less important than the number of times you have performed the habit successfully. | . | . 12. The Law of Least Effort . It is human nature to follow the Law of Least Effort, which states that when deciding between two similar options, the option that requires less work is more attractive. The most value for the least effort is most attractive. | The most common behaviors are extremely convenient and require little effort, e.g. checking social media or watching TV. | . | Habits are obstacles to desired outcomes. If good habits can be made more convenient, then they are more likely performed. | The idea is to make it as easy as possible in the moment to do things that payoff in the long run. | . | One way to reduce friction is environment design, making cues more obvious and optimizing for convenient actions. Addition by subtraction, or lean production, is the strategy of relentlessly eliminating points of friction that waste time or energy and optimizing to achieve more with less effort. | . | .",
            "url": "https://nhtsai.github.io/notes/atomic-habits",
            "relUrl": "/atomic-habits",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Making a Pull Request",
            "content": "Making a Pull Request . Fork the project you want to work on to get your own copy of the repository. . | Clone your copy of the repository using HTTPS or SSH. . | git clone forked_repo.git . Create a new branch to work on a separate branch. | git checkout origin/master -b fix_bug . Make any modifications to the code, e.g. bug fix, new feature, etc. . | Preview your changes. . | git diff . Check the state of the repository. Files should be modified but untracked. | git status . Track the modified files. | git add file.txt . Commit the changes. | git commit -m &quot;Fixed typo in file.txt&quot; . Check the state of the repository. Files should be modified and changes committed. | git status . Push your new branch up to your remote forked repository. | git push origin HEAD . HEAD is a a shortcut for your current checked-out branch, aka fix_typo. . Open a pull request from your fork. . | Read any contributing guidelines and rules of conduct. . | Review the changes in your pull request. . | Create a pull request if everything looks good. . | References . Anthony Explains #004 Video | .",
            "url": "https://nhtsai.github.io/notes/making-a-pull-request",
            "relUrl": "/making-a-pull-request",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Introduction to Apache Cassandra",
            "content": "Introduction to Apache Cassandra . Supplemental Video Lecture . . Relational Overview . Can RDBMS work for big data? . Replication makes ACID a lie Given a read-heavy workload, we want to create a replicated slave node from the master node. However, data is replicated asynchronously, which introduces replication lag. This means that if the client reads from the replicated slave node, old data will be returned and results are not consistent (ACID fails). | . | Third Normal Form doesn’t scale Unpredictable queries | . | Sharding is a nightmare Data is split all over the place, no more joins or aggregations. | Querying second indexes requires hitting every shard (non-performant), so we are forced to denormalize all things. | Keeping shards consistent, adding shards, or changing schemas requires custom scripts. | . | Low Availability Who is responsible for Master failovers? Whether manual or automatic, failovers need to be detected. | . | Multiple datacenter solutions are messy. | Downtime is frequent. | . | . Failure of RDBMS . Scaling is a mess | ACID naive at best (not consistent). | Re-sharding is a manual process. | Denormalize all 3rd normal form queries for performance. | High availability is complicated to achieve, requires additional operational overhead. | . Lessons Learned for a New Solution . Consistency is not practical –&gt; give up consistency. | Manual sharding and rebalancing is hard –&gt; build it into the cluster. | Every moving part makes systems more complex –&gt; simplify the architecture, no more master-slave. | Scaling up is expensive –&gt; only use commodity hardware, more affordable. | Scatter/Gather queries are not good –&gt; denormalize for real-time query performance, always hit 1 machine. | . Cassandra Overview . What is Apache Cassandra? . Fast Distributed Database: | High Availability: able to be accessed at all times | Linear Scalability: linear scale-up in performance | Predictable Performance: can guarantee service-level agreements (SLA) with very low latency | No single point of failure (SPOF): can withstand multiple points of failure | Multi-Datacenter: can be utilized across multiple datacenters out of the box | Commodity Hardware: can be implemented on affordable hardware when vertically scaling | Easy to manage operationally: can manage no matter the cluster size | Not a drop-in replacement for RDBMS: applications must be designed using Cassandra | . Hash Ring Analogy . No master/slave/replica sets | No config servers, zookeeper | Data is partitioned around the ring | Data is replicated to replication factor (RF=N) servers | All nodes hold data and can answer queries (both read &amp; write) | Location of data on ring is determined by partition key Partition key is a hash of the primary key. | . | . CAP Tradeoffs . CAP Theorem says that when a network partition failure happens, we can either: Cancel the operation and thus decrease the availability but ensure consistency | Proceed with the operation and thus provide availability but risk inconsistency | . | Latency between data centers makes consistency impractical, especially considering datacenters across the world. | Cassandra chooses availbility and partition tolerance over consistency. | . Replication . Data is replicated automatically and asynchronously. | Choose total number of servers to replicate data to, replication factor (RF), usually 3. Set when creating the keyspace, a group of Cassandra tables, like schema. | . | Data is always replicated to each replica. | If a machine is down during replication, missing data writes are replayed via hinted handoff. | . Consistency Levels . Consistency level is set on per query basis, can be configured for read or write. | ALL: all (100%) of replicas must confirm read/write to be considered successful. | QUORUM: majority (51%) of replicas must confirm read/write to be considered successful. | ONE: only 1 replica needs to confirm read/write to be considered successful, allows for fast read/write successes. | . Multiple Datacenters . Write to local datacenter (DC) and replicate asynchronously to other datacenters. Local Consistency Levels: QUORUM –&gt; LOCAL QUORUM | . | Replication factor per keyspace per datacenter. Can configure different RF for each datacenter. | . | Datacenters can be physical or logical. Can use one DC as OLTP for fast reads and one virtual DC for OLAP queries. | . | . Cassandra Internals and Choosing a Distribution . Write Path . Writes are written to any node in the cluster, which serves as the coordinator for the write request. | Writes are written to a commit log, an append-only, immutable data structure for durability. | Merges the write mutation to the memtable, an in-memory representation of the table. | Now Cassandra can respond to client about successful write. This simplicity in write path is why Cassandra is fast. | Every write includes a timestamp. | . | When memory is full, the memtable is serialized into an immutable SSTable and flushed to disk periodically. | New memtable is created in memory. | Cassandra never does any updates or deletes in-place. | Deletes are a special write case known as tombstone, a marker with a timestamp to say that there is no data at that location at that time. | . What is an SSTable? . An SSTable is an immutable data file for row storage. | Every write includes a timestamp of when it was written. | Partition is spread across multiple SSTables. | Same column can be in multiple SSTables. | Merged through compaction, taking small SSTables and merging them into bigger ones. Last write wins means for a row with many changes, only latest timestamp is kept so only the latest version of the row is saved. | . | Deletes are written as tombstones. | Easy backups since SSTables can be copied off to another server. | . Read Path . Any server/node may be queried, and it acts as the coordinator. | The coordinator then contacts nodes with the requested key in the read query. | On each node, data is pulled from SSTable(s) on disk and merged using the latest timestamp, like compaction. The disk type (SSD or hard drive) has a large impact on the speed and performance of Cassandra. | . | Consistency levels under ALL performs read repair in the background Sometimes nodes can disagree about the value of a given piece of data since Cassandra is eventually consistent. | read_repair_chance specifies the chance that Cassandra will update/sync information on all other replicas, default is around 10% chance of all reads. | . | . Open Source Distribution . Latest, bleeding edge features, perfect for hacking. | File JIRAs issues and bug reports for support. | Support via mailing list &amp; IRC. | . DataStax Enterprise Distribution . Open source Apache Cassandra at its core. | Focused on stable releases for enterprise. | Integrated Multi-Datacenter Search (for OLTP) | Integrated Spark or Hadoop for Analytics (for OLAP). | Free Startup Program (&lt; 3MM revenue, &lt; 30M funding). | Extended support, additional QA. | .",
            "url": "https://nhtsai.github.io/notes/cassandra-intro",
            "relUrl": "/cassandra-intro",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Learning Object-Specific Distance From a Monocular Image",
            "content": "Learning Object-Specific Distance From a Monocular Image . Research paper by Jing Zhu, Yi Fang, Husam Abu-Haimed, Kuo-Chin Lien, Dongdong Fu, and Junli Gu 1. . Abstract . TODO . Introduction . In computer vision research for self-driving cars, researchers have not recognized the importance of environment perception in favor of more popular tasks, like object classification, detection, and segmentation. Object-specific distance estimation is important for car safety and collision avoidance, and the lack of deep learning applications is likely due to the lack of datasets with distance measures for each object in the scene. . Current self-driving systems predict object distance using the traditional inverse perspective mapping (IPM) algorithm. This method first locates a point on the object (usually on the lower edge of the bounding box), then projects the located point onto a bird’s-eye view coordinate map using camera parameters, and finally estimates the object distance using the constructed bird’s-eye view coordinate map. This simple method can provide reasonable distance estimates for objects close and in front of the camera, but it performs poorly when objects are located to the sides of the camera or curved roads and when objects are over 40 meters away. . The authors first sought “to develop an end-to-end learning based approach that directly predicts distances for given objects in the RGB images.” End-to-end meaning that object detection and distance estimation parameters are trained jointly. The base model extracts features from RGB images, then utilizes region of interest (ROI) pooling to generate a fixed-size feature vector for each object, and feeds the ROI feature vectors into a distance regressor to predict a distance (Z) for each object. However, this method was not sufficiently precise for self-driving. . The authors then created an enhanced model with a keypoint regressor to predict (X,Y) of the 3D keypoint coordinates (X,Y,Z). Leveraging the camera projection matrix, the authors defined a projection loss between the projected 3D point (X,Y,Z) and the ground truth keypoint (X*,Y*,Z*). The keypoint regressor and projection loss are used for training only. During inference, the trained model takes in an image with bounding boxes and outputs the object-specific distance, without any camera parameters invervention. . The authors constructed an extended dataset from the KITTI object detection dataset and the nuScenes mini dataset by “computing the distance for each object using its corresponding LiDAR point cloud and camera parameters.” . Enhanced model performs better at distance estimation, compared to the traditional IPM algorithm and the support vector regressor, is also more precise than the base model, and is twice as fast during inference than IPM algorithm. . Summary . Base model: use deep learning to predict distance from given objects on RGB image without camera parameter intervention | Enhanced model: base model with keypoint regressor and new projection loss | Dataset: extended KITTI and nuScenes mini object-specific distance datasets | . Related Work . Distance Estimation . Inverse Perspective Mapping (IPM): convert a point or a bounding box in the image to the corresponding bird’s-eye view coordinate | Support Vector Regressor: predict object-specific distance given width and height of a bounding box | DistNet: use YOLO for bounding boxes prediction instead of image features learning for distance estimation, the distance regressor studied geometric mapping from bounding box with certain width and height to distance value In contrast, this paper directly predicts distances from learned image features. | . | Marker-based Methods: use auxiliary information, create segmented markers in the image and estimate distance using the marker area and camera parameters | Calibration Patterns: predict physical distance based on rectangular pattern, where 4 image points are needed to compute camera calibration | . 2D Visual Perception . R-CNNs: boost accuracy, decrease processing time for object detection, classfiication, dsegmentation | SSD and YOLO: end-to-end frameworks to detect and classify objects in RGB images | Monocular Depth Estimation: predict dense depth maps for given monocular color images | . Methods . The authors propose a base model that predicts physical, object-specific distance from given RGB images and object bounding boxes and an enhanced model with keypoint regressor for improved distance estimation. . Base Method . . Feature Extractor . Extract feature map for entire RGB image using image feature learning network. | Use existing architecture (e.g. vgg16, resnet50) as feature extractors. | Output of last layer is max-pooled and extracted as feature map for input RGB image. | . Distance Regressor and Classifier . Feed feature map and object bounding boxes into ROI pooling layer to generate fixed-size feature vector Fi boldsymbol{F_i}Fi​ for each object in the image. | Pass pooled object feature vector into distance regressor for predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) and object classifier for predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Distance Regressor: 3 fully-connected (FC) layers, {2048, 512, 1} for vgg16 and {1024, 512, 1} for resnet50. Softplus activation layer applied on output of last FC layer to ensure predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) is positive. | . | Distance Classifier: A FC layer with (number of categories in dataset) neurons, then softmax function to get predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Loss Functions Regressor Loss: . Ldist=1N∑i=1NL1;smooth(D(Fi),di∗)L_{dist}= frac{1}{N} sum_{i=1}^{N} L_{ text{1;smooth}}(D( boldsymbol{F_i}), d^{*}_{i})Ldist​=N1​i=1∑N​L1;smooth​(D(Fi​),di∗​) L1;smooth={0.5(xi−yi)2/β,if ∣xi−yi∣&lt;β∣xi−yi∣−0.5∗β,otherwiseL_{ text{1;smooth}} = begin{cases}0.5(x_{i} - y_{i})^2/ beta, &amp; text{if $|x_{i}-y_{i}|&lt; beta$} |x_{i}-y_{i}|-0.5* beta, &amp; text{otherwise} end{cases}L1;smooth​={0.5(xi​−yi​)2/β,∣xi​−yi​∣−0.5∗β,​if ∣xi​−yi​∣&lt;βotherwise​ | Classifier Loss: . Lcla=1N∑i=1NLcross-entropy(C(Fi),yi∗)L_{cla}= frac{1}{N} sum_{i=1}^{N} L_{ text{cross-entropy}}(C( boldsymbol{F_i}), y^{*}_{i})Lcla​=N1​i=1∑N​Lcross-entropy​(C(Fi​),yi∗​) Lcross-entropy=−∑i=1Myi∗∗log(C(Fi))L_{ text{cross-entropy}}=- sum_{i=1}^{M} y^{*}_{i} * log(C( boldsymbol{F_i}))Lcross-entropy​=−i=1∑M​yi∗​∗log(C(Fi​)) | NNN: number of objects in the image | MMM: number of categories | β betaβ: smooth loss scaling factor, usually 1.01.01.0 | D(Fi)D( boldsymbol{F_i})D(Fi​): predicted distance | C(Fi)C( boldsymbol{F_i})C(Fi​): predicted category label | di∗d^{*}_{i}di∗​: ground truth distance of the iii-th object | yi∗y^{*}_{i}yi∗​: ground truth category label of the iii-th object | . | . Model Learning and Inference . Train feature extractor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}minLbase​=Lcla​+λ1​Ldist​ Set λ1=1.0 lambda_{1}=1.0λ1​=1.0 during training. | . | Use ADAM optimizer with beta value β=0.5 beta=0.5β=0.5, learning rate of 0.001, exponentially decayed after 10 epochs. | Classifier encourages model to learn features used in estimating more accurate distances, only used during training. | After training, base model can predict object-specific distances given RGB images and object bounding boxes as input. | . Enhanced Method . . Add keypoint regressor to optimize base model by introducing projection constraint for better distance prediction. . Feature Extractor . Same architecture as base model: vgg16 or resnet50 into ROI pooling layer for object specific features Fi boldsymbol{F_i}Fi​. | . Keypoint Regressor . Keypoint regressor KKK learns to predict approximate keypoint position in 3D camera coordinate system. | Distance regressor predicts Z coordinate, while keypoint regressor predicts (X, Y). | Keypoint Regressor: 3 fully-connected (FC) layers, {2048, 512, 2} for vgg16 and {1024, 512, 2} for resnet50. | Since there is no ground truth of 3D keypoint available, project the generated 3D point (X,Y,Z)=([K(Fi),D(Fi)])(X,Y,Z)=([K( boldsymbol{F_i}), D( boldsymbol{F_i})])(X,Y,Z)=([K(Fi​),D(Fi​)]) back to image plane using camera projection matrix PPP. | Compute errors between ground truth 2D keypoint ki∗k^{*}_{i}ki∗​ and projected point P⋅([K(Fi),D(Fi)])P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})])P⋅([K(Fi​),D(Fi​)]). | Keypoint Function . L3Dpoint=1N∑i=1N1di∗∥P⋅([K(Fi),D(Fi)])−ki∗∥2L_{3Dpoint}= frac{1}{N} sum_{i=1}^{N} frac{1}{d^{*}_{i}} | P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})]) - k^{*}_{i} |_{2}L3Dpoint​=N1​i=1∑N​di∗​1​∥P⋅([K(Fi​),D(Fi​)])−ki∗​∥2​ Use weight with regard to ground truth distance to encourage better predictions for closer objects. | . | . Distance Regressor and Classifier . Same architecture as base model and training losses LdistL_{dist}Ldist​ and LclaL_{cla}Lcla​. | Distance regressor parameters also optimized by projection loss L3DpointL_{3Dpoint}L3Dpoint​. | . Model Learning and Inference . Train feature extractor, keypoint regressor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist+λ2L3Dpoint text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}+ lambda_{2}L_{3Dpoint}minLbase​=Lcla​+λ1​Ldist​+λ2​L3Dpoint​ Distance loss weight constant: lambda1=10.0lambda_{1}=10.0lambda1​=10.0 | Keypoint loss weight constant: lambda2=0.05lambda_{2}=0.05lambda2​=0.05 | . | Use same optimizer, beta, and learning rate as the base model. | Training Only: use camera projection matrix PPP, keypoint regressor, and object classifier. | Testing: Given RGB image and bounding boxes, directly predicts object-specific distances without any camera parameter intervention. | Both models trained for 20 epochs with batch size of 1 on the training subset, augmented with horizontally-flipped training images. | After training, input RGB image with bounding boxes into trained model to get the output of the distance regressor as the estimated object-specific distance in the validation subset. | . Dataset Construction . KITTI and nuScenes mini both provide RGB images, 2D/3D bounding boxes, category labels for objects in the images, and the corresponding velodyne point cloud for each image. . Object Distance Ground Truth Generation . Use 3D bounding boxes to segment velodyne point cloud for each object. | Sort the segmented points based on depth values. | Extract the n-th depth value as object-specific ground truth distance, where n=0.1∗(number of segmented points)n=0.1*( text{number of segmented points})n=0.1∗(number of segmented points) to avoid extracing depth values from noise points. | Project velodyne points to corresponding RGB image planes and get their image coordinates as keypoint ground truth distance. | Append both ground truths to the object detection dataset labels. | KITTI . Split KITTI into training (3,712 RGB images, 23,841 objects) and validation sets (3,768 RGB images, 25,052 objects) using 1:1 split ratio. | All KITTI objects are categorized into 9 classes, i.e. Car, Cyclist, Pedestrian, Misc, Person_sitting, Tram, Truck, Van, DontCare. | Generated KITTI ground truth distances should be between [0, 80] meters. | . nuScenes mini . Split nuScenes mini into training (200 images, 1,549 objects) and validation sets (199 images, 1,457 objects). | All nuScenes mini objects are categorized into 8 classes, i.e. Car, Bicycle, Pedestrian, Motorcycle, Bus, Trailer, Truck, Construction_vehicle. | Generated nuScenes mini ground truth distances should be between [2, 105] meters. | . Evaluation . Evaluation Metrics . Use the same metrics as depth prediction. Let di∗d^{*}_{i}di∗​ and did_{i}di​ denote the ground truth distance and the predicted distance, respectively. . Threshold . % of di s.t. max(di/di∗,di∗/di)=δ&lt;threshold % text{ of } d_i text{ s.t. max}(d_i/d^{*}_{i},d^{*}_{i}/d_i)= delta&lt; text{threshold}% of di​ s.t. max(di​/di∗​,di∗​/di​)=δ&lt;threshold | Absolute Relative Difference . Abs Rel=1N∑i=1N∣di−di∗∣/di∗ text{Abs Rel}= frac{1}{N} sum_{i=1}^{N}|d_{i}-d^{*}_{i}|/d^{*}_{i}Abs Rel=N1​i=1∑N​∣di​−di∗​∣/di∗​ | Squared Relative Difference . Squa Rel=1N∑i=1N∥di−di∗∥2/di∗ text{Squa Rel}= frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}/d^{*}_{i}Squa Rel=N1​i=1∑N​∥di​−di∗​∥2/di∗​ | RMSE (linear) . RMSElinear=1N∑i=1N∥di−di∗∥2 text{RMSE}_{ text{linear}}= sqrt{ frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}}RMSElinear​=N1​i=1∑N​∥di​−di∗​∥2​ | RMSE (log) . RMSElog=1N∑i=1N∥log⁡di−log⁡di∗∥2 text{RMSE}_{ text{log}}= sqrt{ frac{1}{N} sum_{i=1}^{N} | log{d_{i}}- log{d^{*}_{i}} |^{2}}RMSElog​=N1​i=1∑N​∥logdi​−logdi∗​∥2​ | . Note: don’t include distances predicted for DontCare objects when calculating errors. . Compared Approaches . Inverse Perpective Mapping Algorithm (IPM): predicts object-specific distance by approximating a transformation matrix between a normal RGB image and its bird’s-eye view image using camera parameters Use IPM from MATLAB’s computer vision toolkit to get transformation matrices for RGB images from the validation subset. | Project middle points of the lower edges of the object bounding boxes into bird’s-eye view coordinates using the transformation matrices. | Take values along forward direction as estimated distances. | . | Support Vector Regressor (SVR): predicts object-specific distance given width and height of a bounding box Compute width and height of each bounding box in the training subset. | Train SVR with the ground truth distance. | Input widths and heights of each bounding box in the validation set to get estimated object-specific distances. | . | . Results . Both proposed models have much lower relative errors and higher accuracies, compared to IPM and SVR, on KITTI. | Enhanced model performs the best, implying effectiveness of keypoint regressor and projection constraint, on both KITTI and nuScenes mini. | . References . J. Zhu, Y. Fang, H. Abu-Haimed, K. Lien, D. Fu, and J. Gu. Learning Object-Specific Distance from a Monocular Image. arXiv:1909.04182, 2019. &#8617; . |",
            "url": "https://nhtsai.github.io/notes/distance-estimation",
            "relUrl": "/distance-estimation",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a university graduate interested in data science and machine learning. Currently looking for internship and job opportunities. .",
          "url": "https://nhtsai.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nhtsai.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}