{
  
    
        "post0": {
            "title": "Introduction to Probability Exercises",
            "content": "Introduction to Probability 2ed. by Blitzstein &amp; Hwang . View the book here or here. . Chapter 1 . Counting . There are $11!$ total permutations. Accounting for overcounting of repeated letters that can be permuted in any order (‘S’, ‘I’, ‘P’), we need to divide by $4! times 4! times 2!$. We find there are $ frac{11!}{4!4!2!}$ permutations. We can also think of this in terms of a binomial coefficient, choosing positions for letters, to find $ binom{11}{4} binom{7}{4} binom{3}{2}$ permutations. | For the first digit, there are 8 possible digits (2-9). For the remaining six digits, there are 10 possible digits (0-9). Using the multiplication rule, there are $8 times 10^{6}$ numbers. | We need to find the number of 7-digit phone numbers that cannot start with 0 or 1 in the first digit and cannot start with 911. We can subtract the complement from the total, or excluding all numbers that start with 0 or 1 or 911. We find there are $10^{7} - (2 times 10^{6}) - (1 times 1 times 1 times 10^{4})$ numbers. | | We can use the multiplication rule to get $5 times 4 times 3 times 2 times 1$ possibilities, where Fred must eat a new restaurant each day. | We can use the multiplication rule to get $5 times 4 times 4 times 4 times 4$ possibilities, where Fred can eat at any restaurant on Monday and restaurants excluding the one from the day before on subsequent days. | | For one game, there are $2$ possible outcomes. There are $2^{ binom{n}{2}}$ possible outcomes. | We can think of this as the number of ways to pick 2 players with replacement, where order does not matter. Using binomial coefficients, we find $ binom{n}{2}$ outcomes, one for each match. | | There are $2^{n}$ players with half of the players eliminated per round, so there are $ log_{2}2^{n} = n$ rounds. | We add up the number of games played each round to find $ sum_{i=0}^{n - 1} 2^{i}$ total games played. | Thinking about how $16$ total players results in $8 + 4 + 2 + 1 = 15$ games played, we find that each game eliminates one player. So if we need to eliminate all but one player, we find the pattern that there are $2^{n} - 1$ games played. | | We can use binomial coefficients to consider which of the 7 games have which results (3 wins, 2 draws, 2 losses) to find $ binom{7}{3} binom{4}{2}$ games. We can also think of all possible outcomes given 3 wins, 2 draws, and 2 losses, and account for the overcounting of permutation of same game results to find $ frac{7!}{3!2!2!}$ games. | | We can use binomial coefficient to choose the team of 2 and a team of 5, where the order within teams does not matter, to find $ binom{12}{2} binom{10}{5}$ teams. The last team is determined by choosing the other two teams. The order of choice does not matter $(2, 5, 5) = (5, 5, 2)$. We can also think of all orderings and grouping the people into teams by their ordering, i.e. the first 3, next 5, and last 5. Accounting for overcounting because ordering within teams does not matter, we find $ frac{12!}{3!5!5!}$ teams. | Similarly, we can use the binomial coefficient to find $ binom{12}{4} binom{8}{4}$ teams, or consider all orderings and account for overcounting to find $ frac{12!}{4!4!4!}$ teams. | | We need to take 110 steps up and 111 steps right for a total of 221 moves. We can use the binomial coefficient to choose the up moves, which will determine the right moves, to find $ binom{221}{110} = binom{221}{111}$ paths. We can also consider all possible paths and account for overcounting because same moves can be permuted in any order to find $ frac{221!}{110! 111!}$ moves. | We need to make 210 up moves and 211 right moves, with fixing the path through (110, 111). We can consider the first 221 moves to reach (110, 111), then count the remaining 110 up moves and 100 right moves to reach (210, 211) to find $ binom{221}{110} times binom{200}{100}$ paths. We can also think of all possible separate paths and overcounting to find $ frac{221!}{110! 111!} times frac{200!}{100! 100!}$ paths. | | We can consider the complement of total possible choices minus the course choices that have no statistics courses to find $ binom{20}{7} - binom{15}{7} = 71085$ possible valid course choices. We can also consider each case of a student choosing 1 to 5 statistics courses to find $ binom{5}{1} binom{15}{6} + binom{5}{2} binom{15}{5} + binom{5}{3} binom{15}{4} + binom{5}{4} binom{15}{3} + binom{5}{5} binom{15}{2} = 71085$ possible valid course choices. | The answer is not $ binom{5}{1} binom{19}{6} = 135660$ course choices because this method overcounts the course choices with the same statistics courses. For example, the two course choices $(STAT1, STAT2, A, B, C, D) text{ and } (STAT2, STAT1, A, B, C, D)$ are equivalent, which is an example of overcounted course choices. | | In this case, there can be many elements of A that map to an element of B. We can think of this as the number of ways to pair an element from A to every element in B to find $n times m$ functions. | A one-to-one function is a function in which for each element in B, there can be at most one element in A that maps to it. We can think of this as the number of ways to | | Story Proofs . Imagine you have $n$ items and want to find all subsets. You start with the empty set, and for each new item, you create a new subset by appending it to an existing subset. The number of subsets doubles each time because adding a new item creates a new subset from an existing subset, leading to $2^{n}$ subsets. &lt;!– 1. For all positive integers $n,k$ with $n geq k$: (nk)+(nk−1)=(n+1k)n!(n−k)!k!+n!(n−k+1)!(k−1)!=(n+1)!(n+1−k)!k!n(n−1)…(n−k+1)k!+n(n−1)…(n−k+2)(k−1)! begin{aligned} binom{n}{k} + binom{n}{k-1} &amp;= binom{n+1}{k} frac{n!}{(n-k)! k!} + frac{n!}{(n-k+1)! (k-1)!} &amp;= frac{(n+1)!}{(n + 1 - k)! k!} frac{n (n-1) dots (n-k+1)}{k!} + frac{n (n-1) dots (n-k+2)}{(k-1)!} end{aligned}(kn​)+(k−1n​)(n−k)!k!n!​+(n−k+1)!(k−1)!n!​k!n(n−1)…(n−k+1)​+(k−1)!n(n−1)…(n−k+2)​​=(kn+1​)=(n+1−k)!k!(n+1)!​​ –&gt; | | References .",
            "url": "https://nhtsai.github.io/notes/probability-exercises",
            "relUrl": "/probability-exercises",
            "date": " • Nov 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Probability",
            "content": "Introduction to Probability by Blitzstein &amp; Hwang . Online Book | . 1. Probability and Counting . Why study probability? . Probability is the logic of uncertainty. | Applications of probability: statistics, physics, biology, computer science, meteorology, gambling, finance, political science, medicine, life. | . Sample spaces and Pebble World . An experiment results in a set of outcomes, and the outcome is unknown before the experiment is run. | Sample Space: set of all possible outcomes of the experiment The sample space $S$ can be finite, countably infinite, or un-countably infinite | . | Event: subset of the sample space Event $A$ occurred if the actual outcome of the experiment is in $A$ | . | If $S$ is finite, we can visualize it as pebble world, in which each pebble is an outcome and each event is a set of pebbles Performing an experiment is randomly selecting one pebble | If all the pebbles are of the same mass, they are equally likely to be chosen, a special case | . | Example: let $S$ be the sample space of an experiment and let $A,B subseteq S$ be events Union $A cup B$ : the event that occurs if and only if at least one of $A, B$ occurs, “OR” | Intersection $A cap B$ : the event that occurs if and only if both $A$ and $B$ occur, “AND” | Complement $A^C$ : the event that occurs if and only if $A$ does not occur, “NOT” | . | De Morgan’s Laws $(A cup B)^C = A^C cap B^C$ : it is not the case that at least one of $A, B$ occur is the same as $A$ does not occur and $B$ does not occur | $(A cap B)^C = A^C cup B^C$ : it is not the case that both $A,B$ occur is the same as at least one $A$ or $B$ does not occur | . | Example: A coin is flipped 10 times. A possible outcome is $HHHTHHTTHT$. The sample space is all possible strings of length 10 of $H$ and $T$. Let $H=1, T=0$, so the sample space is the set of all sequences in $(s_{1}, dots, s_{10}), s_{j} in {0, 1}$ Let $A_{1}$ be the event that the first flip is Heads. $A_{1} = {(1, s_{2}, dots, s_{10}) : s_{j} in {0, 1} text{ for } 2 leq j leq 10}$ This event occurs when the first flip is Heads. | | . Let $B$ be the event that at least one flip was Heads. $B = bigcup_{j=1}^{10} A_{j}$ Where $A_{j}$ is the event that the j-th flip is Heads for $j = 2, 3, dots, 10$ This event is the combination of events where any of the 10 coins being Heads. . | Let $C$ be the event that all flips were Heads. $C = bigcap_{j=1}^{10} A_{j}$ . | Let $D$ be the event there were at least two consecutive Heads. $D bigcup_{j=1}^{9} (A_{j} cap A_{j+1})$ . | | Example: Picking a card from a standard deck of 52 cards. Let $A$ be the event the card is an ace. | Let $B$ be the event the card has a black suit. | Let $D$ be the event the card is a diamond. | Let $H$ be the event the card is a heart. $A cap H$ is the event the card is ace of hearts. | $A cap B$ is the event the card is a black (spades, clubs) ace. | $A cup D cup H$ is the event the card is red (hearts, diamonds) or an ace. | $(A cup B)^C = A^C cap B^C$ is the event the card is not either an ace or black, or not ace and not black, or a red non-ace. | $(D cup H)^C = D^C cap H^C = B$ is the event the card is not either a diamond or heart, or not diamond and not heart, or black | If the card was a joker, then we had the wrong sample space since we assume the outcome of the experiment is guaranteed to an element of the sample space of the experiment. | . | | . Notation . English Sets . Events and occurrences |   | . sample space | $S$ | . $s$ is a possible outcome | $s in S$ | . $A$ is an event | $A subseteq S $ | . $A$ occurred | $s_{ text{actual}} in A$ | . something must happen | $s_{ text{actual}} in S$ | . New events from old events |   | . $A$ or $B$ (inclusive) | $A cup B$ | . $A$ and $B$ | $A cap B$ | . not $A$ | $A^{C}$ | . $A$ or $B$ (exclusive) | $(A cap B^{C}) cup (A^{C} cap B)$ | . at least one of $A_{1}, dots , A_{n}$ | $A_{1} cup dots cup A_{n}$ | . all of $A_{1}, dots , A_{n}$ | $A_{1} cap dots cap A_{n}$ | . Relationships between events |   | . $A$ implies $B$ | $A subseteq B$ | . $A$ and $B$ are mutually exclusive | $A cap B = emptyset$ | . $A_{1}, dots , A_{n}$ are a partition of $S$ | $A_{1} cup dots cup A_{n} = S, A_{i} cap A_{j} = emptyset text{ for } i neq j$ | . Naive definition of probability . The naive probability of an event is the count the number of ways the event could happen divided by total number of possible outcomes for the experiment Restrictive and relies on strong assumptions | . | Let $A$ be an event for an experiment with finite sample space $S$. The naive probability of event $A$ is: | . $P_{ text{naive}}(A) = frac{ lvert A rvert}{ lvert S rvert} = frac{ text{number of outcomes favorable to } A}{ text{total number of outcomes in } S}$ . In terms of Pebble World, probability of $A$ is the fraction of all pebbles that are in $A$. | Similarly, the probability of the complement of event $A$ is made of the remaining events: | . $P_{ text{naive}}(A^{C}) = frac{ lvert A^{C} rvert}{ lvert S rvert} = frac{ lvert S rvert - lvert A rvert}{ lvert S rvert} = 1 - frac{ lvert A rvert}{ lvert S rvert} = 1 - P_{ text{naive}(A)}$ . The naive definition requires $S$ to be finite with equally likely outcomes. | Can be applied in certain situations: There is symmetry in the problem that makes outcomes equally likely. E.g. fair coin or deck of cards. | The outcomes are equally likely by design. E.g. conducting a survey of $n$ people in a population of $N$ people using a simple random sample to select people. | The naive definition serves as a useful null model. We assume the naive definition just to see what predictions it would yield, and then compare observed data with predicted values to assess whether outcomes are equally likely. | . | . How to count . Calculating probability often involves counting outcomes in event $A$ and outcomes in sample space $S$, but sets are often extremely large to count, so counting methods are used. . | Multiplication Rule: If an Experiment A has $a$ possible outcomes, and for each of those outcomes Experiment B has $b$ possible outcomes, then the compound experiment as $ab$ possible outcomes. Each of the $a$ outcomes can lead to $b$ outcomes, so there are $b_{1} + dots + b_{a} = ab$ possibilities. | There’s no requirement Experiment A has to be performed before Experiment B, so no chronological order. | . | Example: Suppose 10 runners in a race, no ties and all 10 will complete the race. How many possibilities are there for first, second, and third place winners? Any of the 10 runners can be in the first place, 9 remaining runners can be in second place, and 8 remaining runners can be in third place. | There are $10 times 9 times 8 = 720$ possibilities. | Could have also considered 10 runners in third place first, order of consideration not important. | . | Example: How many squares are there in an $8 times 8$ chessboard? To specify a square, can consider its position within 8 rows and 8 columns. | There are $8 times 8 = 64$ squares on the board. | . | Example: How many possible ice cream cone combinations if there are 2 types of cones and 3 types of flavors? There are $2 times 3 = 3 times 2 = 6$ possibilities. | Order of choice doesn’t matter, can either choose cone then flavor, or flavor then cone. | Which flavors each cone could have doesn’t matter, only the number of flavor choices for each cone matters. If the waffle cones could only have 2 flavors, multiplication rule does not apply and there are $3 + 2 = 5$ possibilities. | If you bought 2 ice creams in a single day $(x, y)$, there would be $6 times 6 = 36$ possible combinations. | If you didn’t want to distinguish between combinations $(x,y) = (y,x)$ , then there are 15 $(x, y), x neq y$ possibilities and 6 $(x,x)$ possibilities, for a total of 21 possibilities. Cannot just divide $36 / 2 = 18$ ! Since $(x,x)$ pairs are already only listed once each, must do $((6 times 5) / 2) + 6 = 21$ possibilities. | . | If the original 36 cone-flavor pairs are equally likely, then the 21 possibilities are not equally likely. Counting $(x,y) = (y, x)$ doubles likeliness, but $(x,x)$ pairs stay the same. | . | . | Example: A set of $n$ elements has $2^{n}$ subsets, including the empty set $ emptyset$ and the set itself. From the multiplication rule, we can choose to include or exclude each element of the set. | Multiplication rule: think of each number consideration as an experiment with 2 outcomes: include or exclude. | ${1, 2, 3}$ has 8 subsets: $ emptyset, {1}, {2}, {3}, {1,2}, {1,3}, {2,3}, {1,2,3}$ Consider first element 1, then consider second element 2, and then consider last element 3. | $ emptyset$ is (exclude, exclude, exclude). | ${3}$ is (exclude, exclude, include). | Order of consideration does not matter; could have considered element 3 first. | . | . | Sampling with Replacement: Consider $n$ objects and making $k$ choices from them, one at a time with replacement, then there are $n^{k}$ possible outcomes, where order matters $(x,y) neq (y,x)$. | Example: Jar with $n$ balls, labeled from 1 to $n$. Choosing a ball is an experiment with $n$ outcomes, and there are $k$ experiments. | Therefore, multiplication rule says there are $n_{1} times n_{k} = n^{k}$ possibilities. | . | Sampling without Replacement: Consider $n$ objects and making $k$ choices from them, one at a time without replacement, then there are $n(n-1) dots (n-k+1)$ possible outcomes, where order matters, for $1 leq k leq n$. There are 0 possibilities, where order matters, for $k &gt; n$. You cannot choose more than the amount available. | There are $n(n-1) dots (n-k+1) = n$ possible outcomes for $k=1$. | Multiplication rule: each sampled object is an experiment and the the number of outcomes decreases by 1 each time. | . | Example: A permutation of $1, 2, dots, n$ is an arrangement of the $n$ elements in some order. Sampling without replacement: if $k=n$, there are $n!$ permutations of $n$ elements, or ways to choose $n$ items without replacement | . | We can use sampling with and without replacement as probabilities when the naive definition of probability applies. | Example: Birthday Problem There are $k$ people in a room. Assume each person’s birthday is equally likely to be any of the 365 days of the year, and people’s birthdays are independent, e.g. no twins. What is the probability that at least one pair of people in the group have the same birthday? | If we sample with replacement $k$ days from $n = 365$ total days, what is the probability at least two samples are the same? There are $365^{k}$ possible outcomes. | But how can we count all the ways that two more more people have the same birthday? | . | Instead, think about the complement: the number of ways to sample without replacement $k$ different days of $n = 365$ total days. What is the probability that no two people share the same birthday? $P( text{no match}) = frac{365 times 364 times dots times (365 - k + 1)}{365^{k}}$ | . | Therefore, the probability of at least one birthday match is the complement. $P( geq 1 text{ match}) = 1 - frac{365 times 364 times dots times (365 - k + 1)}{365^{k}}$ | . | At $k = 23$, there is over a 50% chance that two people share the same birthday, and over a 99% chance at $k = 57$. | Intuition: For $k = 23$ people, there are $ binom{23}{2} = 253$ pairs of people, any of which could be a birthday match. | . | Think of the objects or people in the population as named or labeled, rather than indistinguishable. | Example: If we roll two fair dice, is a sum of 11 or sum of 12 more likely? Label dice A and B and consider each die to be a sub-experiment. Multiplication rule says there are $6 times 6 = 36$ possibilities, in the form $(a,b)$. | 11 can be made from $(5,6), (6,5)$, while 12 can only be made from $(6,6)$. Therefore, a sum of 11, which has a probability of $ frac{2}{38} = frac{1}{18}$, is twice as likely as a sum of 12, which has a probability $ frac{1}{36}$. | . | Overcounting: It is difficult to directly count each possible outcome once, and we may need to adjust for overcounting by dividing the count by some factor $c$. | Example: For a group of four people, how many ways are there to Choose a two-person group? Labeling people as $1, 2, 3, 4$, we can see possible groups are ${12, 13, 14, 23, 24, 34}$, so there are 6 possible ways. | Multiplication rule and adjust for overcounting: There are 4 ways to choose the first person, and 3 ways to choose the second person, or $4 times 3 = 12$. But since $(a,b) = (b,a)$, we have overcounted by 2, so there are $(4 cdot 3) / 2 = 6$ possible ways. | . | Break people into two teams of two? Labeling people as $1, 2, 3, 4$, we can see possible groups are $(12, 34), (13, 24), (14, 23)$, so there are 3 possible ways. | We can also note that choosing a team of 2 (choosing person 1’s partner) determines other team to be formed from remaining people. There are 3 other people to pair to person 1, so there are 3 possible ways. | We can also see that the 6 possible ways to choose a two-person group is overcounting by a factor of 2, since the other team is determined from choosing one team $(a,b) = (c,d)$, so there are $6 / 2 = 3$ possible ways. | . | . | Binomial Coefficient: For any nonnegative integers $k,n$, the binomial coefficient $ binom{n}{k}$ (“n choose k”) is the number of subsets of size $k$ for a set of size $n$. E.g. The number of ways to choose a group of size $k = 2$ from $n = 4$ people is $ binom{4}{2} = 6$. Sets are unordered by definition $(a,b) = (b,a)$, so a binomial coefficient counts the number of ways to choose $k$ objects out of $n$, without replacement and without distinguishing between different orders. | For $k leq n$, $ binom{n}{k} = frac{n (n-1) dots (n-k+1)}{k!} = frac{n!}{(n-k)!k!}$. | For $k &gt; n, binom{n}{k} = 0$. | For naive definition problems, binomial coefficient can be used to calculate probabilities. | . | Proof: Let $A$ be set with $ lvert A rvert = n$. Any subset of $A$ has size at most $n$, so $ binom{n}{k} = 0$ for $k &gt; n$, or no ways to create a larger subset than what’s available. If $k leq n$, then sampling without replacement tells us there are $n(n-1) dots (n-k+1)$ ways to make ordered choice of $k$ elements without replacement. But this overcounts each subset by $k!$ since order does not matter for binomial coefficient $(a,b) = (b,a)$, so we divide by $k!$ to get $ binom{n}{k} = frac{n (n-1) dots (n-k+1)}{k!} = frac{n!}{(n-k)!k!}$. | Example: In a club of $n$ people, how many ways are there to choose 3 officers? There are $ binom{n}{3} = frac{n(n-1)(n-2)}{3!}$ possible ways. | . | Example: How many ways are there to permute the letters in the word “LALALAAA”? We can just decide the positions of either the 3 L’s or the 5 A’s. So there are $ binom{8}{3} = binom{8}{5} = frac{8 cdot 7 cdot 6}{3!} = 56$ possible permutations. | . | Example: How many ways are there to permute the letters in the word “STATISTICS”? We can choose positions for 3 S’s, 3 T’s, 2 I’s, 1 A, and 1 C is determined. So there are $ binom{10}{3} binom{7}{3} binom{4}{2} binom{2}{1} = 50400$ permutations. | We can also start with $10!$ permutations and account for overcounting of a factor of $3!3!2!$ for the 3 S’s, 3 T’s, and 2 I’s because the repeated letters can be permuted among themselves in any way. So there are $ frac{10!}{3!3!2!} = 50400$ permutations. | . | Binomial Theorem: For any nonnegative integer $n$, $(x + y)^{n} = sum_{k=0}^{n} binom{n}{k} x^{k} y^{n-k}$ | Proof: We decompose the term $(x+y)^{n} = underbrace{(x + y) (x + y) dots (x + y)}_{n text{ factors}}$ The terms of $(x + y)^{n}$ are obtained by picking either the $x$ or the $y$ from each factor $(x + y)$. This is similar to $(a + b)(c + d) = ab + ac + bc + bd$, where each product term is created from only one term of each factor. | There are $ binom{n}{k}$ ways to choose exactly $k$ of the $x text{‘s}$ to make the term $x^{k} y^{n-k}$. | . | Example: A 5-card hand is dealt from a standard, well-shuffled 52-card deck. What is the probability of a full house (3 cards of some rank, 2 cards of another rank)? Since all of the $ binom{52}{5}$ total possible hands are equally likely by symmetry, the naive definition of probability applies. | Multiplication rule: Choosing a rank for the 3 cards is one experiment, and choosing the suits of the 3 cards is another experiment. Similarly, choosing a rank for the 2 cards is one experiment, and choosing the suits of the 2 cards is another experiment. | So the probability of a full house is $ frac{13 binom{4}{3} times 12 binom{4}{2} }{ binom{52}{5} } = frac{3744}{2598960} approx 0.00144$ | Note: We cannot use $ binom{13}{2}$ to choose the ranks because that approach treats the order of the ranks as interchangeable, which undercounts by a factor of $2$. E.g. $( underbrace{7 heartsuit, 7 diamondsuit, 7 clubsuit}{3 text{-card}}, underbrace{5 diamondsuit, 5 clubsuit}{2 text{-card}}) neq ( underbrace{5 heartsuit, 5 diamondsuit, 5 clubsuit}{3 text{-card}}, underbrace{7 diamondsuit, 7 clubsuit}{2 text{-card}})$ | . | . | Example: Newton-Pepys Problem Which of the following events has the highest probability? Event A: At least one 6 appears when 6 fair dice are rolled. | Event B: At least two 6’s appear when 12 fair dice are rolled. | Event C: At least three 6’s appear when 18 fair dice are rolled. | . | The three experiments have $6^{6}, 6^{12}, 6^{18}$ possible outcomes, respectively. | By symmetry, all outcomes are equally likely, so the naive definition applies in all three experiments. | Event A Rather than count all possible ways to roll at least one 6, we consider the complement of all possible ways to roll no 6’s. $P(A) = 1 - P(A^{C}), P( geq text{ one } 6) = 1 - P( text{no } 6)$ | Therefore, $P(A) = 1 - frac{5^{6}}{6^{6}} approx 0.67$ | . | Event B Similarly, we consider the complement of all possible ways to roll either zero or one 6’s in 12 dice rolls. $P(B) = 1 - P( text{zero } 6) - P( text{one } 6)$ | Therefore, $P(B) = 1 - frac{5^{12} + binom{12}{1} 5^{11}}{6^{12}} approx 0.62$ | . | Event C Similarly, we consider the complement of all possible ways to roll either zero, one, or two 6’s in 18 dice rolls. $P(C) = 1 - P( text{zero } 6) - P( text{one } 6) - P( text{two } 6)$ | Therefore, $P(C) = 1 - frac{5^{18} + binom{18}{1} 5^{17} + binom{18}{2} 5^{16}}{6^{18}} approx 0.62$ | . | The event with the highest probability is Event A. | . | Example: Bose-Einstein Problem How many ways are there to choose $k$ objects from a set of $n$ objects with replacement, if order does not matter? | When order does matter, we know from sampling with replacement there are $n^{k}$ times, but what about when order does not matter? | We consider an isomorphic (equivalent) problem: How many ways are there to put $k$ indistinguishable particles into $n$ distinguishable boxes? We use $ bullet$ to represent a particle and $ vert$ to represent a wall. | Consider putting $k=7$ particles in $n=4$ boxes, such as $ vert bullet vert bullet bullet vert bullet bullet bullet vert bullet vert$ | A sequence must start and end with a wall, and it must have exactly $n - 1$ walls and $k$ particles in between. There are $(n - 1) + k$ slots between 2 outer walls to place $n - 1$ inner walls and $k$ particles, so the number of possible placements is $ binom{n + k - 1}{k}$. | . | We can consider another isomorphic problem: How many solutions $(x_{1}, dots, x_{n})$ to the equation $x_{1} + x_{2} + dots + x_{n} = k$, where $x_{i}$ are nonnegative integers. We can think of $x_{i}$ as the number of particles in the $i text{th}$ box. | Note: Bose-Einstein result cannot be used in naive definition of probability in most cases. E.g. Survey by sampling $k$ people from population of size $n$ one at a time, with replacement and equal probabilities. The $n^{k}$ ordered samples are equally likely (naive definition applies), but the $ binom{n + k - 1}{k}$ unordered samples are not equally likely (naive definition does not apply). | E.g. How many unordered birthday lists are possible for $k$ people and $n=365$ days in a year? For $k=3$, we want to count lists, where $(a, b, c) = (c, b, a)$ and so on, but we cannot simply adjust for overcounting like $ frac{n^{k}}{3!}$ for the permutations because there are $3!$ permutations for $(a, b, c)$ but only $3$ permutations for $(a, a, c)$. The ordered birthday lists are equally likely (naive definition applies), but the unordered birthday lists are not equally likely (naive definition does not apply) as the number of permutations is not equal across all lists. | . | . | . Story Proofs . *Story Proof: a proof by interpretation for explaining why results of counting problems are true | Example: For any nonnegative integers $n, k$ with $k leq n$, $ binom{n}{k} = binom{n}{n-k}$. Story: Consider choosing a committee of size $k$ in a group of $n$ people, which has $ binom{n}{k}$ possibilities. | Another way is to choose the complement, or $n-k$ people not on the committee, which has $ binom{n}{n-k}$ possibilities. | The two methods are equal because they count the same thing. | . | Example: For any positive integers $n, k$ with $k leq n$, $n binom{n - 1}{k - 1} = k binom{n}{k}$. Story: Consider choosing a team of $k$ people, with one being the team captain, from $n$ total people. We could first choose the team captain and then choose the remaining $k-1$ team members for $n binom{n-1}{k-1}$ possibilities. | We could also first choose the team of $k$ members and then choose who is the team captain, for $ binom{n}{k} k$ possibilities. | . | Example: Vandermonde’s Identity states $ binom{m + n}{k} = sum_{j=0}^{k} binom{m}{j} binom{n}{k-j}$ Story: Consider an organization of $m$ juniors and $n$ seniors and choosing a committee of $k$ members, for $ binom{m + n}{k}$ possible committee formations. | This is equal to all of ways to form committees where there are $j$ juniors and the remaining $k-j$ members are seniors, for $ sum_{j=0}^{k} binom{m}{j} binom{n}{k-j}$ possibilities. | . | Example: Proving $ frac{(2n)!}{2^{n} cdot n!} = (2n - 1)(2n - 3) dots (3)(1)$ describes the number of ways to break $2n$ people into $n$ partnerships. Story: Take $2n$ people and label them with IDs $1 dots 2n$. We can form pairs by taking one of $(2n)!$ orderings and grouping adjacent people, but this overcounts by a factor of $n! cdot 2^{n}$. Order of pairs does not matter: $( underbrace{1, 2}{ text{pair 1}}, underbrace{3, 4}{ text{pair 2}}) = ( underbrace{3, 4}{ text{pair 1}}, underbrace{1, 2}{ text{pair 2}})$ | Order within pairs does not matter: $( underbrace{1, 2}{ text{pair 1}}, underbrace{3, 4}{ text{pair 2}}), ( underbrace{2, 1}{ text{pair 1}}, underbrace{4, 3}{ text{pair 2}})$ | . | We can also consider the number of possible groups by noting there are $(2n-1)$ possible partners for person 1, $(2n - 3)$ possible partners for person 2, and so on. | . | . Non-naive definition of probability . General Definition of Probability: A probability space consists of a sample space $S$ and a probability function $P$ which takes an event $A subseteq S$ as input and returns $P(A)$, a real number between $0$ and $1$, as output. The function $P$ must satisfy the following axioms: $P( emptyset) = 0, P(S) = 1$ Probability of a non-existent event is 0; probability of an event in S is 1 (certain). | . | If $A_{1}, A_{2}, dots $ are disjoint events, then $P( bigcup_{j=1}^{ infty} A_{j}) = sum_{j=1}^{ infty} P(A_{j})$ The probability of the union of disjoint events is equal to the sum of the individual probabilities. | Disjoint events are mutually exclusive such that $A_{i} cap A_{j} = emptyset$ for $i neq j$. | . | | In Pebble World, general probability is like mass; mass of empty pile is 0 and total mass of all pebble is 1. Given non-overlapping piles of pebbles, we can get combined mass by adding individual masses, and we can have a countably infinite number of pebbles as long as the total mass is 1. | Any function $P$ that maps events to the interval $[0, 1]$ that satisfies the two axioms is a valid probability function, but the axioms don’t state how the probability should be interpreted. Frequentist View: Probability represents a long-run frequency over a large number of repetitions of an experiment. E.g. If we say a coin has a probability of $1/2$ of Heads, the coin will land Heads 50% of the time if tossed over and over. | Bayesian View: Probability represents a degree of belief about the event in question, so probabilities can be assigned to hypotheses, such as “candidate A will win the election” or “the defendant is guilty”, even if it isn’t possible to repeat the same election or same crime over and over again. | Frequentist and Bayesian views are complementary. | . | Properties of Probability: For any events $A, B$: Complement: $P(A^{C}) = 1 - P(A)$ Proof: Since $A$ and $A^{C}$ are disjoint and their union makes up $S$, $P(S) = P(A cup A^{C}) = P(A) + P(A^{C})$. Since $P(S) = 1$ by the first axiom, $P(A) + P(A^{C}) = 1$. | . | Subset: If $A subseteq B$, then $P(A) leq P(B)$ Proof: Since $A$ and $B cap A^{C}$ are disjoint, $P(B) = P(A cup (B cap A^{C})) = P(A) + P(B cap A^{C})$ by the second axiom. Probability is nonnegative, so $P(B cap A^{C}) geq 0$, proving $P(B) geq P(A)$. | The probability of Event B is all outcomes in Event A and all outcomes in Event B but not in Event A. Since these groups are disjoint, we can add the individual probabilities together. Since we know group in Event B but not in Event A has a positive probability, we know Event A must be smaller than group B. | . | Inclusion-Exclusion: $P(A cup B) = P(A) + P(B) - P(A cap B)$ Proof: The probability of a union of two events is the sum of separate probabilities minus any overlap. $P(A cup B) = P(A) + P(B cap A^{C}) = P(A) + P(B cap A^C)$ by the second axiom, since $A$ and $B cap A^{C}$ are disjoint events. Since $A cap B$ and $B cap A^{C}$ are disjoint and $P(A cap B) + P(B cap A^{C}) = P(B)$ by the second axiom, then $P(B cap A^{C}) = P(B) - P(A cap B)$. Plugging this in, we get $P(A cup B) = P(A) + P(B) - P(A cap B)$. | . | | Inclusion-Exclusion: For any events $A_{1}, dots, A_{n}$, $P( bigcup_{i=1}^{n} A_{i}) = sum_{i} P(A_{i}) - sum_{i &lt; j} P(A_{i} cap A_{j}) + sum_{i &lt; j &lt; k} P(A_{i} cap A_{j} cap A_{k}) - dots + (-1)^{n + 1} P(A_{1} cap dots cap A_{n})$ The probability of the union of events is equal to the sum of the individual probabilities minus any overlapping and add back any non-overlapping that was overly removed. | . | Example: de Montmort’s Matching Problem Consider a well-shuffled deck of $n$ cards, labeled $1 dots n$. You flip over cards one by one while saying numbers $1 dots n$. If the number you say is the same number as the card, you win. What is the probability of winning? | Let $A_{i}$ be the event that the $i text{th}$ card in the deck has the number $i$ written on it. We want to find the probability of the union $P(A_{1} cup dots cup A_{n})$, which is the win condition. A ordering with no matching card numbers to positions is called a derangement. | We know $P(A_{i}) = frac{1}{n}$. Naive Probability: The probability of a card number matching its position is $P(A_{i}) = frac{(n-1)!}{n!} = frac{1}{n}$, or the number of orderings where the card matches its position $(n-1)!$ divided by all possible orderings $n!$. | Symmetry: Alternatively, the card with number $i$ is equally likely to be in any of the $n$ positions in the deck, so the probability of a card number matching its position is $P(A_{i}) = frac{1}{n}$. | . | Similarly, $P(A_{i} cap A_{j}) = frac{(n - 2)!}{n!} = frac{1}{n(n - 1)}$. Since we fix the cards with numbers $i,j$ to be in the $i text{th}, j text{th}$ spots in deck, there are $(n-2)!$ permutations out of $n!$ total permutations. | . | Similarly, $P(A_{i} cap A_{j} cap A_{k}) = frac{1}{n(n - 1)(n - 2)}$, and so on. | Inclusion-Exclusion: There are $ binom{n}{i}$ terms involving $i$ events, representing the number of ways to intersect $i$ events, i.e. $n$ terms for 1-event, $ binom{n}{2}$ terms for 2-events, etc. By symmetry, all $ binom{n}{i}$ terms are equal, e.g. $P(A_{i}) = 1/n text{, for all }i$. | Therefore, $P( bigcup_{i=1}^{n} A_{i}) = frac{n}{n} - frac{ binom{n}{2}}{n(n - 1)} + frac{ binom{n}{3}}{n(n - 1)(n - 2)} - dots + (-1)^{n+1} cdot frac{1}{n!}$ | Simplify, $P( bigcup_{i=1}^{n} A_{i}) = 1 - frac{1}{2!} + frac{1}{3!} - dots + (-1)^{n+1} cdot frac{1}{n!}$ | . | Similar to the Taylor series for $e^{-1} = 1 - frac{1}{1!} + frac{1}{2!} - frac{1}{3!} + dots$, we see that the probability of winning approaches $1 - 1/e approx 0.63$ as $n$ gets larger. | Rather than approaching 0 or 1, the probability of winning is determined by the competing forces of increasing possible locations for matching $(n)$ and the decreasing probability of any particular match $(1/n)$. | . | Inclusion-Exclusion is a general formula for probability of a union of events that is most helpful when there is symmetry among events (which helps us add up all terms and simplify easily). In general, consider inclusion-exclusion as a last resort if no symmetry. | . 2. Conditional Probability . References .",
            "url": "https://nhtsai.github.io/notes/probability-book",
            "relUrl": "/probability-book",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Semi-Supervised Classification with Graph Convolutional Networks",
            "content": "Semi-Supervised Classification with Graph Convolutional Networks . Abstract . TODO . Introduction . The authors first consider a problem: How do we classify nodes in a graph, where labels are only available for a small subset of nodes? Think of a citation network where some documents have labels, but most do not. The authors frame it as “graph-based semi-supervised learning, where label information is smoothed over the graph via some form explicit graph-based regularization term in the loss function.” . graph-based: working with a network | semi-supervised: only some nodes have labels | smoothed over the graph: labels are approximated across the graph | explicit graph-based regularization: | . For example, using graph Laplacian regularization: . $ mathcal{L} = mathcal{L}0 + lambda mathcal{L}{ text{reg}}$ . where $ mathcal{L}{ text{reg}} = sum{i,j} A_{i,j} lVert f(X_{i}) - f(X_{j}) rVert ^{2} = f(X)^{T} Delta f(X)$ . $ mathcal{L}_0$ : supervised loss with respect to labeled part of graph | $ mathcal{L}_{ text{reg}}$ : graph Laplacian regularization term | $f( dots)$ : a neural network-like differentiable function | $ lambda$ : regularization weight | $X$ : matrix of node feature vectors $X_{i}$ | $ Delta = D - A$ : un-normalized graph Laplacian of an undirected graph $G=(V, E)$ , with $N$ nodes $v_{i} in V$ , edges $(v_{i}, v_{j}) in E$ $A in Reals^{N times N}$ : adjacency matrix (binary or weighted) 1 if connected, 0 otherwise. | Symmetric since graph is undirected, i.e. both sides of diagonal are same | . | $D_{ii} = sum_{j} A_{ij}$ : degree matrix The diagonal is the number of edges for that nodes | . | Basically, positive diagonal of each node’s number of edges and -1’s for each neighbor | . | Assumes that connected nodes in the graph share the same label This may restrict modeling capacity because edges could contain more information than node similarity | . | . The authors want to encode the graph structure in a neural network model $f(X, A)$ . For all nodes with labels, train on a supervised target $ mathcal{L}{0}$ and avoid graph regularization $ mathcal{L}{ text{reg}}$ in the loss function. . By conditioning $f( dots)$ on adjacency matrix $A$ , the model can distribute gradient information from supervised loss $ mathcal{L}_0$ to learn representations of nodes both with and without labels. Because the model knows each node’s neighbors, the labeled nodes can help inform the unlabeled nodes. . Fast Approximate Convolutions on Graphs . The overall goal is to learn a function of graph features that uses: . node features, like a $N times D$ feature matrix $X$ , with $N$ nodes and $D$ input features | graph features, like an adjacency matrix $A$ and produces a node-level $N times F$ output matrix $Z$ , with $N$ nodes and $F$ output features | . Every neural network layer $H$ can then be written as a non-linear function: . $H^{(l+1)} = f(H^{(l)}, A)$ . There are $L$ layers in the neural network | $H^{(0)} = X$ : input feature matrix is the first layer | $H^{(L)} = Z$ : output feature matrix is the last layer | . In this case, the layer-wise propagation rule is: . $H^{(l+1)} = sigma ( tilde{D}^{- frac{1}{2}} tilde{A} tilde{D}^{- frac{1}{2}} H^{(l)} W^{(l)})$ . $ tilde{A} = A + I_{N}$ : adjacency matrix of undirected graph G plus self connections | $I_{N}$ : identity matrix | $ tilde{D}{ii} = sum{j} tilde{A}_{ij}$ : | $W^{(l)}$ : trainable weight matrix of layer $l$ | $ sigma( dots)$ : activation function, such as $ text{ReLU}( dots) = text{max}(0, dots)$ | $H^{(l)} in Reals^{N times D}$ : activation matrix of layer $l$ | This propagation rule comes from first-order approximation of local spectral filters on graphs | . Spectral Graph Convolutions . A spectral graph convolution is the signal $x in Reals^{N}$ (scalar for every node) multiplied by a filter $g_{ theta} = text{diag}( theta)$ parameterized by $ theta in Reals^{N}$ in the Fourier domain. . $g_{ theta} star x = U g_{ theta} U^{T} x$ . $U$ : matrix of eigenvectors of normalized graph Laplacian $L_{ text{norm}}$ | $L_{ text{norm}} = I_{N} - D^{- frac{1}{2}} A D^{- frac{1}{2}} = U Lambda U^{T}$ : symmetric normalized graph Laplacian | $ Lambda$ : diagonal matrix of $L_{ text{norm}}$ eigenvalues | $U^T x$ : graph Fourier transform of x | $g_{ theta}$ : function of eigenvalues of $L_{ text{norm}}$ , i.e. $g_{ theta}( Lambda)$ | . Calculating graph spectral convolutions is expensive . multiplication with eigenvector matrix $U$ is $O(N^{2})$ | eigen-decomposition of $L_{ text{norm}}$ is expensive for large graphs | . We can approximate $g_{ theta}( Lambda)$ using a truncated expansion in terms of Chebyshev polynomials $T_{k}(x)$ up to $K^{ text{th}}$ order: . $g_{ theta’}( Lambda) approx sum_{k=0}^{K} theta’{k} T{k}( tilde{ Lambda})$ . $ tilde{ Lambda} = frac{2}{ lambda_{ text{max}}} Lambda - I_{N}$ : rescaled diagonal matrix of eigenvalues | $ lambda_{ text{max}}$ : largest eigenvalue of $L_{ text{norm}}$ | $ theta’ in Reals^{K}$ : vector of Chebyshev coefficients | Chebyshev polynomials are recursively defined as $T_{k}(x) = 2x T_{k-1}(x) - T_{k-2}(x)$ , with $T_{0}(x) = 1$ and $T_{1}(x) = x$ | . Now, we have an approximated spectral graph convolution of a signal $x$ with a filter $g_{ theta’}$ : . $g_{ theta’} star x approx sum_{k=0}^{K} theta’{k} T{k}( tilde{L}_{ text{norm}})x$ . $ tilde{L}{ text{norm}} = frac{2}{ lambda{ text{max}}} L_{ text{norm}} - I_{N}$ : rescaled normalized graph Laplacian, verified by $(U Lambda U^{T})^{k} = U Lambda^{k} U^{T}$ | This expression is $K$-localized as a $K^{ text{th}}$-order polynomial in the Laplacian It depends only on the $K^{ text{th}}$-order neighborhood, or nodes that are at maximum $K$ steps away from the central node | . | Complexity of calculating this is $ mathcal{O}( lvert mathcal{E} rvert)$ , linear in the number of edges | . Layer-Wise Linear Model . We can stack multiple approximated spectral graph convolutions to build a neural network, with each layer followed by a point-wise non-linearity. What if we limited the neighborhood to nodes at most 1 step away $(K=1)$ ? This makes the approximated convolution a function that is linear with respect to $L_{ text{norm}}$ and therefore a linear function on the graph Laplacian spectrum . Layer-Wise linear formulation: We can stack multiple of these layers without being limited to explicit parameterization of Chebyshev polynomials. Such model can alleviate problem of over-fitting on local neighborhood structures for graphs with very wide node degree distributions, i.e. some nodes are highly connected and some nodes are barely connected. This is common in social networks, citation networks, knowledge graphs, etc. This layer-wise linear formulation can build deeper models using a fixed computational budget, which is known to improve modeling capacity on a number of domains. . We further approximate $ lambda_{ text{max}} approx 2$ because the neural network parameters will adapt to during training: . $g_{ theta’} star x approx theta’{0} + theta’{1} (L_{ text{norm}} - I_{N}) x = theta’{0} x - theta’{1} D^{- frac{1}{2}} A D^{- frac{1}{2}} x$ . $ theta’{0}$ and $ theta’{1}$ : filter parameters | . Applying the filters successively effectively convolves the $k^{ text{th}}$-order neighborhood of a node, where $k$ is the number of successive filtering operations or convolutional layers in the neural network model. . Each layer takes into consideration 1-order neighborhood, each each successive layer considers more and more neighbors. . To address over-fitting and minimize operations per layer, we constrain the number of parameters: . $g_{ theta} star x approx theta (I_{N} + D^{- frac{1}{2}} A D^{- frac{1}{2}}) x$ . $ theta = theta’{0} = - theta’{1}$ : single parameter | $I_{N} + D^{- frac{1}{2}} A D^{- frac{1}{2}}$ has eigenvalues in range [0, 2] | . Repeating this operation can lead to numerical instabilities and exploding/vanishing gradients in deep neural network models. We ca address this using a renormalization trick. . $I_{N} + D^{- frac{1}{2}} A D^{- frac{1}{2}} rightarrow tilde{D}^{- frac{1}{2}} tilde{A} tilde{D}^{- frac{1}{2}}$ . $ tilde{A} = A + I_{N}$ : adjusted adjacency matrix | $ tilde{D}{ii} = sum{j} tilde{A}_{ij}$ : degree matrix | . We can generalize this definition to a signal $X in Reals^{N times C}$ with $C$ input channel (features for every node) and $F$ filters for feature maps: . $Z = tilde{D}^{- frac{1}{2}} tilde{A} tilde{D}^{- frac{1}{2}} X Theta$ . $ Theta in Reals^{C times F}$ : matrix of filter parameters | $Z in Reals^{N times F}$ : convolved signal matrix | Filtering operation has complexity $ mathcal{O}( lvert mathcal{E} rvert F C)$ | $ tilde{A}X$ can be efficiently implemented as product of sparse matrix and dense matrix | . Semi-Supervised Node Classification . Now we have a flexible model $f(X,A)$ for efficient information propagation on graphs. We can relax certain assumptions typically made in graph-based semi-supervised learning by condition our model $f(X,A)$ both on the data $X$ and adjacency matrix $A$ of underlying graph structure. . When the adjacency matrix $A$ contains information not present in the data $X$ , using both can be especially powerful. . E.g. Citation links between documents in citation network | E.g. Relations in a knowledge graph | . Example: Cora Dataset . . Consider a two-layer GCN for semi-supervised node classification on a graph with symmetric adjacency matrix $A$ (binary or weighted). . Preprocessing step: . $ hat{A} = tilde{D}^{- frac{1}{2}} tilde{A} tilde{D}^{- frac{1}{2}}$ . Forward propagation step: . $Z = f(X,A) = text{softmax}( hat{A} text{ ReLU}( hat{A} X W^{(0)}) W^{(1)})$ . $ hat{A}$ : adjacency matrix with symmetric normalization trick | $W^{(0)} in Reals^{C times H}$ : input-to-hidden weight matrix for hidden layer with $H$ feature maps | $W^{(1)} in Reals^{H times F}$ : hidden-to-output weight matrix | $ text{softmax}(x_{i}) = frac{1}{z} exp{x_{i}}$ : softmax activation function $z = sum_{i} exp{x_{i}}$ : applied row-wise | . | . Evaluate cross-entropy error over all labeled examples: . $ mathcal{L} = - sum_{l in mathcal{Y}{L}} sum{f=1}^{F} Y_{lf} ln{Z_{lf}}$ . $ mathcal{Y}_{L}$ : set of node indices that have labels | $F$ : number of output labels | $Y_{lf}$ : correct value for node index $l$ and output label $f$ Should be 0 for incorrect output labels, 1 for correct output label | . | $Z_{lf}$ : predicted value for node index $l$ and output label $f$ Should be higher for the correct output label, lower for incorrect output labels | . | . Neural network weights $W^{(0)}$ and $W^{(1)}$ are trained using batch gradient descent using full dataset for every training iteration. . Using sparse representation for $A$ , memory requirement is $ mathcal{O}( lvert mathcal{E} rvert)$ , linear in the number of edges. Use dropout to introduce stochasticity in the training process. Future work: memory-efficient extensions with mini-batch stochastic gradient descent . Implementation . Use efficient sparse-dense matrix multiplication to make the computation complexity of the forward propagation step $ mathcal{O}( lvert mathcal{E} rvert C H F)$ , linear in the number of graph edges. . Related Work . Graph-Based Semi-Supervised Learning . Neural Networks on Graphs . Experiments . Datasets . Citation Networks . NELL . Random graphs . Experimental Set-Up . Baselines . Results . Semi-Supervised Node Classification . Evaluation of Propagation Model . Training Time per Epoch . Discussion . Semi-Supervised Model . Limitations and Future Work . Memory requirement . Directed edges and edge features . Limiting Assumptions . Conclusion . References . http://tkipf.github.io/graph-convolutional-networks/ | .",
            "url": "https://nhtsai.github.io/notes/gcn",
            "relUrl": "/gcn",
            "date": " • Oct 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "JSON",
            "content": "JSON . Video | W3 Tutorial | . JSON (JavaScript Object Notation) . * lightweight data representation used to send data between computers * Commonly used for APIS and configs * Lightweight and Easy to Read/Write * Integrates with Javascript (JSON is a superset of Javascript) and most languages . JSON Types . Strings: “Hello World”, “I”, double quotes only | Numbers: integers, floating point numbers, scientific notation, 10, 1.5, -30, 1.2e10 | Booleans: true, false | null: null | Arrays: [1, 2, 3], [“Hello”, “World”], access index using brackets | Objects: {key: value}, keys must be strings, value can be any JSON type, get values using {}.key, {“Age”: 30} | Cannot use a function, a date, undefined | . Syntax . Derived from JS object notation, but JSON format is text only | Originally specified by Douglas Crockford | Usually an array or object as top-level of JSON file | . { &quot;name&quot;: &quot;Nathan&quot;, &quot;number&quot;: 3, &quot;isProgrammer&quot;: true, &quot;hobbies&quot;: [&quot;Weight Lifting&quot;, &quot;Community Service&quot;], &quot;friends&quot;: [ { &quot;name&quot;: &quot;Matthew&quot;, &quot;number&quot;: 22, &quot;isProgrammer&quot;: true, &quot;hobbies&quot;: [&quot;Biking&quot;, &quot;Relaxing&quot;], &quot;friends&quot;: null } ] } . JSON vs XML . Similarities “self-describing”, human-readable | hierarchical | parsed by many programming languages | fetched using XMLHttpRequest | . | Differences JSON doesn’t use closing tags | JSON is more succinct | JSON is quicker to read and write | JSON can use arrays | JSON can be parsed by standard JS function, while XML has to be parsed with XML parser | . | Advantages of JSON Faster and easier than XML | Parsed into ready to use JS object using JSON.parse | . | . { &quot;employees&quot;: [ { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Doe&quot; }, { &quot;firstName&quot;: &quot;Anna&quot;, &quot;lastName&quot;: &quot;Smith&quot; }, { &quot;firstName&quot;: &quot;Peter&quot;, &quot;lastName&quot;: &quot;Jones&quot; } ] } . &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; .",
            "url": "https://nhtsai.github.io/notes/json",
            "relUrl": "/json",
            "date": " • Sep 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to Probability",
            "content": "Harvard STAT 110: Introduction to Probability, Fall 2011 . Course Resources . Professor Joe Blitzstein | Course Lectures | Course Website | Course Syllabus | Course EdX | Course Textbook: Introduction to Probability, 2 ed. by Blitzstein &amp; Hwang | Course focuses on using probability as a language and set of tools for understanding statistics, science, risk, and randomness. | Topics: sample spaces/events, conditional probability, Bayes’ Theorem, univariate distributions, multivariate distributions, limit laws, Markov chains | . 1. Probability, Counting . Probability has its roots in gambling and is an important part of many disciplines today . | Sample Space: set of all possible outcomes of an experiment . Experiment: anything with certain possible outcomes that are unknown beforehand | Event: a subset of the sample space | Example: The experiment of rolling 2 dice has 36 total outcomes in the sample space, and an event could be the subset of all outcomes that sum up to a number | . | . . Naive Definition of Probability $ P(A)= frac{ text{favorable outcomes}}{ text{total possible outcomes}} $ The number of outcomes where Event A occurs over all possible outcomes | . | Example: Flipping a fair coin twice, what is probability that both flips are heads? 1 favorable outcome: $(HH)$ | 4 total outcomes: $(HH, HT, TH, TT)$ | Therefore, $P( text{both heads}) = frac{1}{4}$ | . | Assumes that all outcomes are equally likely and a finite sample space If the coin was not fair, the outcomes would not be equally likely | If the sample space is infinite, the probability definition doesn’t make sense | . | . | . . Multiplication Rule If we have an experiment with $n_1$ possible outcomes, and for each outcome of $1 text{st}$ experiment, there are $n_2$ possible outcomes for the $2 text{nd}$ experiment, $ ldots$, and for each outcome of $(r-1) text{th}$ experiment, there are $n_r$ possible outcomes for the $r text{th}$ experiment, then there are $n_1 * n_2 * ldots * n_r$ overall possible outcomes | The total number of possible outcomes of a series of experiments is the product of each experiment’s outcomes . | Example: 3 flavors of ice cream and 2 types of cones Ice cream flavor experiment has 3 outcomes (chocolate, vanilla, strawberry) | Waffle cone experiment has 2 outcomes (cake, waffle) | There are $6 text{ possible outcomes}= 2 times 3 = 3 times 2$ Order of cone/ice cream choice does not matter | . | . | . | . . Binomial Coefficient $ binom{n}{k}= frac{n!}{(n-k)!k!}, 0 text{ if } k&gt;n$ | The number of possible subsets of size k given n elements, where order does not matter Sets must contain distinct elements, so without replacement | Pronounced “n choose k” | . | Intuition: From the multiplication rule Choose k elements from n elements: $n(n-1)(n-2) ldots (n-k+1)$ | Order does not matter: divide by $k!$ because we over-counted by that factor Consider $ binom{5}{2}$ , we want to count $(1, 2)$ and $(2, 1)$ as equivalent, not separately | There are $k!$ possible orderings of a size k subset, so we divide by $k!$ to reduce it down to 1 | . | $ therefore binom{n}{k} = frac{n(n-1)(n-2) ldots (n-k+1)}{k!}= frac{n!}{(n-k)!k!}$ | . | Example: Find probability of full house in poker, 5 card hand A full house is 3 cards of 1 rank and 2 cards of another rank, e.g. 3 sevens and 2 tens | Assume deck is randomly shuffled so that all sets of 5 cards are equally likely Because we assume cards are evenly shuffled, we can justify using naive definition | . | Number of possible hands: $ binom{52}{5}$ , “52 choose 5” | Favorable hands (full house): $ binom{13}{1} binom{4}{3} times binom{12}{1} binom{4}{2}$ Of 13 ranks choose 1 and of 4 cards need 3, then of 12 remaining ranks choose 1 and of 4 cards we need 2 | . | $ therefore frac{ binom{13}{1} binom{4}{3} times binom{12}{1} binom{4}{2}}{ binom{52}{5}} = frac{13 binom{4}{3} times 12 binom{4}{2}}{ binom{52}{5}}$ | . | . | . . Sampling Table From the multiplication rule, we can determine how many ways there are to choose $k$ elements out of $n$ total elements | We can choose to sample with replacement (we can choose the same element again) or without replacement (we cannot choose the same element again) | .   Order Matters Order Doesn’t Matter . With Replacement | $n^k$ | $ binom{n+k-1}{k}$ | . Without Replacement | $nPk = frac{n!}{(n-k)!}$ | $ binom{n}{k} = frac{n!}{(n-k)!k!}$ | . Sample Method: With replacement, order matters $n_1 times n_2 times ldots times n_k = n^k$ | Have $k$ experiments, each with $n$ outcomes and order matters, so ${1, 2} neq {2, 1}$ | . | Sample Method: Without replacement, order matters $n times (n-1) times ldots times (n-k+1)$ | Have $k$ experiments, with $n$ outcomes without replacement and order matters, so ${1, 2} neq {2, 1}$ | . | Sample Method: Without replacement, order does not matter (Binomial Coefficient) $ frac{n times (n-1) times ldots times (n-k+1)}{k times (k-1) times ldots times 1} = frac{n!}{(n-k)! times k!} = binom{n}{k}$ | Have $k$ experiments, with $n$ outcomes without replacement and order does not matter, so ${1, 2} = {2, 1}$ | Want 1 combination of $k$ elements to represent all $k!$ permutations with the same $k$ elements, so need to divide by $k!$ | Think of grouping all possible permutations with the same $k$ elements into $k!$ groups For the $k=3$ choice $(1,2,3)$, there are $3!$ orderings that we want to group into 1 representation ${1,2,3}$ | To do this grouping for all different choices, we divide by $k!$ so that we’re left with the number of unique combinations of $k=3$ elements | . | . | Sample Method: With replacement, order does not matter (Bose-Einstein) $ frac{(n+k-1)!}{(n-1)! times k!} = binom{n+k-1}{n-1} = binom{n+k-1}{k}$ | Hardest to understand, not obvious from multiplication rule | Have $k$ experiments, with $n$ outcomes and order does not matter, so $(1, 2) = (2, 1)$ | . | . | . 2. Story Proofs, Axioms of Probability . General Advice for Homework Don’t lose common sense | Do check answers, especially by doing simple and extreme cases | Label people, objects, etc. (If we have n people, label them 1, 2, …, n) | . | . . Bose-Einstein Bose said flipping a fair coin twice had 3 equally likely outcomes $(HH, TT, 1H+1T)$ | Bose thought of the two coin flips as indistinguishable $(HT = TH)$, rather than $(HT, TH)$ | Thinking of coins as indistinguishable led to the discovery Bose-Einstein condensate | Sample Method: Pick k times from a set of n objects, where order does not matter and with replacement Prove the answer is $ binom{n+k-1}{k}$ ways | . | Case: if $k=0, binom{n-1}{0} = 1$ Only 1 way to choose 0 elements from $n-1$ elements, choose none! | . | Case: if $k=1, binom{n}{1} = n$ $n$ ways to choose 1 of $n$ elements | When only picking 1 element, no difference if with/without replacement or if order matters or does not matter | . | Case: if $n=2, binom{k+1}{k} = binom{k+1}{1} = k+1$ They are equal because: If we choose $k$ of $k+1$ elements, then the remaining is the 1. If we choose 1, then the remaining is the $k+1$ | If we get the first element $x$ times, we know we got the second element $k-x$ times | Number of possible first elements we get is from ${0, ldots, k}$ , so there are $k+1$ possibilities of choosing $k$ from $n=2$ elements | Can think of this as flipping a fair coin twice | . | Equivalent: How many ways are there to put k indistinguishable elements into n distinguishable groups? indistinguishable here means order does not matter and with replacement | Say $n=4, k=6$, a possible way is $(),(),(),()$ | We can also represent groups using separators: $( *** mid mid ** mid * )$ | There must be $k$ stars and $n-1$ separators, we just need to choose $k$ positions for stars in $n+k-1$ total positions, or $ binom{n+k-1}{k} = binom{n+k-1}{n-1}$ | . | . | . . Story Proof Proof by interpretation, rather than by algebra or calculus | Example: $ binom{n}{k} = binom{n}{n-k}$ Think about what this means in terms of choosing k elements rather than calculating factorials | . | Example: $n binom{n-1}{k-1} = k binom{n}{k}$ Imagine we pick k of n people, with one of them designated as the president | How many ways can we do this? | Approach 1: choose the president, then choose k-1 of n-1 people to form the rest of the group (left side) | Approach 2: choose $k$ of $n$ people, then select $1$ of the $k$ people to be the president (right side) | Both approaches count the same thing in different ways | . | Example: $ binom{m+n}{k}= sum_{j=0}^{k} binom{m}{j} binom{n}{k-j}$ (Vandermonde’s Identity) Imagine a group of $m$ people and a group of $n$ people, and we want to select $k$ people total from both groups | One way is we choose $j$ from group $m$ and $k-j$ from group $n$ | How many ways are there to choose $j$ from group $m$ and $k-j$ from group $n$? $ binom{m}{j} binom{n}{k-j}$ ways, using multiplication rule to get total number of ways for this $j$ | . | Then, we add those disjoint cases up to get the total number of ways! | . | . | . . Non-Naive Definition of Probability Probability Space: $S$ and $P$, where S is sample space, or the set of all possible outcomes of an experiment | P is a function that maps Input (Domain): any event A, which is a subset $A subseteq S$ | Output (Range): $P(A) in [0,1]$ a number between 0 and 1 | . | such that $P( emptyset)=0, P(S)=1$ Probability of empty set is 0 and probability of full set is 1 | What does it mean for the empty set to occur? If we get an outcome $S_0$ , if $S_0 in A$, then we say A occurred, else $A$ did not occur | If the empty set occurred, then $S_0 in emptyset$ , but nothing is in the empty set, so it cannot occur. | We want impossible events to be impossible $P( emptyset) = 0$ | . | Why is an outcome in S a certainty? $S_0$ must be in $S$ because $S$ represents the universe of all outcomes | . | . | $P( cup_{n=1}^{ infty} A_n) = sum_{n=1}^{ infty}P(A_n), text{ if } A_1,A_2, ldots text{ are disjoint/non-overlapping}$ Probability of the countably infinitely many union equals the sum of the probabilities if the events $A_1,A_2, ldots$ are disjoint/non-overlapping | . | . | . | Everything in probability basically derives from this definition with the two Axioms of Probability | . | . 3. Birthday Problem, Properties of Probability . Birthday Problem In a group of people, how likely is it that at least one pair of people share the same birthday? | How many people do you need for a 50% chance two people have the same birthday? $K$ people, find probability that 2 have the same birthday | Exclude Feb 29, assume other 365 days are equally likely (use naive definition of probability) | Assume independence of births, one person’s birthday has no effect on another person’s birthday | . | Pigeonhole Principle: If there are more people than days in a year, then the probability two people share the same birthday is certain If $K &gt; 365$, then probability is $1$ | Imagine 365 boxes, if there are more than 365 dots, at least one box must have more than one dot | . | Intuition says if probability of 1 for &gt; 365 people, then to get probability of 0.5, we need around 180 people. The answer is 23 people for 50.7% chance two people share the same birthday | . | Let $K &lt;= 365$ | Probability of no match (0 pairs share a birthday) Each of K persons has $ frac{1}{365}$ chance of a particular birthday | If no matches, every person must have a different birthday (no replacement) | $P( text{no match}) = frac{365 times 364 times ldots times (365 - K + 1)}{365^{K}}$ | . | Probability of match (at least 1 pair shares a birthday) Complement: $P( geq 1 text{ match}) = 1 - P( text{no match})$ | If $K=23$, $P( text{match}) = 1 - frac{365 times 364 times ldots times 343}{365^{23}} approx 1 - 0.492703 approx 0.507297 approx 50.7 %$ | If $K=50$, $P( text{match}) = 1 - frac{365 times 364 times ldots times 316}{365^{50}} approx 97 %$ | If $K=100$, $P( text{match}) = 1 - frac{365 times 364 times ldots times 343}{365^{100}} geq 99.999 %$ | . | How could we get a birthday match with such low number of people? The more relevant quantity is the number of pairs in K people $ binom{K}{2} = frac{K(K-1)}{2}$ | For $K=23$, $ binom{23}{2} = frac{23 * 22}{2} = 23 * 11 = 253 text{ pairs}$ | 253 is a lot of pairs, and any one of those pairs may share a birthday | . | . | If we then consider what K to have 50% chance of finding a pair with same birthday or off by 1 day, the answer is $K=14$ With 14 people, there is a better than 50% chance to find a pair of people with the same birthday or off by 1 day | More complicated problem to calculate | . | . | . . Axioms of Probability $P( emptyset) = 0, P(S) = 1$ Probability of empty set is 0 (impossible) | Probability of full sample space is 1 (certain) | . | $P( cup_{n=1}^{ infty} A_{n}) = sum_{n=1}^{ infty} P(A_{n})$ If $A_1, A2, ldots text{ are disjoint events}$ | Probability of the union (infinite or finite) equals the sum of probabilities of all events if events are disjoint. | Think of probabilities as disjoint sets, union of sets is same as adding the sets up * Assume $0 leq P(A) leq 1$ , probability is between 0 and 1 * As long as a thing satisfies both axioms, we can apply any theorems of probability to the thing. * From these 2 axioms, we can derive every theorem of probability. | . | | . . Properties of Probability $P(A^C) = 1 - P(A)$ Probability of complement of A is 1 minus probability of A | Proof: $1 = P(S) = P(A cup A^C)$ | $1 = P(A) + P(A^C)$ , since $A cap A^C = emptyset$ A and its complement are disjoint, nothing can be in both by definition | . | Therefore, $P(A^C) = 1 - P(A)$ | . | . | If $A in B$ , then $P(A) leq P(B)$ A and B are events, A is in B, “if A occurs then B occurs” | Proof: $B = A cup (B cap A^C)$ , where $A, (B cap A^C)$ are disjoint B is made of of A and everything in B that’s not in A | . | $P(B) = P(A) + P(B cap A^C) geq P(A)$ From axiom #2, adding some non-negative probability so must be $ geq$ | . | . | . | $P(A cup B) = P(A) + P(B) - P(A cap B)$ To get a probability of A union B, we can sum probabilities if A and B are disjoint. | Otherwise, if we add non-disjoint events A and B, we double count the overlapping intersection | Simple case of inclusion-exclusion with 2 events | Proof: Technique: “disjointification” of A and B so that we can apply axiom #2 | $P(A cup B) = P(A cup (B cap A^C))$ Union of A and B equals union of A and everything in B that’s not in A | . | $P(A cup B) = P(A) + P(B cap A^C)$ Since $A, (B cap A^C)$ are disjoint and we can apply axiom #2 | . | Technique: “wishful thinking” to see if two sides are equal | $P(A) + P(B) - P(A cap B) eqsim P(A) + P(B cap A^C)$ | $P(B) eqsim P(A cap B) + P(B cap A^C)$ $A cap B, A^C cap B$ are disjoint because no elements can be in both $A, A^C$ | Using axiom #2, $P(B) = P((A cap B) cup (B cap A^C))$ | B is equal to the part of B in A and part of B not in A | . | . | . | | . . General Inclusion-Exclusion Finds the probability of a union | Alternates inclusion and exclusion of portions | Case: Inclusion-Exclusion for 3 events $P(A cup B cup C) = P(A) + P(B) + P(C) - P(A cap B) - P(A cap C) - P(B cap C) + P(A cap B cap C)$ | Add all 3 events, subtract the intersections of pairs, then add the intersection of all | . | Case: Inclusion-Exclusion for N events $P(A_1 cup A_2 cup ldots cup A_N) = sum^{N}{j=1} P(A_j) - sum^{N}{i lt j} P(A_i cap A_j) + sum^{N}_{i lt j lt k} P(A_i cap A_j cap A_k) - ldots + (-1)^{N+1} times P(A_1 cap A_2 cap ldots cap A_N)$ | . | . | . . Example: Matching Problem / de Montmort’s Problem (1713) Problem originated from gambling problems: Imagine $N$ cards, labeled from 1 to N | Shuffle the cards | Flip over the cards while counting from 1 to N | If the count matches the flipped card’s number, then win | . | What’s the probability that one card has the same number as its position in the deck? | Let $A_j$ be the event “jth card matches” | $P( geq 1 text{ match}) = P(A_1 cup A_2 cup ldots cup A_N)$ | $P(A_j) = frac{1}{N}$ Probability that card’s number matches position is 1 over N since all positions are equally likely for the card labeled j | Naive definition uses $ frac{(N-1)!}{N!} = frac{1}{N}$ $N!$ possible permutations of deck of cards | $(N-1)!$ possible permutations of the deck of cards, fixing card labeled j is at jth position | . | . | $P(A_1 cap A_2) = frac{(N-2)!}{N!} = frac{1}{N(N-1)}$ $N!$ possible permutations of deck of cards | $(N-2)!$ possible permutations of the deck of cards, fixing card labeled i at ith position and card labeled j at jth position | . | $P(A_1 cap ldots cap A_K) = frac{(N-K)!}{N!}$ $N!$ possible permutations of deck of cards | $(N-K)!$ possible permutations of the deck of cards, fixing $K$ cards at their corresponding positions | . | Technique: Apply Inclusion-Exclusion $P(A_1 cup A_2 cup ldots cup A_N) = N ( frac{1}{N}) - ( frac{N(N-1)}{2!})( frac{1}{N(N-1)}) + ( frac{N(N-1)(N-2)}{3!})( frac{1}{N(N-1)(N-2)}) - ldots$ | Include $N$ 1-matches, subtract $ binom{N}{2}$ 2-matches, add …, subtract … | Everything cancels! | $P(A_1 cup A_2 cup ldots cup A_N) = frac{1}{1} - frac{1}{2!} + frac{1}{3!} - frac{1}{4!} + frac{1}{5!} - ldots + (-1)^{N+1} times ( frac{1}{N!})$ | This is the Taylor series for $e^x$ . | $P(A_1 cup A_2 cup ldots cup A_N) approx 1 - frac{1}{e}$ | . | . | . 4. Conditional Probability I . Example: Matching Problem / de Montmort’s Problem (1713) Most famous example of inclusion-exclusion Deck of N cards, labeled from 1 to N | Random shuffle and flip through deck | Win if a card’s label matches its position | . | Case: $A_j$ , the jth card in the deck is labeled j $P(A_j) = frac{1}{N}$ : for the jth card, there are N possible positions in the deck | . | Case $P( cup_{j=1}^{N} A_j)$ , the probability of a match on any card j To apply inclusion-exclusion we need: For any $K$ cards, $P(A_1 cap A_2 cap ldots cap A_K) = frac{(N-K)!}{N!}$ If K cards match their positions, then we fix those cards in place | The rest of the $N-K$ cards can be in any order, but the $K$ cards are constrained | . | There are $ binom{N}{K} = frac{N!}{(N-K)!K!}$ such terms like this because the inclusion-exclusion does $P( cap_{j=1}^{K} A_j)$ for any $K$ | These terms are all the same by symmetry. | Therefore, $P( text{match}) = P( cup_{j=1}^{N} A_j) = 1 - frac{1}{2!} + frac{1}{3!} - ldots (-1)^{N+1} times frac{1}{N!}$ $ frac{1}{N!}$ is the case where the cards are perfectly ordered from 1 to N, all cards match, which is 1 of $N!$ possible orderings | . | . | $P( text{no match}) = P( cap_{j=1}^{N} A^{C}_{J}) = 1 - (1 - frac{1}{2!} + frac{1}{3!} - ldots (-1)^{N+1} times frac{1}{N!}) approx frac{1}{e}$ Complement of union is the intersection of those complements | The factorials in denominators should remind you of Taylor series of $e^x$ | . | . | If there were an infinite amount of cards, what is $P( text{no match})$ ? The chance a card is in the exact position is very unlikely, but there are so many chances in an infinite deck. | The competing forces somehow reduce the answer to $ frac{1}{e}$ . | . | . | . . Independent Events Case: Two Events A, B are independent if $P(A cap B) = P(A) times P(B)$ The probability that both A and B occurs is the product of probability of A and probability of B | They’re independent, so we just multiply | . | Important: Independence is completely different from disjointness Disjointnesss says if A occurs, B cannot occur | . | Case: Three Events A, B, C are independent if $P(A cap B) = P(A)P(B), P(A cap C) = P(A)P(C), P(B cap C) = P(B)P(C), P(A cap B cap C) = P(A)P(B)P(C)$ The pairwise independence does not imply independence of all 3 events, so we need the last equation too | . | Case: N Events Follows the same pattern | Any 1, 2, 3, …, N events need to be independent | . | Basic Rule: “independent means multiply” from multiplication rule | . | . . Example: Newton-Pepys Problem (1693) Samuel Pepys wanted to know a gambling problem solution, and Newton solved it for him. | Problem Have some fair dice, each with 6 equally-likely sides, independent from each other | Which of these events is more likely? Event A: at least 6 with 6 dice | Event B: at least two 6’s with 12 dice | Event C: at least three 6’s with 18 dice | . | . | Pepys strongly believed Event C was more likely, but Newton solved the answer was Event A. | Can use naive definition but we will use independence to solve. Seeing “at least 1” should signal a union, but a complement of union is an intersection | Intersection of independent events signals multiply! | . | Probability of Event A Take complement: 1 minus probability that all dices are not 6 | $P(A) = 1 - ( frac{5}{6})^6 approx 0.665$ | . | Probability of Event B Take complement: 1 minus probability of no 6’s minus probability of exactly one 6 | $P(B) = 1 - ( frac{5}{6})^6 - 12( frac{1}{6})( frac{5}{6})^11 approx 0.619 $ Probability of exactly one 6: one dice is 6, the other 11 are not 6, and that one 6 dice can be any of the 12 total dice | . | . | Probability of Event C Take complement: 1 minus sum of probabilities of exactly zero to two 6’s | $P(C) = 1 - sum_{2}^{K=0} binom{18}{K} times ( frac{1}{6})^{K}( frac{5}{6})^{18-K} approx 0.597$ Choose position where the K dice will be 6 | Then set each K dice as 6 | Then the remaining dice can be anything not 6 | . | . | Therefore, Event A is the most likely, and Event C is the least likely. Though Newton got the calculation right, his intuition was confusing and wrong because it was not dependent on fair dice. | . | . | . . Conditional Probability How should you update probability/beliefs/uncertainty based on new evidence? “Conditioning is the soul of statistics.” - Professor Blitzstein | . | $P(A mid B) = frac{P(A cap B)}{P(B)}, text{ if } P(B) &gt; 0$ Probability that A occurs given that B occurs | If A, B are independent, this doesn’t matter $P(A mid B) = frac{P(A cap B)}{P(B)} = frac{P(A)P(B)}{P(B)} = P(A)$ | . | . | Intuition 1: Pebble World Imagine a sample space S and not all outcomes are equally likely | Imagine a finite number of outcomes, each represented by a pebble | Consider in our sample space, there are 9 possible outcomes, or 9 pebbles, some larger than others, with a total mass of 1 Event is a subset of sample space | Say Event B is a set of 4 pebbles | Say Event A is a set of 1 pebble in B and 3 pebbles outside of B | . | $P(A mid B)$ Since we know B occurred, the other 5 pebbles outside B are irrelevant | Get rid of pebbles in $B^C$, our universe got restricted to B since we know Event B occurred | In our new universe, find the pebbles that are also in Event A (numerator) | But now total mass is not 1, so we renormalize, or multiply by a constant so new total mass is 1 again (denominator) | . | . | Intuition 2: Frequentist World If we flipped a coin 1000 times, and 612 of them are heads, then we can say $P(H) approx 0.612$ | Interpret probability as the fraction of event occurring from repeating the experiment many times | $P(A mid B)$ Repeat an experiment many times | Circle the experiments where B occurred | Of the circled experiments, what fraction of them did A also occur? | . | . | Theorem 1 Suppose we wanted to find probability of A and B | $P(A cap B) = P(B) times P(A mid B) = P(B cap A) = P(A) times P(B mid A)$ | If A and B are independent, then $P(A mid B) = P(A), P(B mid A) = P(B)$ | . | Theorem 2 $P(A_1, ldots, A_N) = P(A_1) times P(A_2 mid A_1) times P(A_3 mid A_2, A_1) times ldots times P(A_N mid A_1, ldots, A_{N-1})$ | This is basically $N!$ theorems, repeatedly applying Theorem 1 multiple times | . | Theorem 3: Bayes’ Rule We want $P(A mid B)$ to relate to $P(B mid A)$ | So we divide Theorem 1 by $P(B)$ on both sides | $P(B) times P(A mid B) div P(B) = P(A) times P(B mid A) div P(B)$ | $P(A mid B) = frac{P(A)P(B mid A)}{P(B)}$ | . | . | . 5. Conditional Probability II, Law of Total Probability . Problem Solving “Thinking conditionally is a condition for thinking.” - Professor Blitzstein | How to Solve Problems Try simple and extreme cases | Break up the problem into simpler pieces/partitions | | . | . . Law of Total Probability Let $A_1, ldots, A_N$ be disjoint partitions of universe S | Consider a sample space B within universe S | $P(B) = P(B cap A_1)P(A_1) + ldots + P(B cap A_N)P(A_N)$ | . | . . Example: Get a random 2-card hand from standard deck of cards Find $P( text{both cards are aces} mid text{have an ace})$ $P( text{both cards are aces} mid text{have an ace}) = frac{P( text{both cards are aces} cap text{have an ace})}{P( text{have an ace})}$ | $P( text{both cards are aces} cap text{have an ace}) = P( text{both cards are aces})$ Having both cards aces is same as having one of the cards is an ace | . | $P( text{both cards are aces} mid text{have an ace}) = frac{P( text{both cards are aces})}{P( text{have an ace})}$ | $P( text{both cards are aces} mid text{have an ace}) = frac{ binom{4}{2} / binom{52}{2}}{1 - binom{48}{2}/ binom{52}{2}}$ Numerator: choose 2 of 4 Aces, out of all possible 2 card hands | Denominator: 1 minus choose 2 of 48 non-Aces, out of all possible 2 card hands | . | $P( text{both cards are aces} mid text{have an ace}) = frac{1}{33}$ | . | Find $P( text{both cards are aces} mid text{have Ace of Spades})$ We have the Ace of Spades, second card can be any card in the deck other than the Ace of Spades, by symmetry | $P( text{both cards are aces} mid text{have Ace of Spades}) = frac{3}{51} = frac{1}{17}$ | . | It’s twice as likely to have double Aces just by knowing the suit of the Ace we have! What is going on? Think about least one Ace vs. a specific Ace… | . | . | . . Example: A patient gets tested for a disease that affects 1% of the population and tests positive, using a test that is “95% accurate”. Let $D = text{ patient has disease}, T = text{ patient tests positive}$ | $P(T mid D) = 0.95 = P(T^C mid D^C)$ If the patient has the disease, 95% of the time the test will be positive. | If the patient does not have the disease, 95% of the time the test will be negative. | . | $P(D mid T) = frac{P(T mid D) * P(D)}{P(T)}$ Patient wants to know: if the test is positive, how likely do they have the disease? | Bayes’ Rule tells us how these are connected! | . | $P(D mid T) = frac{P(T mid D) * P(D)}{P(T mid D)P(D) + P(T | D^C)P(D^C)}$ | . We use the Law of Total Probability to find $P(T)$ | Probability patient tests positive is the sum of positive tests if patient tests positive and positive tests if patient tests negative | . | $P(D mid T) = frac{(0.95)(0.01)}{(0.95)*(0.01) + (1 - 0.95)(1 - 0.01)} approx 0.16$ | Although the test is 95% accurate, there is only a 16% chance the patient has the disease if they have a positive test! Why is this number so small and our intuition is so off? | We need to consider that it’s both rare that the test is wrong (5%) and rare that someone has the disease (1%)! | . | Intuition: If we had 1,000 patients, roughly 10 would have the disease (1%). If the 10 patients all test positive, but 5% of the 990 patients also test positive. Roughly speaking, 50 patients test positive who don’t have the disease and 10 patients test positive who do have the disease | This ratio of 50/10 is what leads to the $0.16$ . | . | Important: Bayes’ Rule is coherent Updating probabilities using the intersection of two new evidence is equal to updating probabilities using the first new evidence then updating again using the second new evidence, in any order | . | . | . . Common Mistakes with Conditional Probability Confusing $P(A mid B)$ with $P(B mid A)$ , “Prosecutor’s Fallacy” Example: Sally Clark Case Clark’s two babies died suddenly and was charged with murder | The only evidence that probability of baby dies not by murder is $ frac{1}{8500}$ | But since Clark lost two babies, the probability of not murder was $ frac{1}{8500} * frac{1}{8500}$ | However, this assumes independence between baby deaths | Want $P( text{innocence} mid text{evidence})$ , rather than $P( text{evidence} mid text{innocence})$ | . | . | Confusing $P(A)$ , the prior, with $P(A mid B)$ , the posterior If Event A occurs, $P(A mid A) = 1$ , but $P(A) neq 1$ | . | Confusing independence with conditional independence Conditional Independence: Events A and B are conditionally independent given Event C if $P(A cap B mid C) = P(A mid C) * P(B mid C)$ | Conditional independence given Event C does not imply independence. Example: Chess opponent of unknown strength If you play a series of chess games with the opponent, conditioning on the opponent’s strength, all games are independent (conditional independence) However, this does not imply the games are unconditionally independent! | . | If you win the first 5 games, then you think the opponent is weaker than you. The earlier games give you evidence of the opponent’s strength, even though the games are seemingly independent | . | Independence says earlier games give no indication of the opponent’s strength | It may happen that the game outcomes are conditionally independent given strength of opponent but not unconditionally independent | . | . | Unconditional independence does not imply conditional independence given Event C Example: If the alarm goes off (Event A), which can be caused by two causes: fire (Event F) or popcorn (Event C) Suppose F and C are independent. | But what’s the probability that there’s a fire given the alarm goes off and nobody is making popcorn? $P(F mid A, C^C) = 1$ | Must be 1 if we eliminate the only other explanation of making popcorn | . | F and C are initially independent, but not conditionally independent given A | . | . | . | | . 6. Monty Hall, Simpson’s Paradox . Example: Monty Hall Problem Problem Monty Hall was a game show host on “Let’s Make a Deal”. There are 3 doors (Door 1, Door 2, Door 3): one door has a car, the other two have goats. | Monty Hall asks you to choose a door. When you choose a door (Door 1), Monty Hall will open up either of the other doors (Door 2, Door 3), revealing a goat behind a door (Door 2) | You know the car is either the door you initially chose (Door 1) or the remaining unopened door (Door 3). Should you switch you initial door choice? | . | Assumptions The doors are equally likely to have the car | Monty Hall knows which door has the car behind it | You want the car, not the goats | Monty always opens a goat door after you choose an initial door | If Monty has a choice of which door to open (you initially picked the car), he will open one of the goat doors with equal probabilities | . | The answer is that you should switch, which gives a winning chance of $2/3$. Once the door is opened, the probabilities of each door is no longer $1/3$. | Monty opens Door 2 and reveals a goat, so it would seem the probabilities of winning is now $1/2$ each for Doors 1 and 3. | There is a subtle flaw in this thinking. We need to condition on all the evidence, which includes the fact Monty Hall opened Door 2. | . | Intuition: Use a tree diagram based on initial door choice, the car door, and the door Monty opens. . Initial Door Car Door Monty Door . 1 | 1 $(1/3)$ | 2 $(1/2)$ | . 1 | 1 $(1/3)$ | 3 $(1/2)$ | . 1 | 2 $(1/3)$ | 3 $(1)$ | . 1 | 3 $(1/3)$ | 2 $(1)$ | . If we condition on the fact Monty opened Door 2, we either took the top path (car is behind Door 1) or the bottom path (car is behind Door 3). The probability of the top path is $1/3 * 1/2 = 1/6$, and the probability of the bottom path is $1/3 * 1 = 1/3$. We renormalize the two options (make total sum = 1), to find the car has a $1/3$ chance of being behind Door 1 and a $2/3$ chance of being behind Door 3. | This means $1/3$ of the time your initial guess is correct, and the other $2/3$ of the time you should switch | . | Intuition: Law of Total Probability What do we wish we knew beforehand? In this case, we wish we know which door has the car. Now we condition on that. | Let $S$ = succeed (assuming we always switch) and $D_{j}$ = Door j has the car where $j in {1, 2, 3}$ | From the law of Total Probabilities, $P(S) = P(S vert D_{1}) frac{1}{3} + P(S vert D_{2}) frac{1}{3} + P(S vert D_{3}) frac{1}{3}$ In the first case: we pick Door 1, car is behind Door 1, Monty opens either Door 2 or Door 3, and we switch our choice, so $P(S vert D_{1}) = 0$ | In the second case: we pick Door 1, car is behind Door 2, Monty must open Door 3, and we switch our choice, so $P(S vert D_{2}) = 1$ | In the third case: we pick Door 1, car is behind Door 3, Monty must open Door 2, and we switch our choice, so $P(S vert D_{3}) = 1$ | . | Therefore, $P(S) = 0 + 1 ( frac{1}{3}) + 1 ( frac{1}{3}) = frac{2}{3}$ | By symmetry, we also know that $P(S vert text{ Monty opens Door 2}) = frac{2}{3}$ | . | Intuition: Consider if there were 1,000,000 doors and Monty opens 999,9988 doors. In this case it is clear you should switch since your initial guess is most likely incorrect. | . | Simpson’s Paradox Is it possible to have 2 doctors, where the first doctor has a higher success rate at every single type of surgery than the second doctor, yet the second doctor overall has a higher success rate? Simpson’s Paradox says this is possible, but this is very counterintuitive. How can one thing better in all individual cases be worse after aggregating the cases? | . | Consider first doctor is Dr. Hibbert and second doctor is Dr. Nick, and they both do 100 cases total of two types of surgeries: heart surgery and band-aid removal . Hibbert Heart Band-Aid . Success | 70 | 10 | . Failure | 20 | 0 | . Nick Heart Band-Aid . Success | 2 | 81 | . Failure | 8 | 9 | . | If you need surgery, who do you choose? For each individual operation (conditioning on a type of surgery), Dr. Hibbert has a higher success rate. But overall (unconditionally), Dr. Hibbert has a 80% success rate, and Dr. Nick has a 83% success rate. | Simpson’s Paradox in terms of Conditional Probability Let A = surgery is successful | Let B = treated by Dr. Nick | Let C = heart surgery Called the confounder or control, the type of surgery you want to condition on | If we fail to condition on C, then Simpson’s Paradox shows us the inequality can flip. | Knowing we got treated by Dr. Nick gives us information about what type of surgery we had, which then affects the probability of success. | . | $ P(A vert B, C) &lt; P(A vert B^{C}, C)$ The probability of success of heart surgery from Dr. Nick is less than probability of success of heart surgery from Dr. Hibbert. | . | $ P(A vert B, C^{C}) &lt; P(A vert B^{C}, C^{C})$ The probability of success of band-aid removal from Dr. Nick is less than probability of success of band-aid removal from Dr. Hibbert. | . | But overall, $P(A vert B)$ &gt; P(A vert B^{C})$ The probability of successful surgery is higher with Dr. Nick than with Dr. Hibbert. | Notice how the inequality flips | . | . | How is Simpson’s Paradox possible? How can the sign flip from the conditioning on individual events compared to overall? From the Law of Total Probability using conditional probability, $P(A vert B) = P(A vert B, C) P(C vert B) + P(A vert B, C^{C}) P(C^{C} vert B)$ | We know that $P(A vert B, C) &lt; P(A vert B^{C}, C)$ and that $P(A vert B, C^{C}) &lt; P(A vert B^{C}, C^{C})$ | However, we are unable to reduce it further to $P(A vert B)$ due to the terms $P(C vert B), P(C^{C} vert B)$, which are weights conditional on B. | Intuition: $P(C vert B)$ is the probability of performing heart surgery for Dr. Nick, which is very different than $P(C vert B^{C})$ probability of performing heart surgery for Dr. Hibbert. The weights $P(C vert B), P(C^{C} vert B)$ change, and that is what enables Simpson’s Paradox to happen. | . | . | Example: There was a lawsuit against UC Berkeley claiming sex discriminations in graduate admission rates. Looking at the overall admission rates, it looked like it was easier for men to get into UC Berkeley for graduate school. However, looking at the data of each individual apartment, the admission rates were generally more fair. The reason was that certain departments are more popular for women applicants and certain departments are harder to get into than others. . | Example: There are four jars that each contain two flavors of jellybeans. Suppose you like one flavor over the other. Suppose jars are “better” if they have a higher percentage of the jellybean you like. If we combine the two “better” jars into one large jar and likewise with the two “worse” jars, Simpson’s Paradox says the large jar created from the two “worse” jars may now have a higher percentage of the favorable jellybean and be “better”. | . 7. Gambler’s Ruin, Random Variables . 8. Random Variables and Their Distributions . 9. Expectation I, Indicator Random Variables, Linearity . 10. Expectation II . 11. Poisson Distribution . 12. Discrete vs. Continuous, Uniform Distribution . 13. Normal Distribution . 14. Location, Scale, LOTUS . 15. Midterm Review . 16. Exponential Distribution . 17. Moment Generating Functions I . 18. Moment Generating Functions II . 19. Joint, Conditional, Marginal Distributions . 20. Multinomial, Cauchy Distributions . 21. Covariance, Correlation . 22. Transformations, Convolutions . 23. Beta Distribution . 24. Gamma Distribution, Poisson Process . 25. Order Statistics, Conditional Expectation I . 26. Conditional Expectation II . 27. Conditional Expectation Given R.V. . 28. Inequalities . 29. Law of Large Numbers, Central Limit Theorem . 30. Chi-Square, Student-T, Multivariate Normal . 31. Markov Chains I . 32. Markov Chains II . 33. Markov Chains III . 34. A Look Ahead . References .",
            "url": "https://nhtsai.github.io/notes/harvard-stat110",
            "relUrl": "/harvard-stat110",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Database System Concepts",
            "content": "Database System Concepts by Silberschatz, Korth &amp; Sudarshan . Book Website | Online Copy | Student-friendly Source | . 1. Introduction to Database Systems . Database: collection of data that contains information relevant to an enterprise | Database-management system (DBMS): collection of interrelated data and set of programs to access those data Primary goal is to provide a convenient and efficient way to store and retrieve database information | Designed to manage large bodies of information, define structures for storage, provide ways to manipulate information, ensure safety of information in failures or breaches, avoid possible anomalous results | . | Database-System Applications Used to manage collections of data that are highly valuable, relatively large, and accessed by multiple users and applications, often simultaneously | Early database applications had simple, structured data, but modern database applications can use complex data with complex relationships | Databases are essential to every enterprise Sales: customer, product, purchase information | HR: employee information, salaries, payroll taxes, benefits | Banking: customer information, accounts, loans, transactions | Universities: student information, course registration, grades | . | Two modes of database usage Online transaction processing: large number of users retrieve small amounts of data and perform small updates | Data analytics: processing of data to draw conclusions and infer rules or decision procedures to drive business decisions Creating predictive models that take in attributes to output predictions | Data Mining combines knowledge discovery techniques from AI and statistics to implement efficient database operations | . | . | . | Purpose of Database Systems Created to manage commercial data through software | File-processing system: early form of database system in which files stored permanent records and used different applications to manipulate records to appropriate files | Issues with file-processing systems Data redundancy: copies of the same data appear in multiple files | Data inconsistency: copies of same data may no longer agree | Data access: difficulty of parsing entire files or writing custom applications for retrieving data | Data isolation: difficulty of working with data in various files in different formats | Data integrity: data values must satisfy consistency constraints | Data atomicity: upon failure, data should be restored to consistent state that existed prior to failure, either changes occur or not at all | Data concurrency-access: simultaneous manipulation of data without supervision may create inconsistent or invalid data | Data security: data should only be accessed by appropriate users with permission | . | . | View of Data Database system provides abstract view of the data, hiding implementation details of how data are stored and maintained | Levels of Data Abstraction, from lowest to highest Physical level: describes how the data are actually stored in complex, low-level data structures | Logical level: describes what data are stored in database and relationships among data in simple structures Physical data independence: logical level abstraction, or abstracting the complex, physical-level structures as relatively simple, logical level structures | . | View level: describes only part of entire database, simplifying the database to only the parts users are interested in Though logical level abstraction uses simpler structures, complexity comes from variety of information stored in a large database | System may provide many views for the same database | . | . | Example: Consider Instructor(id, name dept_name, salary), Department(dept_name, building, budget), Course(course_id, title, dept_name, credits), and Student(id, name, dept_name, total_credits) Physical level: records are stored in blocks of consecutive storage locations These details are hidden to programmers but important for database administrators | . | Logical level: each record has type definitions and relations to other records | View level: parts of the database are presented without logical level details in applications to users, who have the correct permissions | . | Instances and Schemas Instance: collection of information stored in database at a particular moment | Schema: overall design of the database, not changed or changed infrequently | Physical Schema: database design at the physical level, can be changed without affecting applications | Logical Schema: database design at the logical level, most important to programmers | Subschema: schemas at the view level that describe different views of the database | Physical data independence: not dependent on physical schema, no rewrites needed if physical schema changes | . | Data Models Data Model: collection of conceptual tools for describing data, relationships, semantics, and consistency constraints Describes design of database at physical, logical, and view levels | . | There are four categories of data models | Relational Model: uses a collection of tables, or relations, to represent both data and relationships among those data Each relation contains records with fields or attributes determined by its particular type | . | Entity-Relationship (E-R) Model: uses a collection of basic objects, or entities, and relationships among these objects, important for database design | Object-Based Data Model: uses object-oriented concepts of encapsulation, functions/methods, and object identity to extend entity-relationship model Object-Relational model combines object-oriented data model and relational data model | . | Semi-structured Data Model: allows data items of the same type to have different sets of attributes, often represented in Extensible Markup Language (XML) | Other obsolete data models include network data model and hierarchical data model | . | . | Database Languages Data-Manipulation Language (DML): language used to access or manipulate data as organized by appropriate data model, includes retrieval, insertion, deletion, modification Procedural DML: user specifies what data are needed and how to get those data | Non-Procedural or Declarative DML: user specifies what data are needed but not how to get those data | Query: statement requesting information retrieval, using the query language of a DML | . | Data-Definition Language (DDL): language used to specify database schema and additional properties of the data Data Storage and Definition Language: language used to specify storage structure and access methods used by database system | Consistency Constraints: constraints of data values stored in database, e.g. account balance must never be negative Usually checked every time a record is updated | . | Consistency constraints are implemented using integrity constraints that can be tested with minimal overhead | Domain Constraints: attribute must be of a certain domain (data type) | Referential Integrity: value that appears in one relation for a set of attributes also appears in another relation for the same set of attributes E.g. the dept_name of a course in the Course table must exist in the dept_name column of the Department table | Violations of referential integrity are caused by database manipulations, usually handled by rejecting the manipulation action that caused the violation | . | Assertions: any condition the database must always satisfy, every manipulation of the database is tested by the system for assertion validity | Authorization: expresses differentiation of users and their access permissions on various data values Read Authorization: allows reading but not modification | Insert Authorization: allows insertion but not modification | Update Authorization: allows modification but not deletion | Delete Authorization: allow deletion of data | Users can be assigned any combination of these permissions | . | DDL output is placed a data dictionary that contains metadata, or data about data Data dictionary: special type of table only internally accessible by database system, consulted before reading or modifying actual data | . | . | . | Relational Databases Relational Database: based on relational model, uses collection of tables to represent both data and relationships among those data, includes DML and DDL | Table/Record: multiple columns, each with a unique column name Record based model: database is structured in fixed-format records of several types Each table contains records of a particular type | Each record type defines a fixed number of attributes | Columns of table correspond to attributes of record type | . | Most basic table: CSV file However, unnecessarily duplicated information is a problem | . | . | SQL: non-procedural query language that includes DML and DDL | . -- DML for query SELECT Instructor.id, Department.dept_name FROM Instructor, Department WHERE Instructor.dept_name = Department.dept_name AND Department.budget &gt; 95000; -- DDL for schema CREATE TABLE department ( dept_name char (20), building char (15), budget numeric (12, 2) ); . Application Program: program that interacts with databases through using host language with embedded SQL queries To execute DML queries from host language, can either: Use API to send queries to database | Extend host language syntax to embed DML calls within host language program, uses DML precompiler for conversion | . | DML precompiler: converts DML statements to normal procedure calls in the host language | . | . | Database Design Conceptual-Design Phase: what attributes database should have and how to group these attributes to form the various fields Characterize fully the data needs of prospective database users to produce specification of user requirements | Choose data model, apply concepts of chosen data model to translate user requirements into a conceptual schema of the database Focus on describing data and their relationships, instead of worrying about low-level, physical implementation details | Can form tables using either E-R model or normalization algorithms | . | Review conceptual schema satisfies data requirements without conflicts or redundant features and satisfies functional requirements of project | Specification of Functional Requirements: users describe the kinds of operations/transactions that will be performed on the data | . | Logical-Design Phase: map high-level conceptual schema onto implementation data model of database system that will be used | Physical-Design Phase: specify physical features, including file organization and internal storage structures . | Entity-Relationship Model: collection of entities and their relationships Entity: distinguishable objects that are described by a set of attributes, e.g. Department(dept_name, building, budget). Entity Set: set of all entities of the same type | . | Relationship: association among several entities Relationship Set: set of all relationships of the same type | . | Overall logical structure, or schema, of a data base can be expressed graphically by an Entity-Relationship (E-R) Diagram | E-R diagrams are usually drawn using the Unified Modeling Language (UML) Entity sets are rectangular box with entity set name header and attributes listed below | Relationship sets are diamond with relationship name connecting a pair of related entity sets | Mapping Cardinalities: a constraint that expresses the number of entities to which another entity can be associated via a relationship set E.g. an instructor must be associated with only a single department, but a department can have many instructors | . | . | . | Normalization: process of generating a set of relation schemas to store information without unnecessary redundancy and to easily retrieve information Design schemas in an appropriate normal form using functional dependencies to determine desirable forms | Avoids repetition of information Moving repeated data across multiple rows into a single row of a different table and adding a relationship | . | Avoids inability to represent certain information Use null values to indicate unknown or missing values if inserting rows with incomplete information. | . | . | . | Database Engine Components of database system: storage manager, query processor, transaction management | Storage Manager: provides interface between low-level storage and queries from application programs Authorization and Integrity Manager: tests satisfaction of integrity constraints and checks authority of users who access the data | Transaction Manager: ensures database remains in consistent or correct state despite system failures and concurrent transaction executions proceed without conflicts, consists of concurrency-control manager and recovery manager Allows developers to treat a sequence of database accesses as if they were a single unit that either happens in its entirety or not at all | Abstracts away lower level details of concurrent access and system failures | . | File Manager: manages allocation of space on disk storage and data structures used to represent information on disk | Buffer Manager: fetches data from disk storage into main memory, decides what data to cache in main memory Enables database to handle data sizes that are larger than main memory size | . | . | Data structures implemented by storage manager Data Files: stores database itself | Data Dictionary: stores metadata about structure of the database, its schema | Indices: provides fast access to data items | . | Query Processor: helps simplify and facilitate access to data, translating non-procedural queries at the logical level to an efficient sequence of operations at the physical level DDL Interpreter: interprets DDL statements and records definitions in data dictionary | DML Compiler: translates DML statements in query language into evaluation plan of low-level instructions for query evaluation engine Query can be translated into many different evaluation plans with same result | Query Optimization: chooses the lowest cost evaluation plan for the query | . | Query Evaluation Engine: executes low-level instructions generated by DML compiler | . | Transaction Management Atomicity: operations that form a single logical unit of work happen in its entirety or not at all E.g. funds transfer by removing from A and adding to B should either happen together or not at all | . | Consistency: operations should preserve correctness of values E.g. the sums of balances between A and B should be preserved | . | Durability: changes as a result of successful operations should be persistent despite system failure E.g. new balances after funds transfer should persist even if the application crashes | . | Transaction: collection of operations that performs a single logical function in a database application A transaction is a unit of both atomicity and consistency, so it cannot violate any consistency constraints, though transaction execution can create temporary inconsistency | Programmer should properly define transactions that preserve consistency | E.g. a funds transfer should consist of debiting account A then crediting account B, both operations together form a transaction and should not be separate | . | Recovery Manager: ensures atomicity and durability properties Atomicity is achieved if no failures and all transactions complete successfully | But if transaction is incomplete due to failure, it should have no effect on the database | Failure Recovery: detects system failures and restores database to state prior to failure | . | Concurrency-Control Manager: controls interaction among concurrent transactions to ensure consistency | . | . | Database and Application Architecture | . . Shared-Memory architectures have multiple CPUs and exploit parallel processing using a common shared memory | Parallel databases are designed to run on a cluster consisting of multiple machines | Distributed databases allow data storage and query processing across multiple geographically separated machines | Two-Tier Architecture: application in client machine and queries database system in server machine | Three-Tier Architecture: client machine acts as front-end and does not contain direct database calls, communicates with application server to query database system Business Logic: specifies which actions to carry out under what conditions, embedded in central application server to handle multiple client front-ends | Better security and performance than two-tier architecture | . | Database Users and Administrators Naive User Uses predefined user interfaces, usually a forms interface where users can fill out fields of the form | Views reports generated from database | E.g. student registers for courses using registration website, application verifies user identity and provides form that can query the database to enroll | . | Application Programmer Computer professional who writes application programs | Chooses from many tools to develop user interfaces | . | Sophisticated User Interacts with system without writing programs | Forms requests using database query language or data software | E.g. analysts submit queries to explore data | . | Database Administrator (DBA): central control over DBMS, or data and programs that access those data Schema definition: defines original schema using DDL statements | Storage structure and access-method definition: specifies parameters for the organization of data and indices | Schema and physical-organization modification: changes schema and physical organization of data to reflect changing needs or improve performance | Granting of authorization for data access: regulates access of database by changing authorization information kept in an internal, special structure of the DBMS | Routine maintenance: periodic backups to remote servers, ensuring enough free disk space or upgrading disk space, monitoring performance across workload | . | . | History of Database Systems 1950s to early 1960s Magnetic tapes allowed sequential storage of data | Data processed by reading from the master tape and writing to another, creating a new master tape | . | Late 1960s to early 1970s Hard disks allowed direct access to any position in data | Network and hierarchical data models developed | Codd (1970) defines relational model and non-procedural querying | . | Late 1970s and 1980s IBM’s System R prototype leads to efficient relational DBMSs IBM’s SQL/DS, UC Berkeley’s Ingres, and first version of Oracle | Relational models eventually overtake other data models | Research on parallel, distributed, and object-oriented databases | . | 1990s Creation of SQL language for query-intensive workloads | Web interfaces for databases | High transaction-processing rates and high reliability | . | 2000s Semi-structured, graph, and spatial databases | XML and JSON data-exchange formats | Open-source PostgreSQL and MySQL | Auto-admin features for automatic reconfiguration | Column-stores for data analytics and data mining, | Map-reduce parallelism programming framework | NoSQL systems for lightweight data management for new types of data and eventual consistency allowed for greater scalability and availability | . | 2010s NoSQL systems develop stricter consistency and higher levels of abstractions | Cloud services allowed outsourcing of data storage and applications, software as a service | Data privacy regulations introduced | . | . | . 2. Introduction to the Relational Model . Relational model is the primary data model for commercial data-processing Simplicity compared to earlier data models, i.e. network, hierarchical | Added object-relational features, e.g. complex data types, stored procedures | Added support for XML data and semi-structured data | Independent from any specific underlying low-level data structures | . | Structure of Relational Databases Relational database consists of a collection of relations, or tables Each table has a unique name, columns, and rows | . | Tuple: row, represents a relationship among a set of values | Attribute: column, describes the values in a tuple | Relation Instance: specific instance of a relation, containing a specific set of rows | Domain: set of permitted values for an attribute Atomic Domain: elements of the domain are considered to be indivisible units | . | Null Value: special value signifying value is unknown or does not exist | Strict structure of relations is good for storage and data processing in well-defined and relatively static applications, but bad for applications with changing data types and structures | . | Database Schema Database Schema: logical design of the database | Database Instance: snapshot of data in database at a given instant in time | Relation Schema: definition of a relation variable, consisting of a list of attributes and their corresponding domains Relation may change over time, but the schema does not generally change | . | . | Keys Tuples must be uniquely identified by their attributes, where no two tuples in a relation are allowed to have | Superkey: set of one or more attributes that, taken collectively, uniquely identifies a tuple in the relation Let $R$ denote the set of attributes in the schema of relation $r$ | If subset $K subset R$ is a superkey for $r$ , then no two distinct tuples have the same values on all attributes in $K$ for relation instances of schema $r$ | If $t_1, t_2$ are in $r$ and $t_1 neq t_2$ , then $t_1.K neq t_2.K$ | . | Superkey can contain extraneous attributes Any superset of a superkey is also a superkey | Example: unique ID column with any other column also forms a superkey | . | Candidate Key: minimal superkey for which no proper subset is also a superkey | Primary Key: candidate key that is chosen by database designer as principal means of identifying tuples within a relation A key (primary, candidate, super) is a property of the entire relation, rather than individual tuples | Any two tuples in the relation cannot have same values for key attributes | Primary Key Constraint: the primary key is considered a constraint on real-world enterprise being modeled | Primary keys are usually listed first and underlined in relation schema | Primary keys should be chosen such that attribute values are never or very rarely changed | . | Foreign Key: an attribute set in one relation that references the primary key of another relation Foreign Key Constraint: foreign key attributes in a relation must have the same values for primary key attributes of the referenced relation The foreign key must have the same attributes and values as the primary key being referenced | . | Referencing Relation: relation that has the foreign key constraint | Referenced Relation: relation that is being referenced | . | Referential Integrity Constraint: the values appearing in specified attributes of any tuple in the referencing relation must also appear in specified attributes of at least 1 tuple in referenced relation A general case of the foreign key constraint | Attributes in referencing relation must reference existing attributes in referenced relation | The attribute could be referencing a part of the primary key, so this cannot be enforced with foreign key constraint, which requires referencing attributes form the whole primary key | . | . | Schema Diagrams Schema Diagram: diagram of database schema with primary keys (underlined), foreign key constraints (single-headed arrows), referential integrity constraints (two-headed arrows), relations as boxes with their attributes, and arrows indicating relationships | . | Relational Query Languages Query Language: language used to request information from database | Imperative Query Language: user instructs system to perform a specific sequence of operations on the database to compute desired result State variables are updated in the course of computation | . | Functional Query Language: computation consists of evaluation of functions that may operate on data in database or results of other functions Functions are free of side effects (pure) and don’t update program state | . | Declarative Query Language: user describes desired information in a form of mathematical logic, and the database system has to figure out how to obtain desired information through computation | Example: relational algebra and SQL are functional, tuple relational calculus and domain relational calculus are declarative | . | Relational Algebra Relational Algebra: set of operations that take one or two relations as input and produces a new relation as the result Unary operations operate on one relation (select, project, rename) | Binary operations operate on pairs of relations (union, Cartesian product, set difference) | Though a relation is a set of tuples and duplicate tuples are prohibited (by definition of set), database systems typically allow duplicate tuples in practice in multisets (sets that allow duplicates) | . | Select: selects tuples that satisfy a given predicate Example: $ sigma_{ theta}(r_1)$ | . | Project: produces a relation Example: $ Pi_{ text{attributes}}(r_1)$ | . | Composition of Relational Operations Relational Algebra Expression: expression consisting of relational algebra operations composed together | . | Cartesian Product: combines information from any two relations Example: $r_1 times r_2$ | . | Join: combines a selection and Cartesian product into a single operation Example: $r_1 bowtie_{ theta} r_2$ | Equivalent to $ sigma_{ theta}(r_1 times r_2)$ , selecting all the matching rows of the cartesian product | . | Union: combines tuples that appear in either or both input relations Example: $r_1 cup r_2$ | Input relations must be compatible: Input relations must have same arity, or number of attributes | Those attributes must have same types | . | . | Intersection: finds tuples that appear in both input relations Example: $r_1 cap r_2$ | . | Set Difference: finds tuples that are in one relation but not in another Example: $r_1 - r_2$ | . | Assignment: assigns parts of an expression to temporary relation variables Example: $r_3 leftarrow ( Pi_{L}( sigma_{ theta}(r_1)))$ | . | Rename: gives names to results of expressions Example: $ rho_{X}(E)$ to rename expression result to x | Example: $ rho_{X(A_1, A_2, ldots, A_n)}(E)$ to also rename attributes | . | Equivalent Queries There is often more than 1 way to write a query in relational algebra that are equivalent, or give the same result on any database | Query optimizers often look at the result to figure out the most efficient but equivalent alternative expressions | . | . | . 3. Introduction to SQL . SQL Overview . | SQL Data Definition . | Basic Structure of SQL Queries . | Additional Basic Operations . | Set Operations . | Null Values . | Aggregate Functions . | Nested Subqueries . | Database Modification . | . 4. Intermediate SQL .",
            "url": "https://nhtsai.github.io/notes/database-system-concepts",
            "relUrl": "/database-system-concepts",
            "date": " • Sep 23, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Linear Algebra",
            "content": "MIT 18.06SC: Linear Algebra, Fall 2011 . Course Resources . Professor Gilbert Strang | Course Lectures | Course Website | Course Syllabus | . 1. The Geometry of Linear Equations . Lecture Summary | . The fundamental problem of linear algebra is to solve a system of equations. This is represented by Ax=bA boldsymbol{x} = boldsymbol{b}Ax=b. . 2-variable example: . 2x−y=0−x+2y=3 begin{aligned} 2x-y &amp;= 0 -x+2y &amp;= 3 end{aligned}2x−y−x+2y​=0=3​ . In the Row Picture method, we view the system of equations as separate lines on the xy-plane. The solution is (1,2)(1, 2)(1,2), the point where the two lines intersect. . In the Column Picture method, we view the system of equations as a linear combination of column vectors. The solution is x=1,y=2x=1,y=2x=1,y=2, the coefficients of column vectors that linearly combine to create b boldsymbol{b}b. . x[2−1]+y[−12]=[03]x begin{bmatrix} 2 -1 end{bmatrix} + y begin{bmatrix} -1 2 end{bmatrix} = begin{bmatrix} 0 3 end{bmatrix}x[2−1​]+y[−12​]=[03​] . In the Matrix Form method, we view the system of equations as a matrix multiplication. . Ax=bA boldsymbol{x} = boldsymbol{b}Ax=b [2−1−12][xy]=[03] begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix} begin{bmatrix} x y end{bmatrix} = begin{bmatrix} 0 3 end{bmatrix}[2−1​−12​][xy​]=[03​] . where A=[2−1−12]A = begin{bmatrix} 2 &amp; -1 -1 &amp; 2 end{bmatrix}A=[2−1​−12​] is the coefficient matrix, x=[xy] boldsymbol{x}= begin{bmatrix} x y end{bmatrix}x=[xy​], and b=[03] boldsymbol{b}= begin{bmatrix} 0 3 end{bmatrix}b=[03​] . 3-variable example: . {2x−y=0−x+2y=−1−3y+4z=4 left { begin{array}{rl} 2x-y &amp;= 0 -x+2y &amp;= -1 -3y+4z &amp;= 4 end{array} right.⎩⎪⎨⎪⎧​2x−y−x+2y−3y+4z​=0=−1=4​ . 2. Elimination with Matrices . References .",
            "url": "https://nhtsai.github.io/notes/mit-18-06sc",
            "relUrl": "/mit-18-06sc",
            "date": " • Sep 22, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Azure Fundamentals",
            "content": "AZ-900: Microsoft Azure Fundamentals Certification Course . Course Video | Content Outline 15-25%: Cloud Concepts | 30-35%: Azure Core Services | 25-30%: Security, Privacy, Compliance, and Trust | 20-25%: Pricing and Support | . | Need to get around 700/1000 or 70% to pass. | Exam has 40-60 questions, can get 12-18 questions wrong. | Question types: multiple choice, multiple answer, drag and drop, hot area (multiple drop-down menus) | Exam time is 60 min, seat time is 90 min. | Valid for 24 months/2 years before re-certification. | . Cloud Concepts . Cloud Computing: practice of using a network of remote servers hosted on Internet to store, manage, and process data, rather than a local server or a personal computer On-Premise: you own the server, manage the infrastructure, own all the risk | Cloud Providers: servers are owned and managed by other parties | . | Dedicated Servers: one physical machine dedicated to a single business Runs a single website or application | Very expensive, high maintenance, high security | . | Virtual Private Server: one physical machine dedicated to a single business virtualized into sub-machines to run multiple websites or applications | . | Shared Hosting: one physical machine shared by hundreds of businesses relies on most tenants under-utilizing their resources | very cheap, very limited | . | Cloud Hosting: multiple physical machines that act as one system abstracted into multiple cloud services | flexible, scalable, secure, cost-effective, high configurability | . | Most Common Infrastructure as a Service (IaaS) Cloud Services Compute: a virtual machine that can run applications, programs, and code | Storage: a virtual hard-drive that can store files | Networking: a virtual network that can define internet connections or network isolations | Databases: a virtual database for storing reporting data or general purpose application use | . | Benefits of Cloud Computing Cost Effective: pay for what you consume, no upfront cost | Global: launch workloads anywhere in the world | Secure: cloud services can be secure by default, handled by provider | Reliable: data backup, disaster recover, replication, fault tolerance | Scalable: adapt resources and services based on demand | Elastic: automate scaling | Current: underlying software is upgraded by provider | . | Types of Cloud Computing Software as a Service (SaaS): product that is run and managed by a service provider, for consumers Salesforce, Gmail, Office 365, other software in the cloud | . | Platform as a Service (PaaS): focus on deployment and management of your apps, for developers AWS Elastic Beanstalk, Heroku, Google App Engine | . | Infrastructure as a Service (IaaS): cloud IT, provides access to networking features, computers, and data storage space, for administrators Microsoft Azure, AWS, Oracle Cloud | . | . | Types of Cloud Computing Responsibilities Customer is responsible | . On-Premise IaaS PaaS SaaS . Applications | Applications | **Applications** | Applications | . Data | Data | Data | Data | . Runtime | Runtime | Runtime | Runtime | . Middleware | Middleware | Middleware | Middleware | . OS | OS | OS | OS | . Virtualization | Virtualization | Virtualization | Virtualization | . Servers | Servers | Servers | Servers | . Storage | Storage | Storage | Storage | . For on-premise, customer is responsible for everything! | . | . Microsoft Azure . Tech company based in Redmond, Washington that makes software, devices, cloud services, search engines. Most known for Windows Operating System | . | Azure is the cloud service provider (CSP) from Microsoft Means “bright blue color of the cloudless sky” | . | Azure Deployment Models .   Cost Security Configuration Technical Knowledge   . Public Cloud | everything built on cloud provider, cloud native | most cost-effective | security controls by default, may not meet your security requirements | configuration limited by provider | no in-depth knowledge needed | . Private Cloud | everything built on company’s data centers, on-premise | most-expensive | no security guarantee, can meet your security requirements | configurable anyway | in-depth knowledge needed to configure private infrastructure | . Hybrid Cloud | using both on-premise and cloud-native from cloud service provider | depends on cloud usage | need to secure, can meet your security requirements | best of both words | in-depth knowledge needed to configure private infrastructure and of cloud services | . Cross Cloud | using multiple cloud providers, multi-cloud | security controls spread across providers | configurations limited across providers | in-depth knowledge needed of multiple cloud providers |   | . | Total Cost of Ownership (TCO) On-premise (CAPEX): software license fees, implementation, configuration, training, physical security, hardware, IT personnel, maintenance | Azure/Cloud (OPEX): subscription fees, implementation, configuration, training | Usually save 75% by moving to cloud services | . | Capital vs. Operational Expenditure Capital Expenditure (CAPEX): spending money upfront on physical infrastructure, deduct that expense from tax bill over time, on-premise business model Server costs | Storage costs | Network costs | Backup and Archive costs | Disaster recovery costs | Data center costs | Technical personnel costs | Need to guess upfront what you plan to spend | . | Operational Expenditure (OPEX): physical costs are shifted to cloud service provider, customer handles non-physical costs Software lease and customization costs | Cloud training costs | Cloud support costs | Cloud metric billing costs (compute, storage) | Can try a product or service without investing in equipment | . | . | Availability: ability to ensure service remains available High Availability: ensure no single point of failure and/or ensure a certain level of performance | Having multiple data centers in multiple availability zones (AZs) ensures availability | Load Balancer: evenly distributes traffic to multiple servers in a data center, rerouting traffic in case of server failure | Azure Load Balancer | . | Scalability: ability to grow rapidly or unimpeded High Scalability: increase capacity based on increased demand of traffic, memory, and compute power | Vertical Scaling: upgrading a single machine, scaling up | Horizontal Scaling: adding additional servers of the same size, scaling out | . | Elasticity: ability to shrink and grow to meet demand High Elasticity: automatically change capacity based on current demand of traffic, memory, and compute power | Use horizontal scaling to scale in or scale out machines | Generally, high elasticity is hard to accomplish with vertical scaling | Azure VM Scale Sets, SQL Server Stretch Database | . | Fault Tolerance: ability to prevent or handle failures | Disaster Recovery: ability to recover from failures, high durability High Durability: ability to recover from a disaster and prevent loss of data | Backup archives, restoring backups, backup functionality, ensure live data is not corrupt | . | . Evolution of Computing . Dedicated: physical server wholly utilized by a single customer Need to guess total capacity needed when buying a server, might be underutilized | Upgrading is slow and expensive | Limited by operating system | Multiple apps can fight over shared resources | Guaranteed security, privacy, and full utility of underlying resources | . | Virtual Machines (VMs): multiple virtual machines run on one machine Hypervisor: software layer to run virtual machines | Physical server is shared by multiple customers | Need to guess total capacity, might be underutilized, pay for a fraction of the server | Limited by guest operating system | Multiple apps can fight over shared resources if run on a single VM | . | Containers: multiple containers run on one virtual machine Docker Daemon: software layer that lets you run multiple containers | Can maximize capacity used by application to be more cost-effective | Containers share same underlying OS, but each container can run a different OS for more flexibility | Multiple apps can run side by side in containers without fighting over shared resources | . | Functions: managed VMs running managed containers, apps divided up into functions, serverless compute Upload code and choose amount of memory and duration, only responsibility for code | Very cost-effective, only pay for code execution time, VMs only execute when needed | Cold Starts: overhead time needed to start a server affects execution time | . | . Global Infrastructure . Region: grouping of multiple data centers, or availability zones | Geography: discrete market of 2+ regions that preserves data residency and compliance boundaries United States, US Government, Canada, Brazil, Mexico | Used to guarantee data will remain within a country’s boundaries | . | Paired Regions: each region is paired with another region 300 miles away only one region is updated a a time to ensure no outages, also used for disaster recovery | Azure Geo-Redundant Storage: replicates data to a secondary region automatically, ensuring data is durable even in the event that the primary region isn’t recoverable. | . | Not all Azure cloud services are available in every region. | Recommended Region: a region that provides the broadest range of service capabilities and is designed to support availability zones now or in the future | Alternate Region: a region that extends Azure’s footprint within a data residency boundary where a recommended region also exists, not designed to support availability regions. | General Availability (GA): when a service is considered ready to be used publicly by everyone | Azure Cloud services are grouped into 3 categories by when services are available Foundational: available immediately or within 12 months in recommended and alternate regions | Mainstream: available immediately or within 12 months in recommended regions, available in alternate regions based on customer demand | Specialized: available in recommended or alternate regions based on customer demand | . | Special Regions: regions to meet compliance or legal reason US Government | Chinese Government via partnership with 21Vianet for data centers | . | Availability Zones (AZs): physical location made up of 1+ data centers Data Center: secured building that contains hundreds of thousands of computers | A region will generally contain 3 AZs | Data centers within a region are isolated from each other but will be close enough for low-latency | Common practice to run workloads in 3 AZs to ensures high availability | . | AZ Supported Regions: not every region has support for availability zones, i.e. Alternate or Other Recommended regions have at least 3 AZs | . | Fault and Update Domains An AZ in an Azure region is a combination of a fault domain and an update domain | Fault Domain: logical grouping of hardware to avoid a single point of failure within an AZ Group of VMs that share a common power source and network switch | If part of data center fails, other servers won’t be taken down. | . | Update Domain: ensure resources don’t go offline when underlying hardware and software is updated | . | Availability Set: logical grouping that ensures VMs are in different fault and update domains to avoid downtime Each VM in an availability set is assigned a fault domain and an update domain. | Fault Domain e.g. server rack | Update Domain e.g. a group of specific servers across racks | Higher number of domains means more isolated VMs. Update domain must be 1 when fault domain is 1. | . | . | . Azure Services Overview . Computing Services Azure Virtual Machines: Windows or Linux VMs, most common type of compute You choose OS, Memory, CPU, and Storage, share hardware with other customers | . | Azure Container Instances: run containerized apps on Azure without provisioning servers or VMs, Docker as a Service | Azure Kubernetes Service (AKS): easy to deploy, manage, and scale containerized applications, Kubernetes (K8) as a Service | Azure Service Fabric: distributed systems platform for easy to package, deploy, and manage scalable and reliable microservices, Tier-1 Enterprise Containers as a Service | Azure Functions: event-driven, serverless compute (functions) run code without provisioning or managing servers, pay for compute time, Functions as a Service | Azure Batch: Plans, schedules, and executes batch compute workloads across running 100+ jobs in parallel, can use Spot VMs to save money (using low-priority VMs to save money) | . | Storage Services Azure Blob Storage: store very large files and large amounts of unstructured files, pay for what you store, unlimited storage, no-resizing volumes, filesystem protocols, Object Serverless Storage | Azure Disk Storage: a virtual SSD or HDD volume, encryption by default, attached to VMs | Azure File Storage: shared volume that you can access and manage like a file server | Azure Queue Storage: data store for queueing and reliably delivering messages between applications, Messaging Queue | Azure Table Storage: Wide-column NoSQL database that hosts unstructured data independent of any schema | Azure Databox (Heavy): rugged briefcase computer and storage designed to move TB or PB data, ship data in a physical device | Azure Archive Storage: long-term cold storage for when you need to hold onto files for years | Azure Data Lake Storage: centralized repository to store structured and unstructured data at any scale | . | Database Services Azure Cosmos DB: fully managed NoSQL database designed for scale with 99.999% availability | Azure SQL DB: fully managed MS SQL database with auto-scale, integral intelligence, and robust security | Azure Database: fully managed and scalable MySQL, PostgreSQL, MariaDB database with high availability and security | SQL Server on VMs: host enterprise SQL Server apps in cloud, convert MS SQL on-premise to Azure Cloud | Azure Synapse Analytics: fully managed data warehouse with integral security at every level of scale at no extra cost | Azure Database Migration Service: migrates databases to cloud with no application code changes | Azure Cache for Redis: caches frequently used and static data to reduce data and application latency | Azure Table Storage: wide-column NoSQL database that hosts unstructured data independent of any schema | . | Application Integration Services Azure Notifications Hub: send push notifications to any platform from any backend, Pub/Sub | Azure API Apps: quickly build and consume APIs in the cloud, API Gateway to Azure Services | Azure Service Bus: reliable cloud messaging as a service and simple hybrid integration | Azure Stream Analytics: serverless real-time analytics, from cloud to edge | Azure Logic Apps: schedule, automate, and orchestrate tasks, business processes, and workflows, integrate with Enterprise SaaS and Enterprise applications | Azure API Management: hybrid, multi-cloud management platform for for APIs across all environments, put in front of existing APIs for additional functionality | Azure Queue Storage: data store for queueing and reliably delivering messages between applications, Messaging Queue | . | Developer and Mobile Tools Azure SignalR Service: easily add real-time web functionality to applications, Real-Time Messaging, like Pusher for Azure | Azure App Service: easy to use service for deploying and scaling web applications, like Heroku for Azure | Visual Studio: IDE designed for creating powerful, scalable applications for Azure | Xamarin: create powerful and scalable native mobile apps with .NET and Azure, Mobile-App Framework | . | DevOps Services Azure Boards: proven agile tools to plan, track, and discuss work across teams, Kanban Boards or Github Projects | Azure Pipelines: build, test, and deploy with CI/CD that works with any language, platform, and cloud | Azure Repos: unlimited, cloud-hosted, private Git repos to collaborate and build code with pull requests and advanced file management | Azure Test Plans: test and ship with confidence using manual and exploratory testing tools | Azure Artifacts: create, host, and share packages with your team, add artifacts to CI/CD pipelines | Azure DevTest Labs: fast, easy, and lean dev-test environments | Azure Resource Manager (ARM): programmatically create Azure resources via JSON template Infrastructure as Code (IaC): process of managing and provisioning data centers through scripts, rather than physical hardware configuration or interactive configuration tools | . | Azure Quickstart Templates: library of pre-made ARM templates to quickly launch new projects, use scripts to quickly set up a project E.g. Deploy a Django App, Deploy Ubuntu VM with Docker Engine, Web App on Linux with PostgreSQL, etc. | . | . | Cloud-Native Networking Services Azure DNS: ultra-fast DNS responses and ultra-high domain availability | Azure Virtual Network (vNet): logically isolated section of Azure network where you launch your Azure resources, choose range of IP addresses using CIDR Range Create a virtual network within an Azure network with public and private subnets | CIDR Range of 10.0.0.0/16 = 65,536 IP Addresses lower number (/16) means more IP addresses | . | Subnets: logical partition of IP network, breaking up vNet IP range into smaller CIDR ranges, i.e. subnet CIDR Range 10.0.0.0/24 = 256 IP Addresses Public Subnet: can reach the internet, e.g. web app | Private Subnet: cannot reach internet, e.g. database | . | . | Azure Load Balancer: OSI Level 4 (Transport, lower-level) load balancer | Azure Application Gateway: OSI Level 7 (HTTP, app-level) load balancer, can apply Web Application Firewall service | Networking Security Groups: virtual firewall at subnet level to secure ports | . | Enterprise/Hybrid Networking Services Azure Front Door: scalable, secure entry point for fast delivery of global applications | Azure Express Route: connection between on-premise to Azure Cloud, 50 MB/s to 10 GB/s | Virtual WAN: networking service that brings networking, security, and routing functionalities together to provide single operational interface | Azure Connection: VPN connection securely connecting two Azure local networks via IPSec | Virtual Network Gateway: site-to-site VPN connection between Azure virtual network and local network. | . | Azure Traffic Manager: operates at DNS layer to direct incoming DNS requests based on routing method of your choice, i.e. weighted, performance, priority, geographic, multivalue, subnet Route traffic to servers geographically near to reduce latency | Fail-over to redundant systems in case primary systems failure | Route to random VM to simulate A/B testing | . | Azure DNS: host domain names on Azure, create DNS Zones, manage DNS records . | Azure Load Balancer: evenly distribute incoming network traffic across a group of backend resources or servers, operates on OSI Layer 4 (Transport), does not understand HTTP requests Public Load Balancer: routes incoming traffic from internet to public-facing servers (Public IPs) | Private Load Balancer: routes incoming traffic from internet to private-facing servers (Private IPs) | . | Scale Sets: group identical VMs and automatically increase or decrease amount of servers based on: change in CPU, memory, disk, network performance | on a predefined schedule | used for Elasticity | . | IoT Services Internet of Things (IoT): network of internet connected objects able to collect and exchange data | IoT Central: connects IoT devices to cloud | IoT Hub: secure and reliable communication between IoT app and managed devices | IoT Edge: fully managed IoT Hub service, data processing and analysis nearest the IoT devices Edge Computing: offload compute from cloud to local computing hardware, IoT devices | . | Windows 10 IoT Core Services: subscription for long-term OS support and services for Windows 10 IoT Core devices | . | Big Data and Analytics Services Big Data: massive volumes of structured and unstructured data that are difficult to move and process using traditional techniques | Azure Synapse Analytics: enterprise data warehousing and big data analytics, run SQL queries against large databases for reporting | HDInsight: open-source analytics software for Hadoop, Kafka, Spark | Azure Databricks: Spark-based analytics platform on Azure cloud | Data Lake Analytics: on-demand analytics job service that simplifies big data Data Lake: storage repo that holds large amounts of raw data | . | . | AI/ML Services Artificial Intelligence (AI): machines perform jobs that mimic human behavior | Machine Learning (ML): machines that get better at a task without explicit programming | Deep Learning (DL): machines have artificial neural network to solve complex problems | Azure Machine Learning Service: service to run AI/ML workloads to build flexible pipelines to automate workflows | Personalizer: personalized experiences for every user | Translator: real-time translation | Anomaly Detector: detect anomalies to quickly identify and troubleshoot issues | Azure Bot Service: serverless bot services that scales on demand | Form Recogniser: automate extraction text, key/value pairs, and tables | Computer Vision: customize computer vision models | Language Understanding: build natural language understanding into apps | QnA Maker: create conversational QnA bot from existing content | Text Analytics: extract sentiment, key phrases, named entities, language from text | Content Moderator: moderate text and images for safer user experience | Face: detect and identify people and emotions in images | Ink Recogniser: recognize digital handwritten content | . | Serverless Services Serverless: underlying servers, infrastructure, and OS fully managed by service provider, highly available, scalable, cost-effective Event Driven Scale: serverless function triggered by or trigger other events to compose complex, scaling applications | Abstraction of Servers: code described as functions, running on different compute instances | Micro-Billing: bill in micro-seconds of compute time | . | Azure Functions: run small amounts of code, serverless functions | Blog Storage: serverless object storage | Logic Apps: build serverless workflows, state machines for serverless compute | Event Grid: use Pub/Sub messaging system to react and trigger events using Azure Cloud | . | . Management Tools . Azure Portal: web-based, unified console to access Azure Cloud services | Azure Preview Portal: portal to utilize new features that are not fully released | Azure Powershell: manage Azure resources directly from Powershell CLI Powershell: command-line shell and scripting language for task automation and configuration management framework, built on .NET Common Language Runtime (CLR) | . | Visual Studio Code: source-code editor that can be run on Azure Cloud | Azure Cloud Shell: interactive, authenticated, browser-accessible shell for managing Azure resources using PowerShell or Bash | Azure CLI: type az followed by other commands to manage Azure resources Command Line Interface (CLI): process commands to computer program using lines of text, implemented using a shell or terminal | . | . Security . Azure Trust Center: website portal to provide information on privacy, security, and regulatory compliance on Azure | Compliance Programs: enterprise companies may specify compliance programs required for applications built on Azure | Azure Active Directory (Azure AD): identity and access management service, helps sign-in and access internal and external resources, Single-Sign On (SSO) | Multi-Factor Authentication (MFA): security control that uses a second device to confirm user logins, protect against stolen passwords | Azure Security Center: unified infrastructure security management system to evaluate security of data centers | Key Vault: safeguard cryptographic keys by cloud apps and services Secrets Management: control access to tokens, passwords, certificates, API keys, and other secrets | Key Management: control encryption keys | Certificate Management: manage public and private SSL certificates | Hardware Security Module (HSM): piece of hardware designed to store encryption keys, data stored in memory not disk and deleted upon device failure Multi-tenant (multiple customers virtually isolated on single HSM) HSMs are FIPS 140-2 Compliant | Single-tenant (single customer on dedicated HSM) HSMs are FIPS 140-3 Compliant | . | Azure DDoS Protection: free basic protection and paid standard protection ($3k/month) with metrics, alerts, reporting, support, and SLAs Distributed Denial of Service (DDoS) Attack: malicious attack by flooding website with large amounts of fake traffic | . | Azure Firewall: managed network security service to protect Azure virtual network resources create, enforce, and log application and network connectivity policies across subscriptions and virtual networks | static public IP address for virtual network resources to allow outside firewalls to identify traffic originating from your virtual network | high availability, no additional load balancers are required | configure deployment to span multiple AZs for increased availability | no additional cost for firewall in Availability Zone (AZ) | additional costs for inbound and outbound data transfers in AZs | . | Azure Information Protection (AIP): protect sensitive information with encryption restricted access and rights, and integrated security in Office apps | Azure Application Gateway: web-traffic load balancer (OSI Layer 7 HTTP) to reroute traffic based on set of rules Web Application Firewall (WAF) can be attached for additional protection on OSI Layer 7 | . | Azure Advanced Threat Protection (ATP): cloud-based security that leverages on-premises Active Directory to identify, detect, and investigate advanced threats, compromised identities, and malicious insider actions Intrusion Detection System (IDS)/Intrusion Protection System (IPS): device or application that monitors network or systems for malicious activity or policy violations | . | Microsoft Security Development Lifecycle (SDL): software security assurance process Training, Requirements, Design, Implementation, Verification, Release, Response | . | Azure Policy: create, assign, and manage policies to enforce or control properties of a resource evaluates resources by comparing resource properties to Policy Definitions, or business rules in JSON | . | Azure Role-Based Access Control (RBAC): manage access to Azure resources and permissions using role assignments Role Assignments: security principal, role definition, scope | Security Principal: access identities User: individual profile in Azure Active Directory | Group: set of users in Azure Active Directory | Service Principal: security identity used by applications or services to access specific Azure resources | Managed Identity: identity automatically managed by Azure | . | Role Definition: collection of permissions (e.g. read, write, delete), can be high-level or specific Use built-in roles (Owner, Contributor, Reader, User Access Administrator) or custom roles | . | Scope: resources available to be accessed by role, controls resource access at Management, Subscription, or Resource Group level | . | Lock Resources: prevent other users from accidentally deleting or modifying critical resources of a subscription, resource group, or resource CanNotDelete: users can read and modify but not delete resource | ReadOnly: users can read but not modify or delete resource | . | Management Groups: manage multiple subscriptions/accounts in hierarchical structure Each group has top level root and subscriptions inherit conditions of their management group | . | Azure Monitor: collect, analyze, and act on telemetry from cloud and on-premises environments | Azure Service Health: information about current and upcoming issues, e.g. service impacting events, planned maintenance, other availability changes Azure Status: service outage information in Azure | Azure Service Health: personalized view of Azure service and region health | Azure Resource Health: health of individual cloud resources | . | Azure Advisor: personalized cloud consultant for best practices on Azure provides recommendations in High Availability, Security, Performance, Cost, Operational Excellence | . | . | . Billing and Pricing . Service Level Agreement (SLA): Azure’s commitments for uptime and connectivity individualized per Azure service | performance targets in uptime and connectivity are in percentages 99% (two nines), 99.9% (three nines), …, 99.9999999% (nine nines) | . | not available for Free Tier or shared tiers | . | Service Credits: discount applied as compensation for under-performing Azure product or service based on SLA | Composite SLA: combined SLAs across different service offerings, improved with fallback systems Web App + SQL Database = 99.95% * 99.99% = 99.94% composite | Web App + (SQL Database or Queue Fallback) = 99.95% * (99.99999%) = 99.95% composite | . | TCO Calculator: estimate cost savings of migrating to Azure | Azure Marketplace: apps and services from third-party publishers categorized as Free, Free-Trial, Pay-As-You-Go, or Bring-Your-Own-License (BYOL) | Azure Support Basic: email support for billing and account, Azure Advisor, Health Status, Community Support, Documentation | Developer: Basic support, email tech support, third-party software support, minimal business impact response time &lt; 8 hrs, Architecture General Guidance | Standard: Developer support, moderate business impact response time &lt; 4 hrs, critical business impact response time &lt; 1 hr | Professional Direct: Standard support, minimal business impact response time &lt; 4 hrs, moderate business impact response time &lt; 2 hrs, Architecture, Operational Support, Proactive Guidance from ProDirect delivery managers, Webinars by Azure engineers | Enterprise: Professional Direct support, more things | . | Azure Licensing Azure Hybrid Use Benefit (HUB): Repurpose investment of Window Server licenses to use Azure virtual machines (Windows Servers, SQL Servers), Bring your own license (BYOL) | . | Azure Subscriptions Student: no credit card required, $100 USD credits for 12 months | Free: credit card required, $200 USD credits for 30 days, certain Azure products free for 12 months | Pay-as-you-go (PAYG): credit card required, charged monthly based on resource consumption | Enterprise Agreement: receive discounted price for licenses and cloud services | . | Azure Pricing Calculator: configure and estimate costs for Azure products | Azure Cost Management perform cost-analysis, visualize spending on Azure cloud services | create budgets, set budget threshold and alerts | . | . References .",
            "url": "https://nhtsai.github.io/notes/azure-fundamentals",
            "relUrl": "/azure-fundamentals",
            "date": " • Sep 19, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Building a Web Server",
            "content": "Web Servers . Introduction . A webserver is a networking server that waits for a client to send a request and generates a response to send back to the client. | The communication between a client and server uses HTTP protocol. | The client can be a browser or any other software that uses HTTP. | HTTP is just text that follows a certain pattern. Specify which resource and its headers | Separate head and body with blank space | Specify body message | . | A socket is an abstraction that allows you to send and receive bytes through a network. . | Example code in python # Python3.7+ import socket # define socket host and port HOST, PORT = &#39;&#39;, 8888 # Create socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((HOST, PORT)) server_socket.listen(1) print(f&#39;Serving HTTP on port {PORT} ...&#39;) while True: # Wait for client connections client_connection, client_address = server_socket.accept() # Get client request request_data = client_connection.recv(1024) print(request_data.decode(&#39;utf-8&#39;)) # Send HTTP response http_response = b&quot;HTTP/1.1 200 OK n nHello, World!&quot; client_connection.sendall(http_response) client_connection.close() server_socket.close() . We first define server socket’s host and port. | Then we create a server socket and set it to AF_INET for IPv4 address family and SOCK_STREAM for TCP. | This webserver creates a socket and binds it to port 8888 of any available host (localhost) to listen for connections. | The server socket accepts a connection and gets a client socket to receive information and the address of the client socket on the other side of the connection. | The server receives 1,024 bytes of data from the client socket. | The server then sends a message in bytes to the client socket. | Finally, the client closes the client socket connection. | . | URL http://localhost:8888/hello http designates HTTP protocol | localhost is the host name | 8888 is the port number | /hello is the path, or page to connect | . | Before the client can send HTTP requests, a TCP connection with the web server must be established. | Then the client sends an HTTP request and receives a response from the web server. . | HTTP Request GET /hello HTTP/1.1 GET is the HTTP method | /hello is the path, or page to connect | HTTP/1.1 is the protocol version | . | HTTP Response . HTTP/1.1 200 OK Hello, World! . HTTP/1.1 is the protocol version | 200 OK is the HTTP status code | Hello, World! is the HTTP response body | . | By default the browser requests the root of a server, e.g. GET / HTTP/1.0, but we should return the index.html page instead. . while True: # Wait for client connections client_connection, client_address = server_socket.accept() # Get client request request_data = client_connection.recv(1024) print(request_data.decode(&#39;utf-8&#39;)) # Get the contents of htdocs/index.html fin = open(&#39;htdocs/index.html&#39;) content = fin.read() fin.close() # Send HTTP response with content from index.html # Encode entire message to bytes http_response = &quot;HTTP/1.1 200 OK n n&quot; + content client_connection.sendall(http_response.encode()) client_connection.close() . | . References . Let’s Build a Web Server | Python Webserver | .",
            "url": "https://nhtsai.github.io/notes/web-server",
            "relUrl": "/web-server",
            "date": " • Sep 13, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Introduction to Database Systems",
            "content": "CMU 15-445: Introduction to Database Systems, Fall 2019 . Course Resources . Professor Andy Pavlo | Course Lectures | Course Website | Course Syllabus | Course Textbook: Database System Concepts, 6 or 7 ed. by Silberschatz, Korth &amp; Sudarshan | Course focuses on design and implementation of disk-oriented database management systems. | Topics: Relational Databases, Storage, Execution, Concurrency Control, Recovery, Distributed Databases, Potpourri | . 1. Relational Model &amp; Relational Algebra . Lecture Summary . | Database: organized collection of inter-related data that models some aspect of the real-world. Databases are the core component of most computer applications. | E.g. a digital music store database that keeps track of artists and albums needs to store artist biographies and albums those artists released. | . | Flat File: simplest database stored as CSV files that we manage in our own code. Use a separate file per entity, e.g. one file for artists and one file for albums | The application parses each file to read/update records. | E.g. Artist(name, year, country) and Album(name, artist, year) | If we wanted to get the year Ice Cube went solo, we could iterate through every line in the artists file and get year of row where name is Ice Cube. | . | Issues with Flat Files Data Integrity How do we ensure that the artist is same for each album entry, i.e. no typos or artist changes name? | How do we ensure the album year is a valid number and not an invalid string? | How do we store an album with multiple artists, i.e. check if parsed artist is single or multiple? | . | Implementation How do you find a particular record? | What if we want to use a new application with the same database, i.e. connecting logic between two languages? | What if two threads try to write to the same file at the same time, e.g. overwriting data? | . | Durability What if the machine crashes during updating a record? | What if we want to replicate database on multiple databases for high availability? | . | . | Database Management System (DBMS): software that allows applications to store and analyze information in a database Allows definition, creation, querying, update, and administration of databases that can be used with multiple applications. | Handles all the complex underlying logic for applications. | Focus on databases on disk, but there are other databases, e.g. in-memory. | . | Early DBMSs Issues Database applications were difficult to build and maintain | Tight coupling between logical and physical layers | Need to know what queries applications needed before deploying the database | . | Early 50s, mathematician Ted Codd (Edgar F. Codd) of IBM noticed people spent lots of time rewriting database applications, i.e. adjusting APIs depending on storing data as tree or hash table. Ted proposed the relational data model in A Relational Model of Data for Large Shared Data Banks in 1970. | . | The relational data model database abstraction to avoid maintenance. Store database in simple data structures, i.e. as relations aka tables. | Access data through high-level language, i.e. write code describing desired result aka declarative code instead of procedural code. Software has to create query plan to get desired result, similar to compilers creating machine code from high-level languages. | . | Physical storage left up to implementation, e.g.data on disk or in-memory, storing relations as trees or hash tables, etc. | . | . | Data Model: collection of concepts for describing the data in a database. Schema: description of a particular collection of data, using a given data model. | Relational data model is most common. The “best data model” because “9 times out of 10 that’s probably what you need.” | Can be used to model anything! | . | NoSQL Systems Key/Value data model | Graph data model | Document data model | Column-Family data model | . | Array/Matrix data model used in machine learning | . | Hierarchical or Network data models are obsolete/rare. | . | Relational Model Structure: definition of relations and their contents | Integrity: ensure database’s contents satisfy constraints | Manipulation: how to access and modify a database’s contents | Relation (Table): unordered set that contains relationship of attributes that represent entities N-ary relation: a table with N columns | . | Tuple (Record): set of attribute values (aka its domain) for an instance of an entity in the relation Values are normally atomic/scalar (no arrays or strings), originally. Now, anything can be stored. | The special value NULL, aka value is unknown, is a member of every domain. How NULL values are stored is implementation-specific. | . | . | Primary Key: attribute or set of attributes that can uniquely identify a single tuple, usually as an id field. Some DBMSs automatically create an internal primary key if not defined, e.g. SEQUENCE in SQL 2003 or AUTO_INCREMENT in MySQL. | . | Foreign Key: specifies that an attribute from one relation has to map to a tuple in another relation, usually as an id of a tuple in another relation. Can have another table with foreign keys relating Artist IDs to Album IDs to store albums with many artists. | DBMSs can also check IDs exists when inserting as foreign keys. | . | . | Data Manipulation Languages (DML): how to store and retrieve information from a database Procedural: the query specifies the high-level strategy the DBMS should use to find the desired result. E.g. relational algebra | . | Non-Procedural/Declarative: the query specifies only what data is wanted and not how to find it, so the DBMS needs to come up with a strategy. E.g. relational calculus | . | . | . . Relational Algebra: procedural language proposed by Ted Codd for querying relational data models. Fundamental operations, based on set algebra, to retrieve and manipulate tuples in a relation. | Each operator takes one or more relations as its inputs and outputs a new relation . | Select $( sigma)$ : choose a subset of tuples from a relation that satisfies a selection predicate. Predicate is a filter to only select tuples that satisfy its requirement. | Can combine multiple predicates using conjunctions or disjunctions | Syntax: $ sigma _{ text{predicate}}(R)$ | Example: $ sigma _{ text{a textunderscore id=’a2’}}(R)$ | Select corresponds to WHERE in SQL. | . | Projection $( Pi)$ : generate a relation with tuples that contains only the specified attributes. Can rearrange attributes’ ordering | Can manipulate the values | Syntax: $ Pi _{ text{attributes}}(R)$ | Example: $ Pi _{ text{b textunderscore id - 100, a textunderscore id}}( sigma _{ text{a textunderscore id=’a2’}}(R))$ | Projection corresponds to SELECT in SQL. | . | Union $( cup)$ : generate a relation that contains all tuples that appear in either only one or both input relations. Relations must have same attributes with same types. | Syntax: $R cup S$ | Example: $R cup S$ | Union corresponds to UNION ALL in SQL. | . | Intersection $( cap)$ : generate a relation that contains only the tuples that appear in both of the input relations. Relations must have same attributes with same types. | Syntax: $R cap S$ | Example: $R cap S$ | Intersection corresponds to INTERSECT in SQL. | . | Difference $(-)$ : generate a relation that contains only the tuples that appear in first and not the second of input relations. Relations must have same attributes with same types. | Syntax: $R - S$ | Example: $R - S$ | Difference corresponds to EXCEPT in SQL. | . | Product $( times)$ : generate a relation that contains all possible combinations of tuples from the input relations. Syntax: $R times S$ | Example: $R times S$ | Product corresponds to CROSS JOIN in SQL. | . | Join $( bowtie)$ : generate a relation that contains all tuples that are a combination of two tuples (one from each input relation) with a common value(s) for one or more attributes. Syntax: $R bowtie S$ | Example: $R bowtie S$ | Join corresponds to NATURAL JOIN in SQL. | . | Other operators developed over the years. Rename $( rho)$ | Assignment $(R leftarrow S)$ | Duplicate elimination $( delta)$ | Aggregation $( gamma)$ | Sorting $( tau)$ | Division $(R div S)$ | . | Relational algebra defines the high-level steps of how to compute a query, instead of only stating the desired result. E.g. $ sigma _{ text{b textunderscore id=102}}(R bowtie S)$ vs. $(R bowtie ( sigma _{ text{b textunderscore id=102}}(S))$ | The first query natural joins R and S, then performs a filter after. | The second query filters S, then natural joins the output relation to R. | The queries look similar, but the efficiency could differ greatly if S and R have a billion tuples and only one tuple in S has b_id=102. The first query joins two huge relations, but the second one can find the one tuple where b_id=102 before joining. | . | A better approach is to state the high-level answer that you want the DBMS to compute. E.g. Retrieve the joined tuples from R and S where b_id equals 102. | If relations change in sizes, there no need to change the application code manually. | . | . | Queries The relational model is independent of any query language implementation. | SQL is the de facto standard. | The DBMS will take the high-level query and generate the underlying execution plan to produce the desired result. | The DBMS will adapt and improve itself without us having to change the application. | . | Conclusion Databases are ubiquitous, important, and everywhere. | Relational algebra defines the primitives for processing queries on a relational database. Will come up again in query optimization and query execution. | . | IMPORTANT: The original 9 of the 36 Chambers are the RZA, the GZA, Inspectah Deck, Ghostface Killah, Masta Killa, U-God, Method Man, Ol’ Dirty Bastard, and Raekwon. Cappadonna was an original member of the clan but was in jail at the time, thus could not be on the 36 Chambers. | . | . 2. Advanced SQL . Lecture Summary . | SQL History Originally “SEQUEL” from IBM’s System R prototype “Structured English Query Language” | Adopted by Oracle in the 1970s | Not defined by Ted Codd, the creator of relational algebra, who later on developed language Alpha | . | IBM released DB2 in 1983 IBM’s first commercial relational database using SQL | . | ANSI Standard in 1986, ISO in 1987 “Structured Query Language” | . | Current SQL standard is SQL:2016 (added JSON, polymorphic tables) Every new specification/standard adds new features, which are pushed by major bodies that develop own proprietary features | . | Most DBMSs at least support SQL-92 standard No one follows an SQL standard exactly | . | . | Relational Languages The goal is for the user to specify the desired answer at a high level, not how to compute it. | The DBMS is responsible for efficient evaluation of the query Query Optimizer: reorders operations and generates query plan | . | SQL is a collection of languages Data Manipulation Language (DML): commands that manipulate data | Data Definition Language (DDL): methods of creating tables and schemas | Data Control Language (DCL): security authorizations to control data access | View Definition | Integrity &amp; Referential Constraints | Transactions | . | IMPORTANT: SQL is based on bag algebra and relational algebra is based on set algebra List: a collection of elements with a defined order, can have duplicates | Bag: a collection of elements with no defined order, can have duplicates | Set: a collection of elements with no defined order, cannot have duplicates | If we want to define order or ensure no duplicates, the DBMS needs to do extra work to provide this. The database will ignore these requests, making queries more efficient. | . | . | . | . . Example Database Student(sid, name, login, gpa) | Enrolled(sid, cid, grade) | Course(cid, name) | . | Aggregates Aggregate function: functions that return a single value by applying a function on a bag of tuples. AVG(col): Return average col value | MIN(col): Return minimum col value | MAX(col): Return maximum col value | SUM(col): Return sum of values in col | COUNT(col): Return number of values for col | . | Aggregate functions can only be used in SELECT output clause. Example: Count number of students that have an “@cs” login. | . -- login field doesn&#39;t matter for COUNT SELECT COUNT(login) AS cnt FROM Student WHERE login LIKE &#39;%@cs%&#39; -- can just count every row using * SELECT COUNT(*) AS cnt FROM Student WHERE login LIKE &#39;%@cs%&#39; -- or count 1 for every row SELECT COUNT(1) AS cnt FROM Student WHERE login LIKE &#39;%@cs%&#39; . | Multiple aggregates Example: Get number of students that have an “@cs” login and their average GPA. | . SELECT COUNT(sid), AVG(gpa) FROM Student WHERE login LIKE &#39;%@cs%&#39; . | Distinct aggregates Example: Count number of unique students that have an “@cs” login. | . SELECT COUNT(DISTINCT login) FROM Student WHERE login &#39;%@cs%&#39; . | GROUP BY: project tuples into subsets and calculate aggregates against each subset To aggregate a columns and return a non-aggregated column, we need to use GROUP BY. | Non-aggregated columns in the SELECT output clause must be in GROUP BY clause. | Example: Get average GPA of students enrolled in each course. | . SELECT AVG(s.gpa) AS avg_gpa, e.cid FROM Enrolled AS e, Student AS s WHERE e.sid = s.sid GROUP BY e.cid . | HAVING: filters results based on aggregation computation. We cannot filter aggregations in the WHERE clause because the aggregation is not computed yet. | To filter aggregations, we must use HAVING, which is like a WHERE clause for GROUP BY. | Example: Get average GPA of students enrolled in each course with &gt; 3.9 GPA. | . SELECT AVG(s.gpa) AS avg_gpa, e.cid FROM Enrolled AS e, Student AS s WHERE e.sid = s.sid -- cannot use: WHERE e.sid = s.sid AND avg_gpa &gt; 3.9 GROUP BY e.cid HAVING avg_gpa &gt; 3.9 . Knowing the HAVING clause can allow DBMS to optimize query. In a declarative language, we can optimize because we know the desired result ahead of time, compared to a procedural language, we don’t. | Example: Get courses with 10 students or less | . SELECT COUNT(s.sid) AS cnt, e.cid FROM Enrolled AS e, Student AS s WHERE e.sid = s.sid GROUP BY e.cid HAVING cnt &lt;= 10 . While counting the number of students for a course, if more than 10 students, DBMS can stop counting early for that course. | . | . | . | . . Strings Standard says all string fields are case-sensitive and declared with only single quotes. | Exceptions MySQL is case-insensitive and can use double quotes Standard needs to match case to match: WHERE UPPER(name) = UPPER(&#39;KaNyE&#39;) | MySQL is case-insensitive and allows double quotes: WHERE name = &quot;KaNyE&quot; | . | SQLite is case-sensitive and can use double quotes | . | LIKE: used for string matching % matches any substring, including empty strings | _ matches any one character | . WHERE e.cid LIKE &#39;15-%&#39; WHERE s.login LIKE `%@c_` . | String functions Defined in standard, can be used in either output (SELECT) or predicates (WHERE, HAVING) | . -- output SELECT SUBSTRING(name, 0, 5) FROM Student -- predicate SELECT * FROM Student WHERE UPPER(s.name) LIKE &#39;CHRIS%&#39; . | String concatenation Standard says to use || operator to concatenate 2+ strings together | Exceptions: MSSQL use +, MySQL use CONCAT() | . | . | . . Date/Time Operations Manipulate and modify DATE/TIME attributes, can be used in either output or predicates | Syntax and support varies wildly | Example: Get current time | . -- PostgreSQL (SQL standard) SELECT NOW(); SELECT CURRENT_TIMESTAMP; -- MySQL SELECT NOW(); SELECT CURRENT_TIMESTAMP(); SELECT CURRENT_TIMESTAMP; -- SQLite SELECT CURRENT_TIMESTAMP; . Example: Extract day from a date and getting number of days since beginning of year | . -- PostgreSQL (SQL standard) SELECT EXTRACT(DAY FROM DATE(&#39;2021-09-21&#39;)); SELECT DATE(&#39;2021-09-21&#39;) - DATE(&#39;2021-01-01&#39;) AS days; -- MySQL SELECT EXTRACT(DAY FROM DATE(&#39;2021-09-21&#39;)); SELECT DATEDIFF(DATE(&#39;2021-09-21&#39;), DATE(&#39;2021-01-01&#39;)) AS days; -- SQLite -- cannot extract, cannot subtract dates, no datediff function -- method: cast date strings into julian days, subtract and cast to int -- Why is SQLite the most widely-used database? It&#39;s free and public domain (no copyright). . Simple things are difficult to do. | . | . . Output Redirection: store query results in another table What if we want to use query results in subsequent queries? | We can redirect query results into a new table. Table must not already be defined | Table will have same number of columns and same data types as input | . | . -- SQL standard SELECT DISTINCT cid INTO CourseIDs FROM Enrolled; -- MySQL CREATE TABLE CourseIds ( SELECT DISTINCT cid FROM Enrolled ); . We can redirect query results into an existing table. Inner SELECT must generate same number of columns and data types as target table | Syntax and options vary on dealing with inserting duplicate tuples Target table has primary key constraint must be unique and will handle insertion of duplicate primary key values differently. | . | . | . -- assuming CourseIDs table already exists INSERT INTO CourseIDs ( SELECT DISTINCT cid FROM Enrolled ); . | . . Output Control ORDER BY: order output tuples by values in one or more of their columns Use ORDER BY &lt;col&gt; [ASC|DESC], default is ascending order | Can also use any complex expression, e.g. ORDER BY 1+1 | . | Unlike GROUP BY, columns in ORDER BY clause don’t need to be in SELECT clause | Example: Get IDs of students enrolled in 15-445, sorted by descending grade then ascending student ID. | . SELECT sid FROM Enrolled WHERE cid = &#39;15-445&#39; ORDER BY grade DESC, sid ASC . LIMIT: limit the number of tuples returned in output, can offset to return a range Use LIMIT &lt;count&gt; [OFFSET &lt;count&gt;] | Use ORDER BY to make results consistent | . | Example: Get 20 student ID and names, skip first 10. | . SELECT sid, name FROM Student LIMIT 20 OFFSET 10 . | . . Nested Queries: queries containing other queries, difficult to optimize Inner queries can appear (almost) anywhere in a query | Example: Get names of all students enrolled in 15-445. Naive execution: nested loops, for each student in Student, loop over sids in Enrolled | Better execution: produce set of enrolled student IDs once, loop over sids in Student to check if in enrolled set | Optimized execution: inner join tables on s.sid = e.sid | . | . -- nested query using IN SELECT name FROM Student WHERE sid IN ( SELECT sid FROM Enrolled WHERE cid = &#39;15-445&#39; ); -- join SELECT s.name FROM Student AS s, Enrolled AS e WHERE s.sid = e.sid AND e.cid = &#39;15-445&#39; . To construct a nested query, think about what is the answer you want to produce from the outer query. | Then figure out what you need from the inner query. . | ALL: must satisfy expression for all rows in sub-query | ANY: must satisfy expression for at least one row in sub-query | IN: equivalent to =ANY() | EXISTS: at least one row is returned | . -- nested query using ANY SELECT name FROM Student WHERE sid = ANY( SELECT sid FROM Enrolled WHERE cid = &#39;15-445&#39; ); . Can use subqueries in SELECT clause for every single tuple in enrolled table where cid = 15-445, do a match up in Student table where student IDs are the same | essentially doing a join inside SELECT clause, reversing order of evaluating tables | this reversal is important for optimization because it might be faster to go through Enrolled table first | . | . -- nested query in output SELECT (SELECT s.name FROM Student AS s WHERE s.id = e.sid) AS sname FROM Enrolled AS e WHERE e.cid = &#39;15-445&#39; . Example: find student record with highest ID that is enrolled in at least one course | . SELECT sid, name FROM Student WHERE sid &gt;= ALL( SELECT sid FROM Enrolled ) SELECT sid, name FROM Student WHERE sid IN ( SELECT MAX(sid) FROM Enrolled ) SELECT sid, name FROM Student WHERE sid IN ( SELECT sid FROM Enrolled ORDER BY sid DESC LIMIT 1 ) . Example: find all courses with no enrolled students Inner query can reference outer query, but outer cannot reference inner. | . | . SELECT * FROM Course WHERE NOT EXISTS ( SELECT * FROM Enrolled WHERE Course.cid = Enrolled.cid ) . | . . Window Functions Performs a calculation across a set of tuple that relate to a single row | Like an aggregation, but tuples are not grouped into single output tuples | Basic Syntax FUNC-NAME: aggregation functions, special functions | OVER: how to ‘slice’ up data, can also sort | . | . SELECT ... FUNC-NAME(...) OVER (...) FROM tableName . Special window functions ROW_NUMBER(): number of current row | RANK(): sort order position of current row, used with ORDER BY | . | . SELECT *, ROW_NUMBER() OVER () AS row_num FROM Enrolled . OVER: specifies how to group together tuples when computing the window function | Use PARTITION BY to specify group, like GROUP BY clause | Without PARTITION BY, window group is the whole table | . SELECT cid, sid, ROW_NUMBER() OVER (PARTITION BY cid) FROM Enrolled ORDER BY cid . Use ORDER BY to sort entries in each group | . SELECT cid, sid, ROW_NUMBER() OVER (ORDER BY cid) FROM Enrolled ORDER BY cid . Example: find student with highest grade for each course Window groups tuples by course ID and orders each group by letter grade | . | . SELECT * FROM ( SELECT *, RANK() OVER (PARTITION BY cid ORDER BY grade ASC) AS rank FROM Enrolled ) AS ranking WHERE ranking.rank = 1 . | . . Common Table Expressions (CTE) Provides a way to write auxiliary statements for use in a larger query | Like a temp table just for one query | Alternative to nested queries and views | Basic Syntax | . WITH cteName AS ( SELECT 1 ) SELECT * FROM cteName . Can bind output columns to names before the AS keyword two named columns, return 1 row with 2 values (1, 2) | . | . WITH cteName (col1, col2) AS ( SELECT 1, 2 ) SELECT col1 + col2 FROM cteName . Example: find student with highest id that is enrolled in at least one course Query references the CTE cteSource | . | . WITH cteSource (maxId) AS ( SELECT MAX(sid) FROM Enrolled ) SELECT name FROM Student, cteSource WHERE Student.sid = cteSource.maxId . Difference between CTE and nested queries: CTEs can use recursion! . | Example: print sequence of numbers from 1 to 10 . | . WITH RECURSIVE cteSource (counter) AS ( (SELECT 1) -- base case UNION ALL -- recursive call: (SELECT counter + 1 -- get current value + 1 FROM cteSource WHERE counter &lt; 10) -- specified limit of recursion ) SELECT * FROM cteSource . | Conclusion SQL is not a dead language. | You should (almost) always strive to compute your answer as a single SQL statement. | . | . 3. Database Storage I . Lecture Summary . | Disk-Oriented Architecture DBMS assumes primary storage location of the database is on non-volatile disk | DBMS manages movement of data between volatile and non-volatile storage | . | Storage Hierarchy CPU Registers &gt; CPU Caches &gt; DRAM &gt; SSD &gt; HDD &gt; Network Storage (e.g. AWS EBS/S3) | Ordered from faster, smaller, expensive to slower, larger, cheaper | Volatile not persistent without power, needs power to maintain memory | random access: can quickly jump around randomly in memory, same performance in any order of jumping | byte-addressable: can read exactly 64 bits at a location | volatile is DRAM and above storages, called memory | . | Non-Volatile persistent without power | sequential access: more efficient to read contiguous blocks of storage Think of a HDD like a record player, where random access requires expensive movement of needle to another location | . | block-addressable: need to get a whole 4KB block or page to read the desired 64 bits at a location | non-volatile is SSD and below storage, called disk | . | Exception: non-volatile memory (Intel Octane) Like DRAM in that sits in DIMM slot and is byte-addressable | Like SSD in that it is persistent without power | The future of computers! | . | Access Times | . Access Time Storage Comparison . 0.5 ns | L1 Cache Ref | 0.5 sec | . 7 ns | L2 Cache Ref | 7 sec | . 100 ns | DRAM | 100 sec | . 150,000 ns | SSD | 1.7 days | . 10,000,000 ns | HDD | 16.5 weeks | . ~30,000,000 ns | Network Storage | 11.4 months | . 1,000,000,000 ns | Tape Archives | 31.7 years | . | . . System Design Goals Allow DBMS to manage databases that exceed the amount of memory available | Reading/writing to disk is expensive, these calls must be managed carefully to avoid large stalls and performance degradation Use concurrent queries, caching, precomputing, etc. | . | . | Disk-Oriented DBMS At the lowest layer, we have a disk that holds database files Database file consists of a directory and pages, or blocks | . | In the next layer, we have the memory that holds the buffer pool, which manages movement of data between disk and memory | At the highest layer, we have the execution engine, which executes queries | Example: The execution engine needs Page 2, but it is not in memory We need to use the directory to locate Page 2 | Fetch Page 2 from disk | The buffer pool will bring Page 2 into memory | Then we return a pointer to Page 2 to the execution engine to interpret the layout of Page 2 | The buffer pool manager ensures the page is there while the execution engine is operating on that memory | . | . | Why not use the OS? Why does the DBMS need to ‘fake’ more memory than available and manage memory in this way when the OS can already do this? Similar to virtual memory, where there is a large address space (virtual) and a place for OS to bring in pages from disk (physical) | The OS is already responsible for moving the files’ pages in and out of memory | . | Under the hood, the OS uses memory mapping (mmap) to store contents of a file into a process’ address space mmap file: takes a file on disk and maps the files’ pages into the address space of our process | Now we can read and write to those memory locations | If the file is not in memory the OS, a page fault occurs | OS needs to bring files into memory before any operations are performed | . | Example We have a bunch of pages in a file on-disk, a 4-slot virtual memory page table, and 2-slot physical memory | Application wants to read Page 1 We look in virtual memory and see Page 1 is not backed by physical memory, get a page fault | Then we fetch Page 1 from disk and back it into physical memory slot | Then we update our virtual page table to point to Page 1 in physical memory | . | Application wants to read Page 3, and the same process occurs and Page 3 is fetched into physical memory | Application wants to read Page 2, but both physical memory slots are taken We need to decide which page (Page 1 or Page 3) to remove, load Page 2 into physical memory, and return a pointer in the virtual memory to the fetched data in physical memory | While this is happening, the thread for Page 2 is stalled while the disk scheduler for the OS goes to fetch Page 2 An optimized approach is to hand off the stalled request task to another thread to allow non-stalled tasks to continue executing without delay | . | . | . | The OS does not understand what the DBMS wants to do; it just sees a bunch of reads and writes to pages and does not understand the high-level semantics of queries and what data queries want to read By relying on the OS, virtual memory, and mmap, we don’t take advantage of our knowledge of the DBMS and give up control of memory management to the blind OS | . | What if we allow multiple threads to access the mmap files to hide page fault stalls? This works good enough for read-only access because stalled requests that are being fetched can be passed to other threads to keep executing non-stalled requests | It is complicated for multiple writing threads because the OS does not know that certain pages need to be flushed out to disk before other pages do The OS just writes data out to disk without knowing if it is okay or not | . | . | Some solutions to issues of using mmap madvise: tell OS how you expect to read certain pages (sequential or random access) | mlock: tell OS that memory ranges cannot be paged out | msync: tell OS to flush memory ranges out to disk | . | mmap files and relying on the OS for memory management sound good but will create performance bottlenecks and correctness issues None of the major databases use mmap completely | . | DBMS (almost) always wants to control things itself and can do a better job at it knows what queries want to do and the workload | flushing dirty pages to disk in correct order | specialized pre-fetching | buffer replacement policy | thread/process scheduling | OS is not your friend | . | . | . . Database Storage Problem 1: How the DBMS represents database in files on disk | Problem 2: How the DBMS manages its memory and moves data back-and-forth from disk | . | File Storage DBMS stores database as one or more files on disk E.g. SQLite stores databases in 1 file, most other systems store in multiple files due to file-size limitations | OS does not know anything about the contents of these files, but the formats of these files are typically specific to the DBMS | . | Early systems in 1980s used custom file systems on raw storage, rather than formatting the hard drive to a common file format like NTFS Some ‘enterprise’ DBMSs still support this, e.g. Oracle DB2 | Most newer DBMSs do not do this | . | . | Storage Manager: responsible for maintaining a database’s files on disk Some DBMS do own scheduling for reads/writes to improve spatial and temporal locality of pages, i.e. combining queries that closely-located blocks | A file itself is organized as collection of pages storage manager tracks data read/written to pages | storage manager tracks available space to store new data in the pages | . | Page: fixed-size block of data can contain anything: tuples, meta-data, indexes, log records, … | most systems do no mix page types, e.g. a page with only tuples and a page with only index information | some systems require a page to be self-contained, in which all the information needed to comprehend the contents of a page must be stored within the page itself | Example: a table with 10 columns of different types Consider page’s metadata about the table (schema, layout) was stored in one page and all the tuples of the table stored in another | An issue arises if the metadata page is lost in a system failure, then the tuples page is difficult to comprehend or interpret | By keeping metadata in each page, we trade overhead and storage for better disaster recovery | . | . | Each page is given a unique identifier, generated by the DBMS The DBMS uses an indirection layer to map page IDs to physical location in a file at some offset | We want to do this to move pages around (compacting disk, setting up another disk) and still know where pages are physically located because page IDs are unchanged | . | 3 notions of pages in DBMS Hardware Page (usually 4KB): the page access level from the storage device itself, what the HDD or SSD exposes The lowest level we can do atomic writes to the storage device, usually in 4KB writes | The level the device can guarantee a “failsafe write”, or a write and flush to the disk is atomic | Example: if we need to write 16KB and we write 8KB, crash, and write 8KB There is a “torn write” where we only see the first half but not the second half | The hardware can only guarantee 4KB writes at a time | . | . | OS Page (usually 4KB): the page access level when moving data from disk into memory | Database Page (512B - 4KB SQLite - 8KB PostgreSQL - 16KB MySQL): the page access level the DBMS operates on Why use more than 4KB pages? | Internally, we use a table to map pages to locations on disk. Using large sized pages, we can reduce that page mapping table size to represent more data with fewer page ID mappings | Tradeoff is handling 4KB writes very carefully | . | . | . | . . Page Storage Architecture Different DBMSs manage pages in files on disk differently, how pages are organized within files Heap File Organization | Sequential/Sorted File Organization | Hashing File Organization | . | At this lowest level of hierarchy, we don’t care what data is stored in the pages (tuples, indexes, etc.) | . | Database Heap Heap File: unordered collection of pages where tuples are stored in random order Relational model does not have any order, insertion order is not guaranteed | API functions: Create, Get, Write, Delete page | Must support iterating over all pages for scanning entire table | . | Metadata to keep track of what pages exist and which ones have free space for inserting new data | Representations of heap files Doubly-Linked List (naive) . . maintain a header page at beginning of file that stores two pointers HEAD of free page list, pages with available space | HEAD of data page list, pages completely full | . | each page keeps track of number of free slots in itself to insert new data, we look through pages in the free page list | . | to find a particular page, we iterate over the entire data page list | if the pages were sorted in a unified linked list, we would have faster page searching but need to iterate over all pages to find free space | . | Page Directory (optimal) . . maintain a directory page that tracks location to data pages in the database files, similar to a hash table | directory also records number of free slots per page | DBMS has to make sure that directory pages are in sync with the data pages the directory holds metadata about the pages themselves that need to be in sync, but this cannot be guaranteed | the hardware cannot guarantee (over 4KB writes) that we can write to two pages at the same time | Example: if we deleted a bunch of data in a page and freed up space, the page directory should update the number of free slots for that page | But if the system crashes before the page directory is updated, the DBMS may think the page is full when it is not | . | . | . | . | . . Page Layout Page Header: metadata about the page’s contents Page Size, Checksum (used to determine torn writes/crashes), DBMS Version, Transaction Visibility, Compression Information | Some systems require pages to be self-contained | . | How do we store data within a page (assuming only storing tuples)? Tuple-Oriented | Log-Structured | . | Tuple Storage How to store tuples in a page? | Simple Append: strawman idea Header keep tracks of the number of tuples in the page | Append new tuples to end (determined by offset calculated from number of tuples) | If we delete a tuple, there is a gap where the removed tuple was. | If tuples are fixed length, we can just insert a new one in the gap, but we need to sequentially scan for gaps | But if not fixed-length, then the gap may be too big or too small | . | Slotted Pages: most common layout scheme . . Header keeps track of number of used slots and the offset of the starting location of the last slot used | Slot array maps ‘slots’ to the tuples’ starting position offsets | Tuples are stored at the end of the page, and the offset of start of the tuple is written to a slot array Tuples can be fixed or variable length | The slot array grows from the beginning to end of page | The tuple storage grows from end to the beginning of page | Page is full when two section meet or gap too small to store anything | . | Now we can move tuples around the page without affecting upper levels Locating a record (record Id) is just page ID and slot number | To move a tuple, just update its slot number | To delete a tuple, some systems use compaction to remove gap or some systems can mark slot value as available or some systems just append and deal with the gap later | . | . | . | . | Tuple Layout Tuple: essentially a sequence of bytes | The DBMS needs to interpret the bytes into attribute types and values | Prefixed with header that contains metadata visibility info (concurrency control) | bitmap for NULL values | . | Do not need to store metadata about tuple in itself because it is stored in the page or another structure Need to store metadata in JSON/Schema-less databases like MongoDB because every single document can be different, so you need metadata in itself. | . | Attributes are typically stored in the order specified when the table is created Not necessary in relational model, but easier for software engineering | . | Denormalized Tuple Data Can physically denormalize (e.g. pre-join) related tuples and store them together in the same page This is when data from different tables are stored within the same page | Technically allowed, but now have to keep track of which tuple is from which table | . | Potentially reduces amount of I/O for common workload patterns | Can make updates more expensive | Normalization: how we split up data across different tables Naturally happens with foreign keys | Sometimes, we want embed tables inside another table to avoid joins | Packing tuples inside of a tuple value | . | Not a new idea IBM System R did this in 1970s, abandoned in DB2 | Several NoSQL DBMSs (CloudSpanner, MongoDB, etc.) do this without calling it physical denormalization | . | . | . | Conclusion Database is organized in pages. | Different ways to track pages. | Different ways to store pages. | Different ways to store tuples. | . | . 4. Database Storage II . Lecture Summary . | Log-Structured File Organization . . Instead of storing tuples in pages (slotted pages), DBMS only stores log records, or records of how the database was modified Insert, store entire tuple | Delete, mark tuple as deleted | Update, record delta of the just the modified attributes | . | Advantages Fast writes, sequential read/write is faster than random access on disk storage In slotted pages, if we want to update 10 tuples all in different pages, then we need make and write changes at 10 separate, slotted pages | In log records, if we want to update 10 tuples, we just sequentially append the log records to a single page and write it out to disk. | . | Rollback capability, can revert changes to tuples | . | Disadvantages Reading tuples: DBMS scans the log backwards and recreates the tuple to find what it needs Can build indexes to allow jumping to particular locations in the log that reference the tuple needed | Can periodically compact logs back into tuples | . | . | Seen in: Apache HBase, Cassandra, LevelDB, RocksDB | . | . . Data Representation Tuple: a sequence of bytes that the DBMS interprets into attribute types and values Tuple layout determined by table schema | . | For most DBMS, we represent data similar to C++ standards Integer/BigInt/SmallInt/TinyInt (C/C++/IEEE-754 standard) | Float/Real vs. Numeric/Decimal (IEEE-754 standard vs. fixed-point decimals) | Varchar/Varbinary/Text/Blob (header with length, followed by data bytes) | Time/Date/Timestamp (32/64-bit integer of (micro)seconds since Unix epoch) | . | . | Variable Precision Numbers Inexact, variable-precision numeric type that uses native C/C++ types | Stored directly as specified by IEEE-754 standard | E.g. float, real, double | Typically faster than (fixed point) arbitrary precision numbers because CPU has instructions to operate on these types, but can have rounding errors because no exact way to store decimals One CPU instruction to add floating point numbers | Already built in functionality of underlying language | . | Rounding Example Looks equal on the surface, but expanding precision shows inexactness of storing floating point numbers | . | . #include &lt;stdio.h&gt; int main(int argc, char* argv[]) { float x = 0.1; float y = 0.2; printf(&quot;x+y = %.20f n&quot;, x+y); // 0.3000...1192 printf(&quot;0.3 = %.20f n&quot;, 0.3); // 0.2999...8850 } . | Fixed Precision Numbers Numeric data types with arbitrary precision and scale | Used when round errors and inexactness are unacceptable | E.g. numeric, decimal | Typically stored in an exact, variable-length binary representation with additional meta-data (num digits, weight of first digit, scale factor, sign, digits) Like a varchar but not stored as a string | . | Operations with fixed precision numbers are slower because of the extra calculations for the exact rounding Large switch statement function to add fixed precision numbers with exact rounding | Need to implement this in DBMS ourselves | . | . | . . Storing Large Values Internally . . What happens when the value we want to store doesn’t fit in a single page? The page size is set when the DBMS is started, usually 4KB. | . | To internally store values larger than a page, use overflow storage pages. The tuple attribute’s value is now a pointer to an overflow page using a record ID (page number + offset) | If the value still doesn’t fit the overflow page, point to another overflow page | When we want to get the value, we chain the overflow pages to get all the data | . | Most DBMS do not allow a tuple to exceed the size of a single page, other DBMS have overflow pages Postgres: TOAST (overflow if value &gt; 2KB) | MySQL: Overflow (overflow if value &gt; 0.5 size of page) | SQL Server: Overflow (overflow if value &gt; size of page) | . | We want to use overflow pages because we get all the protections of regular data If we crash when writing to overflow pages, we want to recover in the same way | . | . | Storing Large Values Externally . . Some systems allow you to store really large value in an external file Treated as BLOB type, binary large object, variable length binary large data | Oracle: BFILE data type | Microsoft: FILESTREAM data type | . | To externally store values larger than a page, use a file pointer The tuple attribute’s value is now a pointer or filepath to some external storage device where the data can be found | . | DBMS cannot manipulate the contents of the external file, read only. No durability protections | No transaction protections | . | Commonly used for storing videos or images | “To BLOB or Not to BLOB” paper from Microsoft says any values &lt;= 256KB store as overflow page, else store in external file storage | SQLite inventor says to store thumbnail images up to 1MB internally is faster for phones and smaller devices because the internal file is already open, rather than following a filepath to open an external. | . | . . System Catalogs DBMS stores metadata about the databases in its internal catalogs Tables, columns, indexes, views | Users, permissions, security | Internal statistics (number of values, distribution of values) | Should not use external information that is outside of DBMS control | . | Almost every DBMS stores their catalog inside itself as another table Wrap object abstraction around tuples Some kind of code to access catalogs directly, otherwise we need to query a table to find its name (which we don’t know unless we query it!) | . | Specialized code for “bootstrapping” catalog tables | . | Can query the DBMS internal INFORMATION_SCHEMA catalog to get info about the database ANSI standard specifies INFORMATION_SCHEMA, a set of read-only views that provide info about all of the tables, views, columns, and procedures in a database | . | DBMS also have non-standard shortcuts to retrieve this information Essentially converts shortcut into ANSI standard | Example: List all tables in current database. | . -- ANSI/SQL-92 SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE table_catalog = &#39;&lt;db name&gt;&#39;; -- PostgreSQL d; -- MySQL SHOW TABLES; -- SQLITE .tables; . Example: Get schema for the student table. | . -- ANSI/SQL-92 SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE table_name = &#39;student&#39;; -- PostgreSQL d student; -- MySQL DESCRIBE student; -- SQLITE .schema student; . | Main Point: Internal catalog about database schema is used in querying and other operations Easiest way to implement type system is using a switch statement to handle each type for every tuple | More optimized way is to use just-in-time compilation on the fly | . | . | . . . Different Workloads The relational model does not specify that we have to store all of a tuple’s attributes together in a single page This may not actually be the best layout for some workloads | . | Example: Wikipedia is an OLTP workload application users(userID, userName) | pages(pageID, title, latest, revID) | revisions(revID, userID, pageID, content, updated) | . | . | On-line Transaction Processing (OLTP) Simple queries read/write a small amount of data that is related to a single entity in the database | Ingest new data from outside world and put into database system Only read a small amount of data, over and over again | . | Typical database built first for new applications, websites, etc. | Typical of Traditional NoSQL DBMS: MongoDB, Cassandra, Redis, BigTable Focused on ingesting new data, rather than analytics | . | Example: Amazon.com storefront Operations: adding stuff to cart, making purchases, updating account information | Not updating a lot of data, just your cart or account information | Queries just modify a small part of the database | . | Example: Wikipedia Operations: get current revision of a page, update user login time, add a revision to a page | . | . | On-line Analytical Processing (OLAP) Complex queries that read large portions of database spanning multiple entities Typically read-only, join queries on lots of data | . | Typical of Column-store DBMS or NewSQL DBMS or Big Data: Hadoop Focused on efficient analytics, fast transaction processing without giving up joins like NoSQL DBMS | . | When you already collected a lot of data from OLTP applications and want to execute workloads on that data to analyze and extrapolate new information | Example: Wikipedia Count the number of .gov user accounts that have logged in per month | . | . | Hyper Transaction Analytical Processing (HTAP) When you want to ingest new data and analyze it as it comes in | Commonly used in decision making on-the-fly Internet advertising companies adapting while users are browsing websites | . | . | . . N-ary Storage Model (NSM)/Row Storage Model . . The DBMS can store tuples in different ways depending on what is better for the OLTP or OLAP workloads So far, we have assumed n-ary storage model, or row storage, of tuples. | . | N-ary Storage Model (NSM): DBMS stores all attributes for a single tuple continuously in a page, row store | Ideal for OLTP workloads where queries tend to operate only on an individual entity Accessing small amounts of data relating to a single entity, e.g. user data, is better when the entity’s attributes are stored together (in a row) | . | Ideal for insert-heavy workloads Inserting small amounts of related data in a row is more efficient because writing a row of data into a free slot and flush out in one disk write | . | Example: Accessing all account information given username and password User data is stored as rows in a single tuple continuously in a page | Query uses username and password to lookup in an index | Index tells what page and slot number where the tuple can be found | Perform one-read, one-seek to bring the page into memory | Jump to location in page where tuple can be found and return result | . | Where is row-storage a bad idea? . . Example: Count the number of .gov user accounts that have logged in per month Need to sequentially scan whole user account table, looking for .gov hostnames | Want to first look at hostname attribute of all users, going through each row of each page | Want to group accounts by lastLogin month of all users | Because disk is block-addressable, we can’t just bring in the two columns we need; we need to bring the entire page to memory Need to bring into memory all user attributes (userID, username, password, hostname, lastLogin) despite only needing hostname and lastLogin, 3 columns unneeded | . | Row-storage model is inefficient when analyzing data from disk that doesn’t need all of the attributes | . | . | Advantages Fast inserts, updates, and deletes | Good for queries that need all of the tuple’s attributes | . | Disadvantages Bad for scanning large portions of the table | Bad for queries that only need a subset of the tuple’s attributes | . | . | . . Decomposition Storage Model (DSM)/Column Storage Model . . Decomposition Storage Model (DSM): DBMS stores values of a single attribute for all tuples contiguously in a page, column store | Ideal for OLAP workloads where read-only queries perform large scans over a subset of the table’s attributes Only need to load into memory the pages of the attribute(s) that are needed | . | Example: Count the number of .gov user accounts that have logged in per month . . User data is stored as separate pages of attributes, so we only need to bring the hostname pages and lastLogin pages into memory Instead of scanning all user pages just for two attributes | . | Go through the hostname pages to look for .gov hostnames | From the matching tuples, jump to relevant locations in the lastLogin pages | Process the data needed and return result | . | Because we are storing attributes of the same type together, we can now apply compression techniques Example: instead of storing all temperatures, store a base temperature value and a series of deltas | More efficient: with every page fetch we can get more compressed tuples compared to uncompressed tuples | Some systems can operate on compressed tuples without needing to un-compress | . | Tuple Identification . . After finding the tuples with .gov in hostname pages, how do we match to the tuples in lastLogin pages? | Most Common Choice: Fixed-Length Offsets Each value in an attribute/column is a fixed-length. E.g. every integer is 32-bits | . | To find the same tuple in two different pages, we take the index and multiply by the fixed-length size of the attribute to get the offset or page ID (record ID + offset) | . | Alternative Choice: Embedded Tuple IDs Each value is stored with its tuple ID in a column | To find the same tuple in two different pages, we just look up the same tuple ID | Large storage overhead, extra 32/64-bit int ID for every value | . | . | Advantages Reduces amount of wasted I/O, moving unneeded data into memory, because DBMS only reads the data that it needs | Better query processing Speed depends heavily on query plan, though theoretically better/faster than row storage | . | Better data compression because values of the same type are stored together | . | Disadvantages Slow for point queries, inserts, updates, and deletes because of tuple splitting/stitching Need to insert value in every attribute page separately | Need to read from multiple attribute pages to build tuple | . | . | History of DSM/Column Storage Model 1970s: Cantor DBMS from Swedish military internal project | 1980s: DSM Proposal | 1990s: SysbaseIQ (in-memory only, first commercial product, add-on) | 2000s: Vertica (from creator Postgres/Ingres), VectorWise, MonetDB | 2010s: Everyone (Amazon RedShift, MariaDB, Cloudera Impala, etc.) | . | . | . . Conclusion The storage manager is not entirely independent from the rest of the DBMS Underlying representation of data (row/column/etc.) is involved in other DBMS operations | . | It is important to choose the right storage model for the target workload Use row store for OLTP workloads | Use column store for OLAP workloads | Some systems offer both row st1ore and column store options | You can mix the two in certain cases: trimming old data from OLTP for speed and moving that into OLAP data warehouse for analytics | . | . | . 5. Buffer Pools &amp; Memory Management . Lecture Summary . | Database Workloads On-line Transaction Processing (OLTP) Fast operations that only read/update a small amount of data each time | . | On-line Analytical Processing (OLAP) Complex queries that read a lot of data to compute aggregates | . | Hyper Transaction &amp; Analytical Processing (HTAP) OLTP + OLAP together on the same database instance | . | . | Bifurcated Environment Typical Setup Multiple OLTP Data Silos in the frontend E.g. MySQL, PostGres, MongoDB | . | Large OLAP Data Warehouse in the backend E.g. Hadoop, Spark, GreenPlum, Vertica, RedShift, Snowflake | . | Extract-Transform-Load (ETL) process performed to clean and move data from front end to back end | Perform analytics in data warehouse | Push any new information to the OLTP frontend, e.g. customers also bought/viewed | . | HTAP Setup Multiple HTAP Data Silos in the frontend | Large OLAP Data Warehouse in the backend | ETL process performed to clean and move data from front end to back end | Instead of waiting on ETL process and analytics in the backend, perform some analytics in the front end | . | . | . . How does DBMS manage its memory and move data back and forth from disk? Due to von Neumann architecture, DBMS cannot operate directly on disk; we must bring data from disk into memory first | How can we bring pages into disk and support databases that exceeds the available memory? | How can we minimize the slowdown from movement of data, making it appear everything is in memory? | . | Database Storage Spatial Control Where to write pages on disk | Goal: keep pages used together often as physically close together as possible on disk Avoid long disk seeks | . | . | Temporal Control When to read pages into memory, and when to write them to disk | Goal: minimize the number of stalls from having to read data from disk into memory | . | Buffer Pool Manager: brings the directory into memory, looks up the page location, brings the page into memory, and decides when to evict pages to free memory or write pages out to disk | . | . . Buffer Pool Organization Buffer Pool: memory region organized as an array of fixed-size pages DBMS use the OS malloc to get a chunk of memory to use as the buffer pool | Frame: an array entry, the regions/chunks of a buffer pool | When the DBMS requests a page… Look if the page is already in buffer pool | Otherwise an exact copy of the page is placed from disk into a frame in memory | . | . | . | Buffer Pool Metadata . . Pages can go in any order in the frames of the buffer pool, so we need something to keep track of what page is in what frame | Page Table: keeps track of pages that are currently in the buffer pool memory, hash table that maps page IDs to frame IDs | DBMS also maintains additional meta-data per page Dirty Flag: single bit flag that indicates the page has been modified it was copied from disk DBMS also needs to keep track of who made the modification in a log | . | Pin/Reference Counter: keeps track of number of threads or queries that want the page to be kept in memory Avoids evicting pages to disk before operations are completed (pin) | Reserves frames in the page table while the corresponding page is being read into memory (lock) | . | . | . | Locks vs. Latches Locks Protects the database’s logical contents from other transactions | Higher level logical primitive, e.g. tuple, table, database | Held for transaction duration, could be multiple queries, seconds, or hours | Need to be able to rollback changes for concurrency control | Exposed to programmers, can see locks as you run queries | . | Latches Protects the critical sections of the DBMS’s internal data structure from other threads | Low level protection primitive, e.g. protecting data structures or regions of memory | Held for operation duration, e.g. when updating page table, apply latch on the entry (frame to page mapping) being modified before making the change | Do not need to be able to rollback changes Internal operations, abort if cannot apply latch or operation fails | . | In OS world, this is known as a mutex | . | . | Page Directory vs. Page Table Page Directory: mapping from page IDs to page locations in the database files All changes must be recorded on disk to allow DBMS to find on restart, must be durable If we crash and come back, we want to know where to find the pages we have | . | . | Page Table: mapping from page IDs to a copy of of the page in buffer pool frames In-memory data structure that does not need to be stored on disk If we crash and come back, buffer pool is erased too, so it doesn’t matter | . | . | . | Allocation Policies Global Policies Make decision for all active transactions of the entire workload | “At this point in time, what’s the right thing to do to decide what should or should not be in memory?” | . | Local Policies Allocate frames to a specific transaction without considering the behavior of concurrent transactions | “What’s the best thing to do to make my one transaction faster?” | Still need to support sharing pages | . | For optimizations, most systems implement a combination of both allocation policies | . | . . Buffer Pool Optimizations There are a variety of ways to tailor the buffer pool to make our specific workload more efficient, better management than the OS | . | Multiple Buffer Pools DBMS does not always have a single buffer pool for the entire system E.g. Enterprise DBMSs: MySQL, IBM DB2, Oracle, SyBase, MS SQL Server, Informix | . | DBMS can actually have multiple regions of allocated memory, each with page tables that map page IDs to frames in the buffer pool Multiple buffer pool instances | Per-database buffer pool | Per-page type buffer pool | . | Advantages Improves locality Use a local policy tailored to each buffer pool and the data it holds | . | Reduces latch contention for different threads accessing the buffer pool As a thread looks up a page ID in the page table, it latches the page table entry so the entry does not change while the thread is accessing the corresponding frame | Makes sure other threads do not swap the page table entry out while thread is fetching the frame of the page | . | Latch Contention: multiple threads contending on the same latch while accessing the same page table More buffer pools = more page tables = more threads accessing different page tables at the same time = less threads contending on the same latches = more scalability | . | . | Example: single buffer pool for each table, tables with sequential vs. random access workloads are more efficient with different caching/eviction policies | Example: separate buffer pool for indexes and tables, different access patterns and workloads need different buffer pool policies | Approach #1: Object ID Embed an object identifier in record IDs | Maintain a mapping from object IDs to specific buffer pools | Ensures that pages are fetched to the same buffer pool every time Keeps the page in the same buffer pool to quickly find it | Object ID determines which buffer pool maintains that specific record | . | . | Approach #2: Hashing Hash the page ID to select which buffer pool to access | Take the page ID, hash it, mod by number of buffer pools | . | . | Pre-Fetching DBMS can pre-fetch pages based on query plan to minimize stall time of fetching data from disk If a thread requests a page that’s not in memory, we stall that thread until the page is fetched from disk into the buffer pool, then we return the pointer to the page in memory | Internally the DBMS keeps track of a cursor in disk of the page that was fetched If the thread needs the next page, the cursor can tell us where to fetch the next page into memory | . | . | Sequential Scans If the query wants to scan the entire table as in sequential read, the DBMS can pre-fetch all pages of the table into the buffer pool, rather than wait to fetch page by page Pages already processed can be evicted and replaced with the next page | . | . | Index Scans The index allows you to jump to specific points in the data based on values | If the query wants values in a specific range, the index can tell us which pages pages have values within the range | Because the query wants specific, non-sequential index pages, the DBMS can pre-fetch the specific index pages into the buffer pool, rather than sequentially fetching unneeded pages | . | Advantage Pre-fetching using sequential read minimizes random I/O and reduces disk stalls | . | . | Scan Sharing Queries can reuse data retrieved from storage or operator computations Multiple queries can use the same data from disk | Different from result caching: returning cached answer instead of re-computing query | . | Allow multiple queries to attach to a single cursor that scans a table Queries do not have to be exactly the same | Can also share intermediate results across different threads in some cases, materialized view Intermediate results are stored in a separate memory region outside of buffer pool | . | . | If a query starts a scan and there’s an existing query doing the same scan, the DBMS will attach the new query to the existing query’s cursor DBMS keeps track of where new query joined with existing query so that it can finish the scan when it reaches the end of the data structure, go back to read previous data that was read before new query joined | . | Fully supported in IBM DB2 and MS SQL Server, Oracle only supports basic scan sharing (cursor sharing) for identical queries | Example First query wants to compute sum on table A | Pages are fetched, buffer pool fills up and replaces older frames (Page 0) with new page copies (Page 3) | Second query wants to compute average on table A It hops on to the same cursor as the first query, which is already at Page 3 | . | The cursor scans the rest of the disk pages into buffer pool frames that are needed for the first query | After the first query is done reading data, the cursors disappears | Second query then starts cursor from beginning at Page 0 Second query then reads until Page 3 to make up for missing pages as needed | . | . | The relational model is unordered; answers can be correct without being the same value every time Example: Query wants to compute average, limit 100 tuples Without a shared cursor, the query will read the first 100 tuples, starting from the first disk page | If the query hops on to a shared cursor, the first 100 tuples will be whatever is on the disk pages that the cursor is at | If the data needed is already in the buffer pool, the query can use the existing data in memory | . | All methods can produce a “correct” answer without reading the same data to compute the same value because data is stored unordered | . | . | Buffer Pool Bypass Sequential scan operator will not store fetched pages in the buffer pool in order to avoid overhead of looking at page table Sequential scan does not look at the page table to see if the page is already in memory | Sequential scan also does not want to pollute the cache with data that may not be needed in the near future | Sequential scan just keeps scanning sequential disk pages and evicting them once finished processing in memory | . | Buffer Pool Bypass or Buffer Cache Bypass Allocate a small amount of memory to the query | The query reads pages, fetching from disk if not already in the buffer pool | The pages are fetched into a local memory area, instead of the buffer pool Memory is local to running query and the thread running it | . | When the query is done, all of the local memory is freed at once | . | Advantages Avoids overhead of going to the page table, a hash table with latches | Works well if operator needs to read a large sequence of pages that are contiguous on disk | Can also be used for smaller temporary data, intermediate tables (sorting, joins) Larger temporary data needs to be backed by the buffer pool, written as pages out to disk if larger than memory | . | . | Most DBMS support this optimization, called light scans in Informix | . | . . OS Page Cache What is the OS actually doing at a lower level? | Most disk operations go through the OS API (fopen, fread, fwrite) | Unless specified otherwise, the OS maintains its own filesystem cache By default, the OS maintains the OS page cache | As the DBMS reads pages from disk, the OS also caches a copy of the page | . | Most DBMS use direct I/O (O_DIRECT POSIX flag) to bypass the OS’s cache Avoid redundant copies of pages | Different eviction policies between DBMS buffer pool and OS page cache | Can also help maintain similar performance across different OS’s | . | Postgres is the only major DBMS that relies on the OS page cache Trade avoid maintaining a separate caching system for a minor performance penalty | Main disadvantage is there could be two copies of every single page, and the OS page cache could be old if the DBMS modifies the page | . | If we give the DBMS enough memory, it can store the whole table in memory, which will make queries on that table faster since we don’t have to fetch from disk | . | . . Buffer Replacement Policies What happens if we need to fetch a page from disk but the buffer pool is full? | When the DBMS needs to free up a frame to make room for a new page, it must decide which page to evict from the buffer pool | Goals Correctness: we don’t want to write out data from latched frames before threads are done modifying | Accuracy: we want to make sure we evict pages that are least likely to be used in the future to minimize future disk seeks and stalls | Speed: we want to quickly decide which pages to evict from frames to make space for new fetches | Meta-data overhead: we don’t want metadata for a page (e.g. how likely it’s going to be used) to be larger than the page itself | . | Higher-end DBMS have complex replacement policies compared to open-source DBMS | . | Least-Recently Used (LRU) Maintain a timestamp of when each page was last accessed | Evict the page with the oldest timestamp | Keep pages in sorted order to reduce search time on eviction | Intuition: If the page hasn’t been used in a while, it probably won’t be used often in the future, and we can evict it | . | CLOCK Approximation of LRU without needing a separate timestamp per page Each page has a reference bit, which is set to 1 if the page is accessed since the last time it was checked | . | Organize pages in a circular buffer (clock), sweeping through with a “clock hand” Upon sweeping, check if a page’s bit is set to 1 | If the bit is set to 1, we know it was accessed, and reset to 0 | If the bit is not set to 1, we know it was not accessed, and evict it | Evicted frames can be re-populated with new fetched disk pages | . | . | Problems with LRU and Clock Sequential Flooding A query performs a sequential scan that reads every page | This pollutes the buffer pool with pages that are read once and then never again | The pages with the oldest timestamps that are important to keep are pushed to eviction by a sequential scan of many new pages | The most recently used page is actually the page we should evict in this case | . | Example Query 1 wants to find tuples with ID = 1 and fetches Page 0 | Query 2 wants to find average of all values and starts a sequential scan | The buffer pool fills up (Page 1, Page 2) and a page needs to be evicted Under LRU replacement policy, Page 0 is the oldest and should be evicted to be replaced with Page 3 | . | Query 3 wants to do the same thing as Query 1 Now Page 0 needs to be re-fetched even though it was just evicted! | . | The best approach would have been to evict either Page 1 or Page 2 because the sequential scan is just going to keep reading more pages, not using Page 1 or Page 2 ever again | . | . | LRU-K Track the history of last K references to each page as timestamps List of timestamps of every time the page was accessed | . | Compute the interval between subsequent accesses Which page has the largest interval, or the longest amount of time between accesses? | . | DBMS then uses this history to estimate the next time that page is going to be accessed | Used in more sophisticated DBMS | . | Localization DBMS chooses which pages to evict on a per transaction/query basis Rather than putting sequentially scanned pages into the global buffer pool, set aside some frames in the buffer pool for a specific query | Evict pages least recently used for that specific query, not the global view | . | Minimizes pollution of buffer pool from each query | Postgres maintains a small ring buffer that is private to the query | . | Priority Hints DBMS knows the context of each page during query execution | DBMS can provide hints to the buffer pool on whether a page is important or not | Example We continuously run queries to insert an incrementing ID value, like auto-increment | If the index is sorted on ID in ascending order, we know we want to keep accessing increasing index pages | Therefore, DBMS can tell buffer pool that previous index pages are less important and should be evicted | . | . | . . Dirty Pages How do we handle dirty pages, which have been modified since being fetched into the buffer pool? | FAST: If a page in the buffer pool is not dirty, then DBMS can simply “drop” it | SLOW: If a page in the buffer pool is dirty, DBMS must write back to disk to ensure changes are persisted | Tradeoff in replacement policy between fast evictions vs. dirty writing pages If a non-dirty page will be used in the future, DBMS should take the penalty of writing a dirty page out to disk | Writing a dirty page is 2 disk I/O’s: 1 to write page to disk and evict from buffer pool, 1 to fetch new page into buffer pool | Evicting a non-dirty page is 1 disk I/O: evict from buffer pool and 1 to fetch new page into buffer pool | . | . | Background Writing How do we figure out whether to keep a non-dirty page or a dirty page? | DBMS can periodically walk through page table and write dirty pages to disk | When a dirty page is safely written, DBMS can either evict the page or just unset the dirty flag The replacement policy can now drop more clean pages, since dirty pages have already been written to disk | . | Need to be careful that dirty pages are not written to disk before their log records have been written to disk Don’t want to write to disk without first recording the modifications made | . | . | Other Memory Pools DBMS needs memory for things other than just tuples and indexes | Other memory pools may not be backed by disk, depending on implementation Sorting, Join Buffers | Query Caches | Maintenance Buffers | Log Buffers | Dictionary Caches | . | . | . . Conclusion The DBMS can manage that sweet, sweet memory better than the OS can | Leverage semantics about the query plan to make better decisions Evictions/Replacement Policy | Allocations/Allocation Policy | Pre-fetching/Other Optimizations | . | . | . 6. Hash Tables . Lecture Summary . | | . 7. Tree Indexes I . 8. Tree Indexes II . 9. Multi-Threaded Index Concurrency Control . 10. Sorting &amp; Aggregations . 11. Join Algorithms . 12. Query Execution I . 13. Query Execution II . 14. Query Planning &amp; Optimization I . 15. Query Planning &amp; Optimization II . 16. Concurrency Control Theory . 17. Two-Phase Locking Concurrency Control . 18. Timestamp Ordering Concurrency Control . 19. Multi-Version Concurrency Control . 20. Database Logging Schemes . 21. ARIES Database Recovery . 22. Introduction to Distributed Databases . 23. Distributed OLTP Databases . 24. Distributed OLAP Databases . 25. Shasank Chavan (Oracle In-Memory Databases) . 26. Systems Potpourri . References .",
            "url": "https://nhtsai.github.io/notes/cmu-15-445",
            "relUrl": "/cmu-15-445",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "SQL Overview",
            "content": "SQL Overview . Mode SQL | Select Star SQL | W3 Schools SQL | Window Functions Video | . SQL . SQL, or Structured Query Language, is a declarative language used to access and manipulate databases. | SQL can execute queries, retrieve data, insert records, update records, and delete records. | SQL can create new databases, tables, stored procedures, views. | SQL can set permissions on tables, procedures, and views. | . RDBMS . RDBMS, or Relational Database Management System, is the basis for SQL and all modern database systems (MS SQL Server, MySQL, etc.). | A database is an organized collection of data tables. | A schema is an overview of all tables in a database. | A table is a collection of related data entries, formatted in columns and rows. Every table is broken up into smaller entities called fields. | Reference a table using database.table. | . | A field is a table column (vertical entity) designed to maintain specific information about every record in the table. | A record is a table row (horizontal entity) that holds specific information about each individual entry that exists in a table. | . SQL Syntax . SQL keywords are not case-sensitive. | Semicolons separate each SQL statement, often used to execute more than one statement in the same query call. | Use -- comment for single line comments, /* comment */ for multi-line comments | . SELECT . Extracts data from a database | Use double-quotes for column names if column name is a keyword. . -- Selecting all columns and rows SELECT * FROM table_name -- Using double quotes for column name (since max is a keyword) SELECT &quot;Max&quot; FROM table_name . | . SELECT TOP/LIMIT . Specify the number of records to return | Use LIMIT for MySQL, other databases have different ones . -- SQL Server/MS Access SELECT TOP 10 col1, col2 FROM table_name -- MySQL/PostgreSQL SELECT col1, col2 FROM table_name LIMIT 10 ORDER BY col1 DESC -- use ORDER BY to get meaningful row order . | . FROM . Denotes the database and table to query . -- Selecting all rows of col1 SELECT col1 FROM table_name . | . WHERE . Filters the data retrieved from the database based on a specified condition . -- Filtering for rows where col1 is positive SELECT col1 FROM table_name WHERE col1 &gt; 0 . | . Comparison Operators Symbol . Equal To | = | . Not Equal To | &lt;&gt;, != | . Greater Than | &gt; | . Greater Than or Equal To | &gt;= | . Less Than | &lt; | . Less Than or Equal To | &lt;= | . Arithmetic Operators Symbol . Addition | + | . Subtraction | - | . Multiplication | * | . Division | / | . Use operators to compare across columns in the same row. Need aggregate functions to compare across rows. | . | . Logical Operators Symbol Example . Match similar values | LIKE, % is a multiple wildcard character, _ is an individual wildcard character, [ab] is any single character, [^ab] is any single character not in brackets, [a-b] is any single character in a range of characters | WHERE col LIKE &#39;n%&#39; | . Match similar values, case insensitive | ILIKE | WHERE col LIKE &#39;n%&#39; | . Match on list of values, or query result | IN | WHERE col IN (1, 2, 3) | . Match on a range, inclusive | BETWEEN ... AND ... | WHERE col BETWEEN 5 AND 10 | . Match null values | IS NULL | WHERE col IS NULL | . Match non-null values | IS NOT NULL | WHERE col IS NOT NULL | . Match on 2 conditions | AND | WHERE col &gt;= 1 AND col &lt;= 2 | . Match on either of 2 conditions | OR | WHERE col = 1 OR (col = 2 AND col = 3) | . Match on not a condition | NOT | WHERE col NOT LIKE &#39;%n%&#39; WHERE col NOT BETWEEN 2 AND 3 | . Match on any values meeting condition | ANY | WHERE id = ANY(SELECT id FROM users WHERE score = 10) | . Match on all values meeting condition | ALL | WHERE id != ALL(SELECT id FROM users WHERE score &lt; 90) | . Exists if subquery returns 1+ rows | EXISTS | WHERE EXISTS (SELECT id FROM users WHERE age &lt; 18) | . ORDER BY . Sorts data based on 1+ columns | Ascending sort by default, use DESC for descending sort | Can also use #s in place of column names, corresponding to (1-indexed) order of columns in SELECT . -- Ordering using column names SELECT col1, col2 FROM table_name ORDER BY col1 DESC, col2 -- Ordering using column numbers SELECT col1, col2 FROM table_name ORDER BY 1 DESC, 2 . | . Aggregate Functions . Aggregation Command Example . Gets count of all non-null values | COUNT | SELECT COUNT(col1) | . Gets sum of all values, null values are 0 | SUM | SELECT SUM(col1) | . Gets average of all values, null values are 0 | AVG | SELECT AVG(col1) | . Gets minimum of all values | MIN | SELECT MIN(col1) | . Gets maximum of all values | MAX | SELECT MAX(col1) | . AS (Aliasing) . Renames a table or column . SELECT col1 AS &quot;Sales&quot; FROM table_name AS T . | . GROUP BY . Aggregates across smaller groupings instead of the whole table | Required for non-aggregated columns when performing aggregation | Can also reference columns by 1-indexed order in SELECT | Order of column names in grouping does not matter . SELECT A, B, COUNT(C) FROM table_name GROUP BY A, B -- Equivalent query SELECT A, B, COUNT(C) FROM table_name GROUP BY 2, 1 . | . HAVING . Filters the result of aggregate columns | Like WHERE but for aggregated columns . SELECT A, MAX(B) -- max aggregation FROM table_name WHERE A &gt;= 4 -- filter before aggregation GROUP BY A -- group by non-aggregated cols HAVING MAX(B) &gt; 10 -- filter after aggregation ORDER BY 2 DESC -- descending sort by MAX(B) LIMIT 100 -- return only first 100 rows . | . CASE WHEN . Used to handle if/else logic | Always goes in the SELECT clause | Default value is NULL if no ELSE clause . SELECT CASE WHEN condition THEN value WHEN condition THEN value ELSE value -- optional, default is NULL END AS col_name FROM table_name . | Use with aggregate functions to create different aggregation groups can use CASE WHEN alias in GROUP BY or copy and paste the whole CASE WHEN | . -- vertical table example SELECT CASE -- processes all c values into 5 groups WHEN col=1 THEN &#39;Freshman&#39; WHEN col=2 THEN &#39;Sophomore&#39; WHEN col=3 THEN &#39;Junior&#39; WHEN col=4 THEN &#39;Senior&#39; ELSE &#39;None&#39; END AS grade, COUNT(1) AS count FROM table_name GROUP BY grade -- groups by grade -- horizontal table example SELECT COUNT(CASE WHEN col=1 THEN 1 ELSE NULL END) AS fr_count, COUNT(CASE WHEN col=2 THEN 1 ELSE NULL END) AS so_count, COUNT(CASE WHEN col=3 THEN 1 ELSE NULL END) AS jr_count, COUNT(CASE WHEN col=4 THEN 1 ELSE NULL END) AS sr_count FROM table_name . | . DISTINCT . Selects unique values of 1+ columns | Only need to include DISTINCT once | Slow performance, especially in aggregations . SELECT DISTINCT year, month FROM table_name -- use with aggregate SELECT COUNT(DISTINCT month) AS unique_months FROM table . | . Joins . Uses common identifiers to join related tables Rows without matching common identifiers will default to NULL | . | Use aliases for tables when joining Aliases distinguish identical column names when joining two tables | . | Can use aliases in SELECT clause, like SELECT alias.col_name | . JOIN/INNER JOIN . Joins two tables, returns only matched rows from both tables | Unmatched rows are not included . -- Keeps matched rows SELECT A.*, B.col2 FROM table1 A JOIN table2 B ON A.col=B.col . | . LEFT JOIN/LEFT OUTER JOIN . Joins two tables, returns matched rows and unmatched rows from left table. . -- Keeps all A rows and matched B rows SELECT A.*, B.col2 FROM table1 A LEFT JOIN table2 B ON A.col=B.col . | . RIGHT JOIN/RIGHT OUTER JOIN . Joins two tables, returns matched rows and unmatched rows from right table. | Usually use flip the table order and use LEFT JOIN. . -- Keeps match A rows and all B rows SELECT A.*, B.col2 FROM table1 A RIGHT JOIN table2 B ON A.col=B.col . | . FULL OUTER JOIN . Joins two tables, returns all matched and unmatched rows. | Commonly used in aggregations to find overlap between two tables . -- Keeps all matched/unmatched rows SELECT A.*, B.col2 FROM table1 A JOIN table2 B ON A.col=B.col -- Example SELECT COUNT( CASE WHEN A.a IS NOT NULL AND B.b IS NULL THEN A.a -- must use A.a otherwise NULL and COUNT will be 0 ELSE NULL END ) AS A_only, COUNT( CASE WHEN A.a IS NOT NULL AND B.b IS NOT NULL THEN A.a -- can keep either A.a or B.b ELSE NULL END ) AS both, COUNT( CASE WHEN A.a IS NULL AND B.b IS NOT NULL THEN B.b -- must use B.b otherwise NULL and COUNT will be 0 ELSE NULL END ) AS B_only FROM table1 A JOIN table2 B ON A.a = B.b . | . Joins with Conditions . Can put conditions either in the WHERE or ON clauses . -- Finds rows that fit the condition rather -- than joining all rows then filtering SELECT A.*, B.* FROM table1 A JOIN table2 B ON A.id = B.id AND A.year &gt; B.year + 5 . | Conditioning WHERE vs. ON clauses WHERE joins all rows that match, then filters the rows based on that condition May be less efficient when joining large tables | . | ON only joins rows that match the conditions listed | For INNER JOINs, the two methods effectively produce the same results because unmatched (NULL) rows are discarded. | For OUTER JOINs, this is a slight difference. | . | Example documents table | . id name . 1 | doc1 | . 2 | doc2 | . 3 | doc3 | . 4 | doc4 | . 5 | doc5 | . downloads table | . id doc_id user . 1 | 1 | sandeep | . 2 | 1 | simi | . 3 | 2 | sandeep | . 4 | 2 | reya | . 5 | 3 | simi | . | WHERE conditioning vs. ON conditioning . SELECT documents.name, downloads.id FROM documents LEFT OUTER JOIN downloads ON documents.id=downloads.doc_id WHERE user=&#39;sandeep&#39; -- WHERE condition SELECT documents.name, downloads.id FROM documents LEFT OUTER JOIN downloads ON documents.id=downloads.doc_id AND user=&#39;sandeep&#39; -- ON condition . | Intermediate JOIN table for WHERE conditioning Matches every row using key, filters after the join. | . documents.id name downloads.id doc_id user . 1 | doc1 | 1 | 1 | sandeep | . 1 | doc1 | 2 | 1 | simi | . 2 | doc2 | 3 | 2 | sandeep | . 2 | doc2 | 4 | 2 | reya | . 3 | doc3 | 5 | 3 | simi | . 4 | doc4 | NULL | NULL | NULL | . 5 | doc5 | NULL | NULL | NULL | . | Intermediate JOIN table for ON conditioning Does not match rows that are not user=&#39;sandeep&#39;, filters one table before the join. | . documents.id name downloads.id doc_id user . 1 | doc1 | 1 | 1 | sandeep | . 2 | doc2 | 3 | 2 | sandeep | . 3 | doc3 | NULL | NULL | NULL | . 4 | doc4 | NULL | NULL | NULL | . 5 | doc5 | NULL | NULL | NULL | . | Result of WHERE conditioning . name downloads.id . doc1 | 1 | . doc2 | 3 | . | Result of ON conditioning . name downloads.id . doc1 | 1 | . doc2 | 3 | . doc3 | NULL | . doc3 | NULL | . doc4 | NULL | . doc5 | NULL | . | Because the WHERE condition filters after the LEFT OUTER JOIN, THE NULL rows of the user column are removed. You can see that because the ON condition filters before the LEFT OUTER JOIN, the NULL rows are kept in the final result. | For INNER JOINs, put join conditions in the ON clause and put where conditions in the WHERE clause. | For LEFT OUTER JOINs, put join conditions in the ON clause and put where conditions that reference the right table in the ON clause also. Referencing the right table after the LEFT JOIN in the WHERE clause converts the LEFT JOIN into an INNER JOIN. | Exception: reference the right table in the WHERE clause when looking for records not in the table, e.g. WHERE t2.id IS NULL. | Think of conditions in the ON clause as “right table WHERE clause filters” that are applied prior to joining. | . | . Joins with Multiple Foreign Keys . Accuracy of joining is improved | Performances: SQL uses indexes to speed up queries Using multiple join keys can speed up performance for large datasets | . SELECT A.col1, B.col1 FROM table1 A JOIN table2 B ON A.id = B.id AND A.col = B.col . | . Self Joins . Joins a table with itself | Self joins can help compare different records of the same table . -- WHERE: Find pairs from the same city SELECT A.name, B.name, A.city FROM users A, users B WHERE A.id &lt;&gt; B.id -- avoid matching users to themselves AND A.city = B.city -- find pairs from the same city -- ON: Find all pairs SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id != B.id -- prevents identical pairs, e.g. (5, 5) -- ON: Find all unique pairs SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id &lt; B.id -- prevents identical pairs and permutations, e.g. (1, 5) but not (5, 1) -- ON: Find unique pairs of duplicates SELECT A.name, B.name FROM table1 A JOIN table1 B ON A.id &lt; B.id -- gets different pairs AND A.last = B.last -- with the same value, eg. different people with same last name . | . UNION . Stacks two tables vertically | Only appends DISTINCT values, identical rows are dropped | Use UNION ALL to append all values, including identical rows | Can write 2 SELECT queries, and put the results on top of each other to create one table Result set’s column names usually same as first SELECT clause | . | Every SELECT statement within UNION must have same number of columns The columns must have similar data types | The columns in every SELECT statement must also be in the same order | . -- UNION Example SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; -- Union Example SELECT &#39;Customer&#39; AS Type, ContactName, City, Country FROM Customers UNION SELECT &#39;Supplier&#39;, ContactName, City, Country FROM Suppliers; -- UNION ALL Example SELECT City, Country FROM Customers WHERE Country=&#39;Germany&#39; UNION SELECT City, Country FROM Suppliers WHERE Country=&#39;Germany&#39; ORDER BY City . | . Data Types . Differences vary across various versions of SQL | . Imported As Stored As . String | VARCHAR(1024) | . Date/Time | DATE, DATETIME, TIMESTAMP, INTERVAL, YEAR | . Number | Exact: INTEGER, SMALLINT, DECIMAL, NUMERIC Approx: FLOAT, REAL, DOUBLE PRECISION | . Boolean | BOOLEAN | . CAST . In MySQL, changes data type of a column | In PostgreSQL, can also use expression::type casting syntax . SELECT CAST(col AS Integer) AS int_col FROM table_name WHERE CAST(col AS Integer) &gt; 10 . | . Dates . DATE, DATETIME, TIMESTAMP are date types used in SQL versions | YYYY-MM-DD is used for easy ordering, identical orders for dates and strings | Date ranges are stored as INTERVAL Intervals are defined in English terms, e.g. 10 seconds, 5 days, or 2 months | . -- Using intervals SELECT (CAST(table_name.date AS Timestamp) + INTERVAL &#39;1 week&#39;) AS week_after FROM table_name . | EXTRACT(field FROM date_col): helps deconstruct dates field includes: YEAR, MONTH, DAY, HOUR, SECOND, DECADE, DOW (day of week) | . SELECT EXTRACT(YEAR FROM date_col) AS year FROM table_name . | DATE_TRUNC(field, date_col): rounds date to a specified precision field includes: YEAR, MONTH, DAY, HOUR, SECOND, DECADE | . SELECT DATE_TRUNC(DAY, date_col) AS rounded_day FROM table_name . | System Datetime Variables CURRENT_DATE() | CURRENT_TIME() | CURRENT_TIMESTAMP() | LOCALTIME() | LOCALTIMESTAMP() | NOW() | To any above, can add AT TIME ZONE timezone_code to convert to a different timezone. | . SELECT NOW() SELECT CURRENT_DATE() AT TIME ZONE &#39;PST&#39; AS current_date_pst . | . Null Functions . Functions vary by SQL version | IFNULL(col, value): returns default value if expression is null | COALESCE(col, value): returns default value if expression is null . SELECT name, price * (inventory + COALESCE(ordered, 0)) FROM products . | . String Functions . LEFT(str, n): get n characters from left end of string | RIGHT(str, n): get n characters from right end of string | TRIM(type chars FROM str): trim characters from string TRIM(both &#39;()&#39; FROM col1): trim from both ends | TRIM(leading &#39;()&#39; FROM col1): trim from front | TRIM(trailing &#39;()&#39; FROM col1): trim from back | . | POSITION(substr IN str): get position of substring in string, case-sensitive | STRPOS(str, substr): get position of substring in string, case-sensitive | SUBSTR(str, start_idx, n): get n-length substring starting at index of string | CONCAT(str, str, ...): concatenate strings | str || str || ...: concatenate strings | UPPER(str): convert string to upper case | LOWER(str): convert string to lower case | . Subqueries . Subqueries are nested/inner queries to perform operations in multiple steps | Subqueries are required to have aliases . SELECT sub.* FROM ( SELECT * FROM students WHERE grade &gt; 80 ) sub WHERE sub.class_level = &#39;Senior&#39; . | Example: How many incidents happen, on average, on Fridays in December? . -- Count # of incidents that happen on each day. -- Average # of daily incidents over each day of week (5: Friday) in month (December). SELECT EXTRACT(MONTH FROM sub.incident_date) AS &quot;Month&quot;, sub.day_of_week, AVG(sub.daily_incidents) FROM ( SELECT EXTRACT(DOW FROM incident_date) AS day_of_week, incident_date, COUNT(incident_id) AS incidents FROM log_table GROUP BY 1, 2 ) sub WHERE sub.day_of_week = 5 AND EXTRACT(MONTH FROM sub.incident_date) = 12 . | Example: How many incidents happen for each category, averaged over all months? . -- Count # of incidents per category that happen on each month -- Average # of incidents per category over all months SELECT sub.category, AVG(sub.incidents) AS avg_incidents_per_month FROM ( SELECT EXTRACT(MONTH FROM incident_date) AS &quot;month&quot;, category, COUNT(incident_id) as incidents FROM log_table GROUP BY 1, 2 ) sub GROUP BY 1 . | Subqueries in Conditional Logic Don’t use aliases for subqueries in conditional statements . | Example: get all products that have min price . | . SELECT * FROM products WHERE price = (SELECT MIN(price) FROM products) -- subquery returns single cell . Example: get the products in the top 5 prices | . SELECT * FROM products WHERE price IN ( SELECT DISTINCT price FROM products ORDER BY price DESC LIMIT 5 ) -- subquery returns multiple cells (of one column) . | Joining Subqueries Using filtering in the ON clause can handle cases when subquery returns one or multiple rows | ON filtering is the same as WHERE filtering for INNER JOINS . | Example: get the products in the top 5 prices | . SELECT * FROM products JOIN ( SELECT DISTINCT price FROM products ORDER BY price DESC LIMIT 5 ) sub ON products.price = sub.price . | Joining Subqueries with Aggregation Functions . Example: Find the products from the top 3 categories with the least counts. | . -- subquery returns top 3 categories with least product counts -- query selects all products in the specified 3 categories -- result set is ordered by count and descending price -- (products with same category have same counts) SELECT products.*, sub.c AS counts FROM products JOIN ( SELECT category, COUNT(id) AS c FROM products GROUP BY category ORDER BY c LIMIT 3 ) sub ON products.category = sub.category ORDER BY sub.c, products.price DESC . Unlike using WHERE col IN (subquery), JOIN (subquery) ON table.col=sub.col allows aggregated values to be passed into the outer query because an IN clause is limited to one set of values. Additionally, MySQL does not allow LIMIT in subqueries in the WHERE clause. | . | . | Using Subqueries to Improve Performance Inefficient: FULL JOIN creates a huge intermediate join table | . -- Get counts of A and B entities per month SELECT COALESCE(A.month, B.month) AS month, -- fill null values of outer join COUNT(DISTINCT A.id) AS count_A, COUNT(DISTINCT B.id) AS count_B FROM A FULL OUTER JOIN B -- creates a large intermediate join table ON A.month = B.month GROUP BY 1 . Intermediate Join Table . A.id A.month B.month B.id . 1 | A | A | 5 | . 1 | A | A | 6 | . 2 | B | B | 7 | . 3 | B | B | 7 | . 4 | C | NULL | NULL | . NULL | NULL | D | 8 | . | Result . month count_A count_B . A | 1 | 2 | . B | 2 | 1 | . C | 1 | 0 | . D | 0 | 1 | . | Optimized: Use subqueries to divide the problem into smaller tables before joining . | . -- Get counts of A and B entities per month -- Pre-aggregate counts in A and B separately before joining SELECT COALESCE(A.month, B.month) as month, subA.count_A, subB.count_B FROM ( SELECT month, COUNT(DISTINCT id) AS count_A FROM A GROUP BY 1 ) subA FULL OUTER JOIN ( SELECT month, COUNT(DISTINCT id) AS count_B FROM B GROUP BY 1 ) subB ON subA.month = subB.month . Intermediate Join Tables . month count_A . A | 1 | . B | 2 | . C | 1 | . month count_B . A | 2 | . B | 1 | . D | 1 | . | Result . month count_A count_B . A | 1 | 2 | . B | 2 | 1 | . C | 1 | 0 | . D | 0 | 1 | . | . | The two methods get the same result, but the join using subqueries is faster because operating on smaller intermediate join tables is more efficient. Method 1 joins both tables then gets counts for A and B per month. | Method 2 gets counts for A and B per month separately, then joins the results together. | . | . Window Functions . A window is a group of related rows that are somehow related the current row, e.g. all rows with same month or same city | A window function performs calculation across a window, Unlike aggregation functions, window functions don’t group rows into a single output row No need to group by non-aggregated rows | . | Window functions are able to access more than just the current row of query result | A window function takes in a column/row and a window of related rows that includes the row | . | Aggregation functions can serve as window functions Inside OVER(), use ORDER BY to sort the window by a column | Cannot use GROUP BY with window functions | . SELECT SUM(col) OVER (...) SELECT COUNT(col) OVER (...) SELECT AVG(col) OVER (...) -- Example: Get a running total -- Using SUM as a window function -- ORDER BY to calculate chronologically SELECT duration, SUM(duration) OVER (ORDER BY start_time) AS running_total FROM rides . Window Result Durations are summed and ordered by start_time to create a running total | Without ORDER BY, each value would be sum of all seconds in its partition | . | . duration running_total . 1 | 1 | . 1 | 2 | . 2 | 4 | . 1 | 5 | . 4 | 9 | . 3 | 12 | . | Apply window function over individual groups Inside OVER(), use PARTITION BY to specify how the window function builds the window Each partition will be treated as one window, upon which the function will be applied | . | Without PARTITION BY, the whole table will be treated as one window | . -- using aggregation, cannot get individual information from all employees SELECT dept_name, MAX(salary) AS max_salary FROM employees GROUP BY dept_name -- better to use window function -- PARTITION BY specifies how to group rows to form a window SELECT E.*, MAX(salary) OVER(PARTITION BY dept_name) AS max_salary FROM employees E . The aggregation will return each department and the max salary of that department, without any other employee data. | The window function will return every employee data with the max salary of the department that employee works in The window function duplicates the department’s max salary to every row | Without PARTITION BY, the max salary of all departments will be duplicated for every row | . | . | ROW_NUMBER(): window function that returns number of row . | RANK(): window function that ranks values Like ROW_NUMBER() but gives same values when tied, e.g. 1, 2, 2, 4, 4, 4, 7 | . | DENSE_RANK(): window function that ranks values without skipping numbers Like RANK() but does not skip values, e.g. 1, 2, 2, 3, 3, 3, 4 | . SELECT team, score, ROW_NUMBER(score) OVER(PARTITION BY team ORDER BY score), RANK(score) OVER(PARTITION BY team ORDER BY score), DENSE_RANK(score) OVER(PARTITION BY team ORDER BY score) FROM table_name . Team Score ROW_NUMBER() RANK() DENSE_RANK() . A | 30 | 1 | 1 | 1 | . A | 40 | 2 | 2 | 2 | . A | 40 | 3 | 2 | 2 | . A | 55 | 4 | 4 | 3 | . B | 20 | 1 | 1 | 1 | . B | 20 | 2 | 1 | 1 | . B | 35 | 3 | 3 | 2 | . C | 20 | 1 | 1 | 1 | . -- Fetch the top 3 employees in each department by salary -- Using ROW_NUMBER() may miss out on other employees tied for top 3 salary values SELECT * FROM ( SELECT E.*, ROW_NUMBER() OVER (PARTITION BY dept_name ORDER BY salary DESC) AS salary_num FROM employee E ) sub WHERE sub.salary_num &lt; 4 -- Fetch the top employees in each department -- who earn the top 3 salaries in their department -- Returns every employee earning one of the top 3 salaries in their department SELECT * FROM ( SELECT E.*, RANK() OVER (PARTITION BY dept_name ORDER BY salary DESC) AS salary_rank FROM employee E ) sub WHERE sub.salary_rank &lt; 4 . | NTILE(n): window function that determines which bucket row falls in e.g. use n = 4 for quartile, or n = 100 for percentile | For small partition rows with less rows than tiles, the tiles will resemble a numerical ranking | . -- get quartile and percentile of student scores for each class SELECT class, score, NTILE(4) OVER (PARTITION BY class ORDER BY score DESC) AS quartile, NTILE(100) OVER (PARTITION BY class ORDER BY score DESC) AS percentile FROM table_name . | LAG(col, n): window function that creates a column that pulls from the previous n rows of col First n rows of column will be NULL because no previous rows to pull from | Some versions of SQL allow default values, i.e. LAG(col, n, default), to fill in NULL values | . | LEAD(col, n): window function that creates a column that pulls from next n rows of col Last n rows of column will be NULL because no previous rows to pull from | Some versions of SQL allow default values, i.e. LEAD(col, n, default), to fill in NULL values | . -- Example of lag and lead score, partitioned on team SELECT team, score, LAG(score, 1) OVER (PARTITION BY team ORDER BY score) AS lag_score, LEAD(score, 1) OVER (PARTITION BY team ORDER BY score) AS lead_score FROM table_name . team score lag_score lead_score . A | 20 | NULL | 30 | . A | 30 | 20 | 40 | . A | 40 | 30 | NULL | . B | 30 | NULL | 60 | . B | 60 | 30 | 70 | . B | 70 | 50 | 75 | . B | 75 | 70 | NULL | . -- Show if the salary of an employee is higher, lower, or equal to the previous employee. SELECT E.*, LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id) AS prev_emp_salary, CASE WHEN E.salary &gt; LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id THEN &#39;Higher&#39; WHEN E.salary &lt; LAG(salary, 1, 0) OVER (PARTITION BY dept_name ORDER BY id) THEN &#39;Lower&#39; ELSE &#39;Equal&#39; END AS salary_range FROM employee E . id name dept_name salary prev_emp_salary salary_range . 1 | Anna | Admin | 4000 | 0 | Higher | . 3 | Bert | Admin | 5000 | 4000 | Higher | . 4 | Dirk | Admin | 3000 | 5000 | Lower | . 6 | Cris | Admin | 6000 | 3000 | Higher | . 2 | Kurt | Finance | 4000 | 0 | Higher | . 5 | Fred | Finance | 5000 | 4000 | Higher | . 7 | Paul | Finance | 5000 | 5000 | Equal | . 8 | Evan | Finance | 2000 | 5000 | Lower | . -- Example of removing null rows using subquery SELECT sub.* FROM ( SELECT location, duration - LAG(duration, 1) OVER (PARTITION BY location ORDER BY duration) AS difference FROM rides ) sub WHERE sub.difference IS NOT NULL . | Window Aliases can name and re-use same window for several queries | should always come after the WHERE clause | . SELECT location, duration, NTILE(4) OVER ntile_window AS quartile, NTILE(100) OVER ntile_window AS percentile FROM rides WINDOW ntile_window AS (PARTITION BY location ORDER BY duration) -- window alias ORDER BY location, duration . | . SELECT INTO . Copies data from one table into a new table (in external database) . SELECT * INTO adults IN &#39;backup.db&#39; -- optional: [IN external_db] FROM users WHERE age &gt;= 18 . | . UPDATE . Modifies existing records in a table . UPDATE table_name SET col1 = val1, col2 = val2, ... WHERE condition -- without WHERE clause, all rows will be updated . | . DELETE . Deletes data in a database | . INSERT INTO . Inserts new data into a database . -- specifying which columns, values to insert INSERT INTO table_name (col1, col2, col3, ...) VALUES (val1, val2, val3, ...) -- don&#39;t need to specify if adding all columns INSERT INTO table_name VALUES (val1, val2, val3, val4, val5) . | . INSERT INTO SELECT . Copies data from one table and inserts it into another table | data types in source and destination tables must match . INSERT INTO all_profits SELECT * FROM current_profits WHERE company = &#39;Apple&#39; . | . Common Table Expressions (CTE) . A temporary named result set that can be referenced within SELECT, INSERT, UPDATE, or DELETE statements, also used in a CREATE to create a view | Advantage over subqueries: CTEs can be use multiple times in a query . -- Find average grades for every senior student WITH cte_student_grades (student, class_year, avg_grade) AS ( SELECT S.first_name + &#39; &#39; + S.last_name, YEAR(S.graduation_date), AVG(E.grade) FROM students S JOIN exams E ON S.id = E.student_id GROUP BY first_name + &#39; &#39; + last_name, YEAR(graduation_date) ) -- use CTE in a SELECT statement SELECT student, avg_grade, &#39;Senior&#39; AS class_level FROM cte_student_grades WHERE class_year = 2021 . | Use to perform multi-level aggregations, e.g. average minimum grade AVG(MIN(grade)) SQL does not allow subqueries or aggregate functions inside an aggregate function | SO you have to do MIN(grade) first in a CTE, then do AVG(min_grade) | . -- Find average min and average max exam grades across all subjects WITH min_max_grade AS ( SELECT SU.id, SU.subject_name, MIN(E.grade) AS min_grade, MAX(E.grade) AS max_grade FROM subjects SU JOIN exams E ON SU.id = E.subject_id GROUP BY SU.id, SU.subject_name ) SELECT AVG(min_grade) AS avg_min_grade, AVG(max_grade) AS avg_max_grade FROM min_max_grade; -- use CTE in a query -- Alternatively, subqueries are not as readable, reusable, -- and opposite in terms of thought process (AVG -&gt; MIN/MAX) SELECT AVG(min_grade) AS avg_min_grade, AVG(max_grade) AS avg_max_grade FROM ( SELECT SU.id, SU.subject_name, MIN(E.grade) AS min_grade MAX(E.grade) AS max_grade FROM subjects SU JOIN exams E ON SU.id = E.subject_id GROUP BY 1, 2 ) . | . Stored Procedures . A prepared SQL code that can be saved and reused, like a function . -- Storing procedure CREATE PROCEDURE SelectAllUsers AS SELECT * FROM Users GO -- Executing procedure EXEC SelectAllUsers . | A procedure can take in parameters . -- Storing procedure CREATE PROCEDURE SelectAllUsers @City nvarchar(30), @MinAge Int AS SELECT * FROM Users WHERE city = @City AND age &gt;= @MinAge GO -- Executing procedure EXEC SelectAllUsers @City = &#39;Los Angeles&#39;, @MinAge = 18 . | . SQL Database . CREATE DATABASE . Creates a new database: CREATE DATABASE testDB | . DROP DATABASE . Drop an existing database: DROP DATABASE testDB | . BACKUP DATABASE . Create full backup of existing database . BACKUP DATABASE testDB TO DISK = &#39;D: backups latest_backup.bak&#39; WITH DIFFERENTIAL -- include to only back up changes since last full backup . | . SQL Table . Creating a table . -- Create new table CREATE TABLE Users ( id int, name varchar(255), city varchar(255), age int, is_loyal boolean ) -- Create new table from another table CREATE TABLE NYC_Users AS SELECT id, name, city, age FROM Users WHERE city = &#39;New York City&#39; . | . SQL Constraints . Specifies rules for the data in the table | . Constraint Function . NOT NULL | column cannot have null values | . UNIQUE | column has all different values | . PRIMARY KEY | column is NOT NULL and UNIQUE, serves as row id | . FOREIGN KEY | prevents actions that would destroy links between tables | . CHECK | ensures column values specify a specific condition | . DEFAULT | set default value for column if no value specified | . CREATE INDEX | used to quickly create and retrieve data from database | . AUTO_INCREMENT | starting from 1, increments by 1 every time row inserted | . sql -- Create table with various constraints CREATE TABLE Users ( id int NOT NULL AUTO_INCREMENT, dept_id int NOT NULL, full_name varchar(255) NOT NULL, email varchar(255) UNIQUE, age int, city varchar(255) DEFAULT &#39;Seattle&#39;, CHECK (age &gt;= 18), PRIMARY KEY (id), FOREIGN KEY (dept_id) ) . Alter or add, delete, modify columns or various constraints of a table . -- Add column ALTER TABLE Users ADD email VARCHAR(255) -- Drop column ALTER TABLE Users DROP column is_loyal -- Change datatype ALTER TABLE Users MODIFY COLUMN city varchar(300) . | Remove a table . -- Delete a table DROP TABLE Users -- Truncate: delete data inside but not table itself TRUNCATE TABLE Users . | . SQL Index . An index is used to retrieve data more quickly, not seen by users . -- Create index CREATE INDEX idx_name ON Users (last_name) -- Create unique index CREATE UNIQUE INDEX idx_name ON Users (first_name, last_name) -- Drop index DROP INDEX idx_name ON Users . | . Views . A virtual table based on the result-set of an SQL statement . -- Create the view CREATE VIEW [Luxury Products] AS SELECT product_name, price FROM products WHERE price &gt; (SELECT AVG(price) FROM products) -- Query the view SELECT * FROM [Luxury Products] -- Update the view CREATE OR REPLACE VIEW [Luxury Products] AS SELECT product_name, price FROM products WHERE price &gt; (SELECT AVG(price) FROM products) -- Drop the View DROP VIEW [Luxury Products] . | . References .",
            "url": "https://nhtsai.github.io/notes/sql-overview",
            "relUrl": "/sql-overview",
            "date": " • Sep 6, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "System Design Overview",
            "content": "System Design . Scalability . Scalability for Dummies | A Word on Scalability | . Clones . Load balancers evenly distribute user requests to public web servers for processing. Every server contains exactly the same codebase and does not store any user-related data, like sessions or profile pictures, on local disc or memory. | . | Sessions need to stored in a centralized data store that is accessible by all application servers. This way the user session can be maintained if a different application server handles the user’s requests. | E.g. Keeps a user’s shopping cart data the same if multiple servers handle the same user | . | The data store can be an external database or an external persistent cache (e.g. Redis), which will have better performance than an external database. External means data store is somewhere in or near the data center of application servers, does not reside on application servers themselves. | . | Deployment ensures that a code change is sent to all servers without an outdated server still serving old code. | Clones are instances of an machine image based upon a “super-clone” that is created from one of your servers. Just do an initial deployment of your latest code to a new clone and everything is ready. | . | . Databases . Using cloning, you can now horizontally scale across multiple servers to handle lots of requests. | But one day, your application slows and breaks due to the MySQL database. | Approach 1: Keep MySQL Apply active-passive replication strategy on the database. | Upgrade server components like RAM. | Consider sharding, denormalization, SQL tuning, etc. | Eventually the upkeep will become too expensive. | . | Approach 2: Transition to NoSQL Denormalize right from the beginning. | Remove joins from any database query. | Use MySQL as a NoSQL database or switch to a NoSQL database like MongoDB. | If database requests still get too slow, consider a cache. | . | . Cache . An in-memory cache (e.g. Memcached, Redis) is a simple key-value store that resides between the application and data storage. | The application should try to read from the cache first before hitting the database because the cache is lightning fast. The cache holds every dataset in memory (RAM) and handles requests as fast as possible, compared to disk data storage where I/O of reading/writing data is much slower. | . | Approach 1: Cached Database Queries Whenever a query on the database is run, store the result dataset in a cache. | Use a hashed version of query as cache key. | Expiration is a large issue. How do we decide what cached results to evict from the cache? | When data is updated, all cached results calculated from that data are outdated and need to be deleted. | . | . | Approach 2: Cached Objects See the data as an object or class. Let the class assemble the dataset from the database. | Store the complete instance of the class or assembled dataset into the cache. | . | Rather than storing results of multiple queries, we can aggregate the results as data for a class instance and store the instance in the cache. | If this ID is not present in the cache, load the data from DB, translate it and return to the client. You can cache this result by translating this data into the raw data your cache has, and put it into the cache. | This makes it easy to delete the object when a piece of data changes. | This makes asynchronous processing possible. Servers query the database to assemble the data in the class. | The application just serves the latest cached object and never touches the database. | . | . | Example: Let objects be user sessions, fully rendered blog articles, activity streams, user-friend relationships The blog object has multiple methods that query the database for data. | Instead of caching the result of these separate database calls, cache the entire blog object. | When something changes, the blog object queries the database for updated data. | The application only has to serve the latest cached blog post instead of querying the database. | . | . | . Asynchronism . Asynchronously doing work in advance and serving finished work with low request time. Used to turn dynamic content into static content. | Example: Pre-rendering pages into static HTML files to be served quicker. The rendering can be scripted to run every hour by a cronjob. | This pre-computing process helps make web applications scalable and performant. | . | . | Referring unforeseen, immediate requests to an asynchronous service. Upon receiving a compute-intensive task, the job is added to the job queue. | The job is then sent to an asynchronous server to be processed in the background. | Other less compute-intensive tasks can still be processed in the meantime, avoiding idle time. | The results of the compute-intensive job are returned once the server is done processing. | . | Basic idea: Have a queue of tasks or jobs that a worker can process. Backends become scalable and frontends become responsive. | . | Tools to implement async processing: RabbitMQ | . Performance vs Scalability . A service is scalable if it results in increased performance in a manner proportional to resources added. E.g. adding another server to handle requests speeds up the website response time | . | If the system is slow for a single user, there is a performance problem. | If the system is fast for a single user but slow under heavy load, there is a scalability problem. . | An always-on service is scalable if adding resources to facilitate redundancy does not result in loss of performance. E.g. Adding multiple replications of a database for redundancy does not decrease the query response time | . | Scalability requires applications be designed with scaling in mind. | Scalability also has to handle heterogeneity, in which servers may not work in the same manner. As new resources (hardware, software) come online, some nodes will be able to process faster or store more data than other nodes in a system. | . | . Latency vs Throughput . Latency is the time to perform some action or to produce some result. | Throughput is the number of such actions or results per unit of time. | Generally, you should aim for maximal throughput with acceptable latency. | . Availability vs Consistency . CAP Theorem . In a distributed system, you can only support 2 guarantees: Consistency: every read receives most recent write or an error | Availability: every request receives a (non-error) response, without guarantee that it contains the most recent write | Partition Tolerance: system continues to operate despite arbitrary partitioning due to network failures, e.g. a server crashes or goes offline, requests are dropped, etc. | . | CA or CP, or AP? Networks aren’t reliable, so partition tolerance (P) needs to be supported. | When a network partition failure happens, need to either: Cancel the operation, decreasing availability but ensuring consistency | Proceed with operation, decreasing consistency but ensuring availability | . | You need to make a software tradeoff between consistency (C) and availability (A). Consistency (C) and availability (A) is not a practical option. | For (CP), system will return up-to-date response, but it may return an error or time out if a network failure occurs | For (AP), system will return a response if a network failure occurs, but the response may be outdated. | . | . | For systems with no partitions or no network partition failures, all 3 guarantees can be satisfied. | . Consistency and Partition Tolerance (CP) . System is consistent across servers and can handle network failures, but responses to requests are not always available. | Waiting for a response from the partitioned node might result in a timeout error. | Good choice if atomic reads and writes are required, e.g. banks. Atomic refers to performing operations one at a time in a complete manner. | By returning an error, an incomplete operation is stopped. | . | Example: A read operation is performed, but data propagation to all nodes is interrupted by system failure. The system will | . | . Availability and Partition Tolerance (AP) . System is always available and can handle network failures, but the data is not always consistent or up to date across nodes. | Responses return the most readily available version of the data on any node, which might be outdated. | Writes might take some time to propagate when the partition/failure is resolved. | Good choice if eventual consistency is needed or when the system needs to continue working despite external errors. | . Consistency Patterns . With multiple copies of the same data (redundancy), how do we synchronize them across nodes (consistency) to provide all users the same view of the data? The CAP Theorem need to respond to every read with the most recent write or an error to be consistent. | . | . Weak Consistency . After a write, reads may or may not see it, and a best effort approach is taken. | Weak consistency works well for real-time use cases, such as VoIP, video chat, and realtime multiplayer video games. If you briefly lose reception during a phone call, you don’t really care or hear what was lost during connection loss. | . | Weak consistency is used in systems like memcached, where the result might or might not be there. | . Eventual Consistency . After a write, reads will eventually see it, typically within milliseconds. | Data is replicated asynchronously. | Eventual consistency works well in highly available systems. | Eventual consistency is used in systems like DNS and email. | . Strong Consistency . After a write, reads will see it. | Data is replicated synchronously. | Strong Consistency works well in systems that need transactions. | Strong consistency is used in systems like file systems and relational database management systems (RDBMSes). | . Transactions . An extended form of consistency across multiple operations. | E.g. transfering money from account A to account B Operation 1: subtract from A | Operation 2: add to B | What if something happens in between operations? E.g. Another transaction A or B, machine crashes | . | You want some kind of guarantee that the invariants will be maintained. Money subtracted from A will go back to A. | Money created will eventually be added to B. | . | . | Transactions are useful because… Correctness | Consistency | Enforce invariants | ACID: atomic, consistent, isolated, durable | . | . Availability Patterns . Fail-Over . Active-Passive Heartbeats are sent between the active server and the passive server on standby. Only the active server handles traffic. | If a heartbeat is interrupted, the passive server takes over the active server’s IP address and resumes service to maintain availability. | Downtime duration is determined by whether passive server is already running in ‘hot’ standby or starting from ‘cold’ standby. | . | Active-Active Both servers are managing traffic, spreading load between them | If servers are public-facing, DNS needs to know about public IPs of both servers. | If servers are private-facing, application logic needs to know about both servers. | . | Disadvantages More hardware and additional complexity | Potential for loss of data if active system fails before any newly written data can be replicated to the passive | . | . Replication . Master-Slave One master node handles all writes, which are then replicated onto multiple slave nodes. | . | Master-Master Both master nodes handle all write requests, spreading load between them. The changes are then replicated onto multiple slave nodes. | . | . Availability in Numbers . Uptime or downtime is the percentage of time the service is available/not available. | Availability is generally measured in 9s, by which a service with 99.99% availability is described as having “four 9s”. . Acceptable Downtime Duration 99.9% Availability 99.99% Availability . Downtime per year | 8h 45m 57.0s | 52m 35.7s | . Downtime per month | 43m 49.7s | 4m 23.0s | . Downtime per week | 10m 04.8s | 1m 05.0s | . Downtime per day | 1m 26.4s | 08.6s | . | . Availability in Sequence . Overall availability decreases when two components with &lt; 100% availability are in sequence. | Availability(Total)=Availability(Foo)∗Availability(Bar) text{Availability}( text{Total}) = text{Availability}( text{Foo}) * text{Availability}( text{Bar})Availability(Total)=Availability(Foo)∗Availability(Bar) | If both Foo and Bar have 99.9% availability each, their total availability in sequence would be 99.8%. | . Availability in Parallel . Overall availability increases when two components with &lt; 100% availability are in parallel. | Availability(Total)=1−(1−Availability(Foo))∗(1−Availability(Bar)) text{Availability}( text{Total}) = 1 - (1 - text{Availability}( text{Foo})) * (1 - text{Availability}( text{Bar}))Availability(Total)=1−(1−Availability(Foo))∗(1−Availability(Bar)) | If both Foo and Bar have 99.9% availability each, their total availability in parallel would be 99.9999%. | . Domain Name System (DNS) . A Domain Name System (DNS) translates a domain name, e.g. www.example.com, to an IP address, e.g. 8.8.8.8. DNS is hierarchical, with a few authoritative servers at the top level. | Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. | Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. | DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL). | . | A Name Server (NS) Record specifies the DNS servers for your domain/subdomain. | A Mail Exchange (MX) Record specifies the mail servers for accepting messages. | An Address (A) Record points a name to an IP address. | A Canonical Name (CNAME) points a name to another name or to an A Record, e.g. pointing example.com to www.example.com. | Services that provide managed DNS services include: CloudFlare, Route 53, etc. | . DNS Traffic Routing Methods . Round Robin Pairs an incoming request to a specific machine by circling through a list of servers capable of handling the request | May not result in a perfectly-balanced load distribution | . | Weighted Round Robin Each server machine is assigned a performance value, or weight, relative to the other servers in the pool, usually in an automated benchmark testing. This weight determines how many more or fewer requests are sent to that server, compared to other servers in the pool. | . | The result is a more even or equal load distribution. Prevents traffic from going to servers under maintenance. | Weights can help load balance between varying cluster sizes. | A/B Testing | . | . | Latency-Based Create latency records between servers in multiple regions. | When a request arrives, the DNS queries the NS, which looks at the most recent latency data. | The load balancer with the lowest latency is the one chosen to serve the user. | . | Geolocation-Based: Choosing servers to serve traffic based on the geographic location of users. E.g. routing all European traffic to a European load balancer | . | Can localize content and restrict content distribution based on region. | Can load balance predictably so each user location is consistently routed to the same endpoint. | . | . Disadvantages of DNS . Accessing a DNS server introduces a slight delay, which can be mitigated by caching. | DNS server management is complex and generally managed by governments, ISPs, and large companies. | DNS services are susceptible to Distributed Denial of Service (DDoS) attacks, which prevent users from accessing websites without knowing Twitter’s IP address(es). | . Content Delivery Network (CDN) . A Content Delivery Network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files (e.g. HTML, CSS, JS, photos, videos) are served from CDNs. Some CDNs like AWS CloudFront supports serving dynamic content. | . | The website’s DNS resolution tells clients which CDN server to contact. | . | Advantages Improved performance because users receive content from data centers close to them. | Reduced load because your servers do not have to serve requests that the CDN fulfills. | . | . Push CDNs . Receive new content whenever changes occur on your server. Content is only uploaded when it is new or changed, minimizing traffic but maximizing storage. | You take full responsibility for providing content, uploading directly to the CDN, and rewriting URLs to point to the CDN. | . | You can configure when content expires and when it is updated using TTLs. | Push CDNs work well for sites with small amounts of traffic or content that isn’t often updated. Content is pushed to the CDNs when needed, instead of being re-pulled at regular intervals. | For lots of updates, pushing content to the Push CDN places load on the server. | For heavy traffic, the Push CDN’s cached content may not be sufficient and will place more load on the server to push content to the Push CDN. | . | . Pull CDNs . Grabs new content from your server when the first user requests the content. You take full responsibility for providing content and rewriting URLs to point to the CDN. | This results in slower requests until content is cached on the CDN, as users need to pull from the server upon the first request. | . | A time to live (TTL) determines the life of the cached content, which you do not typically have control of. | Pull CDNs minimizing storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed. | Pull CDNs work well for sites with heavy traffic because the traffic spread out more evenly with only recently-requested content remaining on the CDN. Older requested content is expired by the TTL, making space for new content. | For lots of updates, the Pull CDN is able to pull and cache the updated content when requested or old content is expired. | For heavy traffic, the Pull CDN can serve the most requested, cached content, only pulling from the server when for less requested content. | . | . Disadvantages of CDNs . CDN costs could be significant depending on traffic, although this should be compared against additional costs of not using a CDN. | Cached content might be stale if it is updated before the TTL expires it to be updated. | CDNs require changing URLs for static content to point to the CDN, e.g. directing facebook.com to cdn-images.fb.com. | . Load Balancer . | . Reverse Proxy . Advantages | | . | Disadvantages | . Load Balancer vs Reverse Proxy . Application Layer . Microservices . Service Discovery . Disadvantages . Database . SQL/Relational Database Management System (RDBMS) ACID | Master-Slave Replication | Master-Master Replication | Disadvantages of Replication . | Federation . | Sharding . | Denormalization . | SQL Tuning | . | NoSQL BASE | Key-Value Store | Document Store | Wide-Column Store | Graph Database | . | SQL vs NoSQL | . Cache . Client Caching . | CDN Caching . | Web Server Caching . | Database Caching Query Caching | Object Caching | . | Application Caching . | Updating Cache Cache-Aside | Write-Through | Write-Behind/Write-Back | Refresh-Ahead | . | Cache Disadvantages | . Asynchronism . Message Queues | Task Queues | Back Pressure | Aynchronism Disadvantages | . Communication . Hypertext Transfer Protocol (HTTP) | Transmission Control Protocol (TCP) | User Datagram Protocol (UDP) | Remote Procedure Call (RPC) | Representational State Transfer (REST) | RPC vs REST Calls | . Security . References .",
            "url": "https://nhtsai.github.io/notes/system-design-overview",
            "relUrl": "/system-design-overview",
            "date": " • Sep 5, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Atomic Habits",
            "content": "Atomic Habits by James Clear . 1. The Surprising Power of Atomic Habits . Small improvements aren’t notable or noticeable but are far more meaningful in the long run. A 1% improvement everyday results in 1.01365=37.781.01^{365} = 37.781.01365=37.78 change. | A 1% decline everyday results in 0.99365=0.030.99^{365} = 0.030.99365=0.03 change. | . | Habits are the compound interest of self improvement. The value of consistent, small changes are only apparent after years. Likewise, bad habits can accumulate in many missteps which compound to toxic results. | Success is the product of daily habits – not once-in-a-lifetime transformations. | . | Focus on the trajectory of habits. Outcomes are a lagging measure of your habits, built up from many small behavioral changes. | . | Time magnifies the margin between success and failure, in which effects are multiplied after a while. | . . Positive Compounds Negative Compounds . Productivity: Doing extra work builds up. | Stress: Little stresses compound into serious health issues. | . Knowledge: Lifelong learning is transformative. | Negative Thoughts: Bad thoughts can skew your interpretation of life. | . Relationships: Small interactions can build a network of strong connections. | Outrage: Small aggravations build up to mass outrage. | . Breakthrough moments are often the result of many previous actions. | Habits seem to make no difference until a critical threshold is crossed. This Valley of Disappointment is why many people give up on new habits. Compounding is not a linear improvement; the most powerful outcomes are delayed, think exponentially. | . | Habits need to persist long enough to break through the Plateau of Latent Potential in order to make meaningful difference. Change can take years – before it happens all at once. | . | Systems are about the process that lead to goals, the results you want to achieve. Goals are good for setting a direction, but systems are best for making progress. | You do not rise to the level of your goals. You fall to the level of your systems. | . | . . Problems with Goals . Winners and losers have the same goals: The difference in outcomes is due to systems of continuous improvements. | . Achieving a goal is only a momentary change: Systems tackle the underlying issues rather than temporarily achieving satisfactory results. | . Goals restrict happiness: You put off happiness until goals are met, rather than falling in love with the process. | . Goals are at-odds with long-term progress: Commitment to the process continuous improvement determines progress. | . An atomic habit is a little habit that is part of a larger system, building up to remarkable achievements through compound growth. | . 2. How Your Habits Shape Your Identity . Changing habits is challenging because you try to change the wrong thing. You should focus on building identity-based habits (3 -&gt; 2 -&gt; 1), rather than outcome-based habits (1 -&gt; 2 -&gt; 3). | . | . .   Layers of Behavioral Change   . 1. Outcomes | changing your results, setting goals | what you get | . 2. Process | changing your habits and systems | what you do | . 3. Identity | changing your beliefs, assumptions, and biases | what you believe | . Behavior that is incongruent with the self will not last. You need to change the underlying beliefs that led to past behavior in order to change existing habits. | You should change identities to more easily align with new behaviors and habits. | . | However, you should not be so attached to any specific identity as to prevent further behavioral improvements. Progress requires unlearning. | . | Repeated behaviors contribute more evidence supporting a gradual belief in a new identity and self-image. | . . Process of Changing your Identity . 1. Decide the type of person you want to be. What kind of qualities do they have? | . 2. Prove it to yourself with small wins. | . Your habits shape your identity, and your identity shapes your habits, working in a feedback loop. You should let values, principles, and identity drive this loop, rather than results. | . | Building habits is fundamentally about becoming someone, changing your beliefs about yourself. | . 3. How to Build Better Habits in 4 Simple Steps . Edward Thorndike’s studies on cat behavior shows that behaviors that result in favorable outcomes are more repeated than behaviors that result in unfavorable outcomes. | A habit is a behavior that has been repeated enough times to become automatic. Useful actions are reinforced into habits through a try, fail, learn, try differently feedback loop. | . | Brain activity decreases as it learns the cues that predict success, applying automatic solutions when the conditions are right. The conscious mind can only pay attention to one problem at a time. | Habits reduce cognitive load and free up mental capacity, so you can allocate your attention to other tasks. | Habits free your mind to focus on new challenges, allowing you to solve problems with as minimal energy and effort as possible. | . | . .   The Stages of Habit . 1. Cue (Problem Phase) | Triggers brain to initiate a behavior, predicts a reward. | . 2. Craving (Problem Phase) | Motivational force or desire to act, linked to a desired change of internal state. | . 3. Response (Solution Phase) | The habit (thought or action) you perform, depends on level of motivation and ability. | . 4. Reward (Solution Phase) | The end goal of every habit, satisfies cravings and teaches which actions are worth remembering. | . If behavior is insufficient at any of the four stages, the habit will not be formed. | Habit Loop is a neurological feedback loop that ultimately allows you to create automatic habits. The cue triggers a craving, which motivates a response, which provides a reward, which satisfies the craving, and, ultimately, becomes associated with a cue. | The problem phase (cue and craving) indicates when something needs to change. | The solution phase (response and reward) indicates when you take action and achieve the desired change. | . | The Four Laws of Behavior Change is a simple set of rules used to design good habits and eliminate bad ones. | . . Creating a Good Habit Breaking a Bad Habit . 1st Law (Cue): Make it obvious. | Inverse 1st Law (Cue): Make it invisible. | . 2nd Law (Craving): Make it attractive. | Inverse 2nd Law (Craving): Make it unattractive. | . 3rd Law (Response): Make it easy. | Inverse 3rd Law (Response): Make it difficult. | . 4th Law (Reward): Make it satisfying. | Inverse 4th Law (Reward): Make it unsatisfying. | . 4. The Man Who Didn’t Look Right . The brain is continuously analyzing information and your surroundings, picking up on important cues to predict certain outcomes subconciously. | You don’t need to be aware of the cue for a habit to begin. Habits are formed under the direction of your automatic and nonconscious mind. | . | Habits are hard to change because they are so mindless and automatic. | Pointing-and-Calling is the safety system of pointing and calling things out loud in order to reduce mistakes by 85%. It is effective because it raises a nonconscious habit to a more conscious level. | The Habits Scorecard is a similar exercise to maintain awareness of your habits and routines. For each action in a list of daily habits, you write a “+”, “-“, or “=” if it’s a good, bad, or neutral habit, respectively. | The rating is depending on your situation and your goals. “Good” refers to habits that have net positive outcomes and reinforce your desired identity. | . | The goal is to observe your thoughts and actions without any judgements to notice any nonconscious habits. Then you can point it out and say out loud the action and its outcome. | . | Behavior change starts with awareness, recognition of habits, and acknoledgement of the cues that trigger them | . 5. The Best Way to Start a New Habit . An implementation intention is a plan made beforehand about when and where to act, detailing how you intend to implement a habit. Implementation intentions leverage both time and location cues, in the form: “When situation X arises, I will perform response Y.” | . | People who make a specific plan for when and where they will perform a new habit are more likely to follow through. A lack of clarity, not motivation, is what prevents people from making improvements because it’s not always obvious when and where to take action. | Being specific about what you want and how you will achieve it helps you avoid any distractions or obstacles. | . | . . Implementation Intention I will [BEHAVIOR] at [TIME] in [LOCATION]. . Sleeping | I will sleep for 7 hours at 12AM in my bed. | . Studying | I will study math for 30 minutes at 6PM in my bedroom. | . Exercise | I will exercise for 1 hour at 5PM in my local gym. | . Relationship | I will talk to someone I know for 1 hour at 3PM in my living room. | . The Diderot Effect states that obtaining a new possession often creates a spiral of consumption that leads to additional purchases. Many human behaviors follow this principle, deciding on what to do next based on what you have just done; each action becomes a cue that triggers the next behavior. | . | Habit Stacking is a special implementation intention that pairs a new habit with a current habit, rather than a particular time and location. | . . Habit Stacking After [CURRENT HABIT], I will [NEW HABIT]. . Sleeping | After I brush my teeth, I will go to sleep for 7 hours. | . Studying | After I eat breakfast, I will study for 2 hours. | . Exercise | After I finish my work, I will exercise for 1 hour. | . Relationship | After I eat dinner, I will text a friend or family member to catch up. | . Habit Stacking allows you to create larger stacks by chaining small habits together, creating a natural momentum of behaviors. Example Morning Routine Stack: After making a cup of coffee, I will meditate for 60 seconds. | After meditating, I will write my day’s to-do list. | After writing my to-do list, I will immediately begin my first task. | | Example Evening Routine Stack: After finishing my dinner, I will clean the dirty dishes. | After cleaning the dishes, I will wipe down the counter. | After wiping down the counter, I will prepare my coffee mug for tomorrow morning. | | . | Habit Stacking works best when the cue if highly specific and immediately actionable. It’s important to select the right cue to kick things off. It should have the same frequency as your desired habit. | You can create a list of your daily habits and a list of daily occurrences to formulate the best layering of your daily habit stack. | . | The 1st Law of Behavior Change is make it obvious. Implementation Intentions and Habit Stacking help create obvious cues for you habits and design a clear plan for when and where to take action. | . | . 6. Motivation is Overrated; Environment Often Matters More . Anne Thorndike’s studies on environmental design shows that your habits change depending on your environment and the cues in front of you. “Every habit is context dependent.” | . | Kurt Lewin’s equation B=f(P,E)B=f(P,E)B=f(P,E) describes how Behavior is a function of the Person in their Environment. | Hawkins Stern’s Suggestion Impulse Buying describes how customers occasionally buy products due to presentation over desire. | Vision is the most important sensory ability, so small changes in contexts, or visual surroundings, can greatly impact your actions. | Obvious visual cues can trigger behaviors more often and better form habits. Creating multiple cues can increase the odds of triggering a behavior. | Environment design is altering living and work spaces to increase exposure of positive cues and decrease exposure to negative cues. | . | Over time, habits will become associated with the entire context surrounding the behavior, and the context becomes the cue. | Habits can be easier to change or form in a new environment, rather than building habits in the face of competing cues and old contexts. Create separate spaces (rooms or activity zones) for different activities. One space, one use. | . | Habits easily form in stable, predictable environments where everything has a place and a purpose. | . 7. The Secret to Self-Control . The Vietnam War Studies showed that soldiers addicted to heroin got rid of their addiction overnight after returning to an environment devoid of the cues triggering heroin abuse. This finding challenged the conventional association of unhealthy behavior as a moral weakness or lack of discipline. | . | You don’t need tremendous willpower and self-control if you don’t spend lots of time resisting temptations. Creating a more disciplined environment is important. | Habits encoded in the brain can arise again once the internalized cues are present. | . | To eliminate a bad habit, exposure to its cues need to be reduced. | Self-control is a short-term strategy that fails often when willpower is needed to resist temptations in a negative environment. | . 8. How to Make a Habit Irresistible . Niko Tinbergen’s experiments on North American sea birds showed that the brains are preloaded with certain rules that activate stronger than usual when exaggerated cues, or supernormal stimuli, are present. An example of supernormal stimuli is junk food for humans who have developed a craving for salt, sugar, and fat over years of evolution. | Nowadays, this craving is no longer advantageous to our health. | . | Food science studies how to make food more attractive to consumers Orosensation is how a product feels in your mouth. | Dynamic contrast refers to the combination of sensations that can make foods more interesting to eat. | . | The more attractive an opportunity is, the more likely it is to become habit-forming. | James Olds and Peter Milner showed how dopamine controls craving and desire. Habits are dopamine-driven feedback loops. | Dopamine is released both when anticipating pleasure and experiencing pleasure. | The anticipation (“wanting”) of a reward is what motivates action to fulfillment (“liking”) of a reward. | The greater the anticipation, the greater the dopamine spike. | . | Temptation bundling makes habits more attractive by linking an action you want to do with an action you need to do. Over time, the reward gets associated with the cue, and the habit becomes more attractive. | . | Premack’s Principle states that more profitable behaviors will reinforce less probable behaviors. You become conditioned to do undesirable tasks if it means you also get to do a rewarding task. | . | . . Habit Stacking + Temptation Bundling . After [CURRENT HABIT], I will [HABIT I NEED]. | . After [HABIT I NEED], I will [HABIT I WANT]. | . 9. The Role of Family and Friends in Shaping Your Habits . Laszlo Polgar showed how deliberate practice and good habits can overcome innate talent. | The culture you live in determins which behaviors are attractive to you. Humans desire to belong, so earliest habits are imitated and heavily influenced by surrounding culture and family. | . | Humans mainly imitate the close, the many, and the powerful. The Close (Friends &amp; Family) | Proximity greatly impacts behavior and habits, and family and friends provide an invisible peer pressure. | Join a culture where your desired behavior is the normal behavior and you already have something in common with the group, transforming your individual goal to a shared goal. The Many (The Tribe) | | Solomon Asch showed how the behavior of the many can often override the behavior of the individual. | Humans desire to fit in, so matching the tribe to the behavior makes change very attractive. The Powerful (Status and Prestige) | | Humans are attracted to behaviors that earn respect, approval, admiration, and status. | Humans are not attracted to behaviors that lower status or are negatively-viewed culturally. | . | . 10. How to Find and Fix the Causes of Your Bad Habits . Every behavior has a surface level craving and a deeper, underlying motive. A craving is a specific manifestation of a deeper underlying motive, a desire to change your internal state. | A desire is the difference between a current state and an ideal future state. | . | . . Craving Underlying Motive . Using Tinder | Find love and reproduce | . Browsing Facebook | Connect and bond with others | . Posting on Instagram | Win social acceptance and approval | . Searching on Google | Reduce uncertainty | . Playing video games | Achieve status and prestige | . There are many different ways to address the same underlying motive. | Habits are associations between an observed cue and a predicted response, heavily influencing behaviors. Feelings and emotions transform the cues and predictions into applicable signals. | The cause of habits is the preceding prediction, which leads to a feeling. | . | Reframing habits to highlight their benefits (positive) rather than their drawbacks (negative) can change your mindset and make a habit more attractive. A motivation ritual refers to the association of habits with something enjoyable, which can then later be used as a cue for motivation. | Eventually the associated habit becomes a cue to something enjoyable. | . | If you can reprogram your predictions, you can transform a hard habit into an attractive one. | . 11. Walk Slowly, but Never Backward . Taking action is better than being in motion, practice over planning. You want to delay failure, so you avoid taking action. | It’s better to take action and iteratively improve your results. | Preparation or planning can become a form of procrastination. | The key is to start with repetition, not perfection. | . | Habit formation is the process by which a behavior becomes progressively more automatic through repetition. Long-term Potentiation refers to the strengthening of connections between neurons in the brain based on recent patterns of activity. | Hebb’s Law is “Neurons that fire together wire together.” | . | Repetition is a form of change. To build a habit, you need to practice it. Repeating a habit leads to physical changes in the brain, different adaptations for different kinds of tasks. | Active practice and iteration is more effective than passive learning and theorizing. | Automaticity is the ability to perform a behavior without thinking about each step, which occurs when the nonconscious mind takes over after crossing the habit line. | . | Learning curves plot automaticity vs. time spent, revealing that habits form based on frequency, not time. This means the amount of time you have been performing a habit is less important than the number of times you have performed the habit successfully. | . | . 12. The Law of Least Effort . It is human nature to follow the Law of Least Effort, which states that when deciding between two similar options, the option that requires less work is more attractive. The most value for the least effort is most attractive. | The most common behaviors are extremely convenient and require little effort, e.g. checking social media or watching TV. | . | Habits are obstacles to desired outcomes. If good habits can be made more convenient, then they are more likely performed. | The idea is to make it as easy as possible in the moment to do things that payoff in the long run. | . | One way to reduce friction is environment design, making cues more obvious and optimizing for convenient actions. Addition by subtraction, or lean production, is the strategy of relentlessly eliminating points of friction that waste time or energy and optimizing to achieve more with less effort. | . | .",
            "url": "https://nhtsai.github.io/notes/atomic-habits",
            "relUrl": "/atomic-habits",
            "date": " • May 2, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Making a Pull Request",
            "content": "Making a Pull Request . Fork the project you want to work on to get your own copy of the repository. . | Clone your copy of the repository using HTTPS or SSH. . git clone forked_repo.git . | Create a new branch to work on a separate branch. . git checkout origin/master -b fix_bug . | Make any modifications to the code, e.g. bug fix, new feature, etc. . | Preview your changes. . git diff . | Check the state of the repository. Files should be modified but untracked. . git status . | Track the modified files. . git add file.txt . | Commit the changes. . git commit -m &quot;Fixed typo in file.txt&quot; . | Check the state of the repository. Files should be modified and changes committed. . git status . | Push your new branch up to your remote forked repository. . git push origin HEAD . HEAD is a a shortcut for your current checked-out branch, aka fix_typo. . | Open a pull request from your fork. . | Read any contributing guidelines and rules of conduct. . | Review the changes in your pull request. . | Create a pull request if everything looks good. . | References . Anthony Explains #004 Video | .",
            "url": "https://nhtsai.github.io/notes/making-a-pull-request",
            "relUrl": "/making-a-pull-request",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Introduction to Apache Cassandra",
            "content": "Introduction to Apache Cassandra . Supplemental Video Lecture . . Relational Overview . Can RDBMS work for big data? . Replication makes ACID a lie Given a read-heavy workload, we want to create a replicated slave node from the master node. However, data is replicated asynchronously, which introduces replication lag. This means that if the client reads from the replicated slave node, old data will be returned and results are not consistent (ACID fails). | . | Third Normal Form doesn’t scale Unpredictable queries | . | Sharding is a nightmare Data is split all over the place, no more joins or aggregations. | Querying second indexes requires hitting every shard (non-performant), so we are forced to denormalize all things. | Keeping shards consistent, adding shards, or changing schemas requires custom scripts. | . | Low Availability Who is responsible for Master failovers? Whether manual or automatic, failovers need to be detected. | . | Multiple datacenter solutions are messy. | Downtime is frequent. | . | . Failure of RDBMS . Scaling is a mess | ACID naive at best (not consistent). | Re-sharding is a manual process. | Denormalize all 3rd normal form queries for performance. | High availability is complicated to achieve, requires additional operational overhead. | . Lessons Learned for a New Solution . Consistency is not practical –&gt; give up consistency. | Manual sharding and rebalancing is hard –&gt; build it into the cluster. | Every moving part makes systems more complex –&gt; simplify the architecture, no more master-slave. | Scaling up is expensive –&gt; only use commodity hardware, more affordable. | Scatter/Gather queries are not good –&gt; denormalize for real-time query performance, always hit 1 machine. | . Cassandra Overview . What is Apache Cassandra? . Fast Distributed Database: | High Availability: able to be accessed at all times | Linear Scalability: linear scale-up in performance | Predictable Performance: can guarantee service-level agreements (SLA) with very low latency | No single point of failure (SPOF): can withstand multiple points of failure | Multi-Datacenter: can be utilized across multiple datacenters out of the box | Commodity Hardware: can be implemented on affordable hardware when vertically scaling | Easy to manage operationally: can manage no matter the cluster size | Not a drop-in replacement for RDBMS: applications must be designed using Cassandra | . Hash Ring Analogy . No master/slave/replica sets | No config servers, zookeeper | Data is partitioned around the ring | Data is replicated to replication factor (RF=N) servers | All nodes hold data and can answer queries (both read &amp; write) | Location of data on ring is determined by partition key Partition key is a hash of the primary key. | . | . CAP Tradeoffs . CAP Theorem says that when a network partition failure happens, we can either: Cancel the operation and thus decrease the availability but ensure consistency | Proceed with the operation and thus provide availability but risk inconsistency | . | Latency between data centers makes consistency impractical, especially considering datacenters across the world. | Cassandra chooses availbility and partition tolerance over consistency. | . Replication . Data is replicated automatically and asynchronously. | Choose total number of servers to replicate data to, replication factor (RF), usually 3. Set when creating the keyspace, a group of Cassandra tables, like schema. | . | Data is always replicated to each replica. | If a machine is down during replication, missing data writes are replayed via hinted handoff. | . Consistency Levels . Consistency level is set on per query basis, can be configured for read or write. | ALL: all (100%) of replicas must confirm read/write to be considered successful. | QUORUM: majority (51%) of replicas must confirm read/write to be considered successful. | ONE: only 1 replica needs to confirm read/write to be considered successful, allows for fast read/write successes. | . Multiple Datacenters . Write to local datacenter (DC) and replicate asynchronously to other datacenters. Local Consistency Levels: QUORUM –&gt; LOCAL QUORUM | . | Replication factor per keyspace per datacenter. Can configure different RF for each datacenter. | . | Datacenters can be physical or logical. Can use one DC as OLTP for fast reads and one virtual DC for OLAP queries. | . | . Cassandra Internals and Choosing a Distribution . Write Path . Writes are written to any node in the cluster, which serves as the coordinator for the write request. | Writes are written to a commit log, an append-only, immutable data structure for durability. | Merges the write mutation to the memtable, an in-memory representation of the table. | Now Cassandra can respond to client about successful write. This simplicity in write path is why Cassandra is fast. | Every write includes a timestamp. | . | When memory is full, the memtable is serialized into an immutable SSTable and flushed to disk periodically. | New memtable is created in memory. | Cassandra never does any updates or deletes in-place. | Deletes are a special write case known as tombstone, a marker with a timestamp to say that there is no data at that location at that time. | . What is an SSTable? . An SSTable is an immutable data file for row storage. | Every write includes a timestamp of when it was written. | Partition is spread across multiple SSTables. | Same column can be in multiple SSTables. | Merged through compaction, taking small SSTables and merging them into bigger ones. Last write wins means for a row with many changes, only latest timestamp is kept so only the latest version of the row is saved. | . | Deletes are written as tombstones. | Easy backups since SSTables can be copied off to another server. | . Read Path . Any server/node may be queried, and it acts as the coordinator. | The coordinator then contacts nodes with the requested key in the read query. | On each node, data is pulled from SSTable(s) on disk and merged using the latest timestamp, like compaction. The disk type (SSD or hard drive) has a large impact on the speed and performance of Cassandra. | . | Consistency levels under ALL performs read repair in the background Sometimes nodes can disagree about the value of a given piece of data since Cassandra is eventually consistent. | read_repair_chance specifies the chance that Cassandra will update/sync information on all other replicas, default is around 10% chance of all reads. | . | . Open Source Distribution . Latest, bleeding edge features, perfect for hacking. | File JIRAs issues and bug reports for support. | Support via mailing list &amp; IRC. | . DataStax Enterprise Distribution . Open source Apache Cassandra at its core. | Focused on stable releases for enterprise. | Integrated Multi-Datacenter Search (for OLTP) | Integrated Spark or Hadoop for Analytics (for OLAP). | Free Startup Program (&lt; 3MM revenue, &lt; 30M funding). | Extended support, additional QA. | .",
            "url": "https://nhtsai.github.io/notes/datastax-ds101",
            "relUrl": "/datastax-ds101",
            "date": " • Apr 21, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Learning Object-Specific Distance From a Monocular Image",
            "content": "Learning Object-Specific Distance From a Monocular Image . Research paper by Jing Zhu, Yi Fang, Husam Abu-Haimed, Kuo-Chin Lien, Dongdong Fu, and Junli Gu 1. . Abstract . TODO . Introduction . In computer vision research for self-driving cars, researchers have not recognized the importance of environment perception in favor of more popular tasks, like object classification, detection, and segmentation. Object-specific distance estimation is important for car safety and collision avoidance, and the lack of deep learning applications is likely due to the lack of datasets with distance measures for each object in the scene. . Current self-driving systems predict object distance using the traditional inverse perspective mapping (IPM) algorithm. This method first locates a point on the object (usually on the lower edge of the bounding box), then projects the located point onto a bird’s-eye view coordinate map using camera parameters, and finally estimates the object distance using the constructed bird’s-eye view coordinate map. This simple method can provide reasonable distance estimates for objects close and in front of the camera, but it performs poorly when objects are located to the sides of the camera or curved roads and when objects are over 40 meters away. . The authors first sought “to develop an end-to-end learning based approach that directly predicts distances for given objects in the RGB images.” End-to-end meaning that object detection and distance estimation parameters are trained jointly. The base model extracts features from RGB images, then utilizes region of interest (ROI) pooling to generate a fixed-size feature vector for each object, and feeds the ROI feature vectors into a distance regressor to predict a distance (Z) for each object. However, this method was not sufficiently precise for self-driving. . The authors then created an enhanced model with a keypoint regressor to predict (X,Y) of the 3D keypoint coordinates (X,Y,Z). Leveraging the camera projection matrix, the authors defined a projection loss between the projected 3D point (X,Y,Z) and the ground truth keypoint (X*,Y*,Z*). The keypoint regressor and projection loss are used for training only. During inference, the trained model takes in an image with bounding boxes and outputs the object-specific distance, without any camera parameters invervention. . The authors constructed an extended dataset from the KITTI object detection dataset and the nuScenes mini dataset by “computing the distance for each object using its corresponding LiDAR point cloud and camera parameters.” . Enhanced model performs better at distance estimation, compared to the traditional IPM algorithm and the support vector regressor, is also more precise than the base model, and is twice as fast during inference than IPM algorithm. . Summary . Base model: use deep learning to predict distance from given objects on RGB image without camera parameter intervention | Enhanced model: base model with keypoint regressor and new projection loss | Dataset: extended KITTI and nuScenes mini object-specific distance datasets | . Related Work . Distance Estimation . Inverse Perspective Mapping (IPM): convert a point or a bounding box in the image to the corresponding bird’s-eye view coordinate | Support Vector Regressor: predict object-specific distance given width and height of a bounding box | DistNet: use YOLO for bounding boxes prediction instead of image features learning for distance estimation, the distance regressor studied geometric mapping from bounding box with certain width and height to distance value In contrast, this paper directly predicts distances from learned image features. | . | Marker-based Methods: use auxiliary information, create segmented markers in the image and estimate distance using the marker area and camera parameters | Calibration Patterns: predict physical distance based on rectangular pattern, where 4 image points are needed to compute camera calibration | . 2D Visual Perception . R-CNNs: boost accuracy, decrease processing time for object detection, classfiication, dsegmentation | SSD and YOLO: end-to-end frameworks to detect and classify objects in RGB images | Monocular Depth Estimation: predict dense depth maps for given monocular color images | . Methods . The authors propose a base model that predicts physical, object-specific distance from given RGB images and object bounding boxes and an enhanced model with keypoint regressor for improved distance estimation. . Base Method . . Feature Extractor . Extract feature map for entire RGB image using image feature learning network. | Use existing architecture (e.g. vgg16, resnet50) as feature extractors. | Output of last layer is max-pooled and extracted as feature map for input RGB image. | . Distance Regressor and Classifier . Feed feature map and object bounding boxes into ROI pooling layer to generate fixed-size feature vector Fi boldsymbol{F_i}Fi​ for each object in the image. | Pass pooled object feature vector into distance regressor for predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) and object classifier for predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Distance Regressor: 3 fully-connected (FC) layers, {2048, 512, 1} for vgg16 and {1024, 512, 1} for resnet50. Softplus activation layer applied on output of last FC layer to ensure predicted distance D(Fi)D( boldsymbol{F_i})D(Fi​) is positive. | . | Distance Classifier: A FC layer with (number of categories in dataset) neurons, then softmax function to get predicted category label C(Fi)C( boldsymbol{F_i})C(Fi​). | Loss Functions Regressor Loss: . Ldist=1N∑i=1NL1;smooth(D(Fi),di∗)L_{dist}= frac{1}{N} sum_{i=1}^{N} L_{ text{1;smooth}}(D( boldsymbol{F_i}), d^{*}_{i})Ldist​=N1​i=1∑N​L1;smooth​(D(Fi​),di∗​) L1;smooth={0.5(xi−yi)2/β,if ∣xi−yi∣&lt;β∣xi−yi∣−0.5∗β,otherwiseL_{ text{1;smooth}} = begin{cases}0.5(x_{i} - y_{i})^2/ beta, &amp; text{if $|x_{i}-y_{i}|&lt; beta$} |x_{i}-y_{i}|-0.5* beta, &amp; text{otherwise} end{cases}L1;smooth​={0.5(xi​−yi​)2/β,∣xi​−yi​∣−0.5∗β,​if ∣xi​−yi​∣&lt;βotherwise​ | Classifier Loss: . Lcla=1N∑i=1NLcross-entropy(C(Fi),yi∗)L_{cla}= frac{1}{N} sum_{i=1}^{N} L_{ text{cross-entropy}}(C( boldsymbol{F_i}), y^{*}_{i})Lcla​=N1​i=1∑N​Lcross-entropy​(C(Fi​),yi∗​) Lcross-entropy=−∑i=1Myi∗∗log(C(Fi))L_{ text{cross-entropy}}=- sum_{i=1}^{M} y^{*}_{i} * log(C( boldsymbol{F_i}))Lcross-entropy​=−i=1∑M​yi∗​∗log(C(Fi​)) | NNN: number of objects in the image | MMM: number of categories | β betaβ: smooth loss scaling factor, usually 1.01.01.0 | D(Fi)D( boldsymbol{F_i})D(Fi​): predicted distance | C(Fi)C( boldsymbol{F_i})C(Fi​): predicted category label | di∗d^{*}_{i}di∗​: ground truth distance of the iii-th object | yi∗y^{*}_{i}yi∗​: ground truth category label of the iii-th object | . | . Model Learning and Inference . Train feature extractor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}minLbase​=Lcla​+λ1​Ldist​ Set λ1=1.0 lambda_{1}=1.0λ1​=1.0 during training. | . | Use ADAM optimizer with beta value β=0.5 beta=0.5β=0.5, learning rate of 0.001, exponentially decayed after 10 epochs. | Classifier encourages model to learn features used in estimating more accurate distances, only used during training. | After training, base model can predict object-specific distances given RGB images and object bounding boxes as input. | . Enhanced Method . . Add keypoint regressor to optimize base model by introducing projection constraint for better distance prediction. . Feature Extractor . Same architecture as base model: vgg16 or resnet50 into ROI pooling layer for object specific features Fi boldsymbol{F_i}Fi​. | . Keypoint Regressor . Keypoint regressor KKK learns to predict approximate keypoint position in 3D camera coordinate system. | Distance regressor predicts Z coordinate, while keypoint regressor predicts (X, Y). | Keypoint Regressor: 3 fully-connected (FC) layers, {2048, 512, 2} for vgg16 and {1024, 512, 2} for resnet50. | Since there is no ground truth of 3D keypoint available, project the generated 3D point (X,Y,Z)=([K(Fi),D(Fi)])(X,Y,Z)=([K( boldsymbol{F_i}), D( boldsymbol{F_i})])(X,Y,Z)=([K(Fi​),D(Fi​)]) back to image plane using camera projection matrix PPP. | Compute errors between ground truth 2D keypoint ki∗k^{*}_{i}ki∗​ and projected point P⋅([K(Fi),D(Fi)])P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})])P⋅([K(Fi​),D(Fi​)]). | Keypoint Function . L3Dpoint=1N∑i=1N1di∗∥P⋅([K(Fi),D(Fi)])−ki∗∥2L_{3Dpoint}= frac{1}{N} sum_{i=1}^{N} frac{1}{d^{*}_{i}} | P cdot([K( boldsymbol{F_i}), D( boldsymbol{F_i})]) - k^{*}_{i} |_{2}L3Dpoint​=N1​i=1∑N​di∗​1​∥P⋅([K(Fi​),D(Fi​)])−ki∗​∥2​ Use weight with regard to ground truth distance to encourage better predictions for closer objects. | . | . Distance Regressor and Classifier . Same architecture as base model and training losses LdistL_{dist}Ldist​ and LclaL_{cla}Lcla​. | Distance regressor parameters also optimized by projection loss L3DpointL_{3Dpoint}L3Dpoint​. | . Model Learning and Inference . Train feature extractor, keypoint regressor, distance regressor, and object classifier simultaneously using: . minLbase=Lcla+λ1Ldist+λ2L3Dpoint text{min}L_{base}=L_{cla}+ lambda_{1}L_{dist}+ lambda_{2}L_{3Dpoint}minLbase​=Lcla​+λ1​Ldist​+λ2​L3Dpoint​ Distance loss weight constant: lambda1=10.0lambda_{1}=10.0lambda1​=10.0 | Keypoint loss weight constant: lambda2=0.05lambda_{2}=0.05lambda2​=0.05 | . | Use same optimizer, beta, and learning rate as the base model. | Training Only: use camera projection matrix PPP, keypoint regressor, and object classifier. | Testing: Given RGB image and bounding boxes, directly predicts object-specific distances without any camera parameter intervention. | Both models trained for 20 epochs with batch size of 1 on the training subset, augmented with horizontally-flipped training images. | After training, input RGB image with bounding boxes into trained model to get the output of the distance regressor as the estimated object-specific distance in the validation subset. | . Dataset Construction . KITTI and nuScenes mini both provide RGB images, 2D/3D bounding boxes, category labels for objects in the images, and the corresponding velodyne point cloud for each image. . Object Distance Ground Truth Generation . Use 3D bounding boxes to segment velodyne point cloud for each object. | Sort the segmented points based on depth values. | Extract the n-th depth value as object-specific ground truth distance, where n=0.1∗(number of segmented points)n=0.1*( text{number of segmented points})n=0.1∗(number of segmented points) to avoid extracing depth values from noise points. | Project velodyne points to corresponding RGB image planes and get their image coordinates as keypoint ground truth distance. | Append both ground truths to the object detection dataset labels. | KITTI . Split KITTI into training (3,712 RGB images, 23,841 objects) and validation sets (3,768 RGB images, 25,052 objects) using 1:1 split ratio. | All KITTI objects are categorized into 9 classes, i.e. Car, Cyclist, Pedestrian, Misc, Person_sitting, Tram, Truck, Van, DontCare. | Generated KITTI ground truth distances should be between [0, 80] meters. | . nuScenes mini . Split nuScenes mini into training (200 images, 1,549 objects) and validation sets (199 images, 1,457 objects). | All nuScenes mini objects are categorized into 8 classes, i.e. Car, Bicycle, Pedestrian, Motorcycle, Bus, Trailer, Truck, Construction_vehicle. | Generated nuScenes mini ground truth distances should be between [2, 105] meters. | . Evaluation . Evaluation Metrics . Use the same metrics as depth prediction. Let di∗d^{*}_{i}di∗​ and did_{i}di​ denote the ground truth distance and the predicted distance, respectively. . Threshold . % of di s.t. max(di/di∗,di∗/di)=δ&lt;threshold % text{ of } d_i text{ s.t. max}(d_i/d^{*}_{i},d^{*}_{i}/d_i)= delta&lt; text{threshold}% of di​ s.t. max(di​/di∗​,di∗​/di​)=δ&lt;threshold | Absolute Relative Difference . Abs Rel=1N∑i=1N∣di−di∗∣/di∗ text{Abs Rel}= frac{1}{N} sum_{i=1}^{N}|d_{i}-d^{*}_{i}|/d^{*}_{i}Abs Rel=N1​i=1∑N​∣di​−di∗​∣/di∗​ | Squared Relative Difference . Squa Rel=1N∑i=1N∥di−di∗∥2/di∗ text{Squa Rel}= frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}/d^{*}_{i}Squa Rel=N1​i=1∑N​∥di​−di∗​∥2/di∗​ | RMSE (linear) . RMSElinear=1N∑i=1N∥di−di∗∥2 text{RMSE}_{ text{linear}}= sqrt{ frac{1}{N} sum_{i=1}^{N} |d_{i}-d^{*}_{i} |^{2}}RMSElinear​=N1​i=1∑N​∥di​−di∗​∥2​ | RMSE (log) . RMSElog=1N∑i=1N∥log⁡di−log⁡di∗∥2 text{RMSE}_{ text{log}}= sqrt{ frac{1}{N} sum_{i=1}^{N} | log{d_{i}}- log{d^{*}_{i}} |^{2}}RMSElog​=N1​i=1∑N​∥logdi​−logdi∗​∥2​ | . Note: don’t include distances predicted for DontCare objects when calculating errors. . Compared Approaches . Inverse Perpective Mapping Algorithm (IPM): predicts object-specific distance by approximating a transformation matrix between a normal RGB image and its bird’s-eye view image using camera parameters Use IPM from MATLAB’s computer vision toolkit to get transformation matrices for RGB images from the validation subset. | Project middle points of the lower edges of the object bounding boxes into bird’s-eye view coordinates using the transformation matrices. | Take values along forward direction as estimated distances. | . | Support Vector Regressor (SVR): predicts object-specific distance given width and height of a bounding box Compute width and height of each bounding box in the training subset. | Train SVR with the ground truth distance. | Input widths and heights of each bounding box in the validation set to get estimated object-specific distances. | . | . Results . Both proposed models have much lower relative errors and higher accuracies, compared to IPM and SVR, on KITTI. | Enhanced model performs the best, implying effectiveness of keypoint regressor and projection constraint, on both KITTI and nuScenes mini. | . References . J. Zhu, Y. Fang, H. Abu-Haimed, K. Lien, D. Fu, and J. Gu. Learning Object-Specific Distance from a Monocular Image. arXiv:1909.04182, 2019. &#8617; . |",
            "url": "https://nhtsai.github.io/notes/distance-estimation",
            "relUrl": "/distance-estimation",
            "date": " • Mar 26, 2021"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a university graduate interested in data science and machine learning. Currently looking for internship and job opportunities. .",
          "url": "https://nhtsai.github.io/notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nhtsai.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}