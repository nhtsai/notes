<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="robots" content="none, noindex, nofollow, noarchive, noimageindex, nosnippet" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Semi-Supervised Classification with Graph Convolutional Networks | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Semi-Supervised Classification with Graph Convolutional Networks" />
<meta name="author" content="Nathan Tsai" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes on GCNs by Kipf &amp; Welling." />
<meta property="og:description" content="Notes on GCNs by Kipf &amp; Welling." />
<link rel="canonical" href="https://nhtsai.github.io/notes/gcn" />
<meta property="og:url" content="https://nhtsai.github.io/notes/gcn" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-24T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Semi-Supervised Classification with Graph Convolutional Networks" />
<script type="application/ld+json">
{"url":"https://nhtsai.github.io/notes/gcn","@type":"BlogPosting","headline":"Semi-Supervised Classification with Graph Convolutional Networks","dateModified":"2021-10-24T00:00:00-05:00","datePublished":"2021-10-24T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://nhtsai.github.io/notes/gcn"},"author":{"@type":"Person","name":"Nathan Tsai"},"description":"Notes on GCNs by Kipf &amp; Welling.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nhtsai.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/katex.min.css" integrity="sha512-SgbQw5g+dYpW7CUSasc2ontF/7/JIHORpkzukPxbO2JIcyGy7p2WtlkfBGHvekdhJawrdp+p1bEzBH+nU/cbEg==" crossorigin="anonymous" />
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/katex.min.js" integrity="sha512-vhW78Uk043jhhQnaIpK3Io8CbTmcD3fcfZyhl7qz+Md1P78MI7Hlw39lEqdddt8nobv4sclKZU8CeVDzIfLUNA==" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/contrib/auto-render.min.js" integrity="sha512-ZA/RPrAo88DlwRnnoNVqKINnQNcWERzRK03PDaA4GIJiVZvGFIWQbdWCsUebMZfkWohnfngsDjXzU6PokO4jGw==" crossorigin="anonymous"></script>
    <!-- MathJax -->
    <!-- <script defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.min.js" integrity="sha512-93xLZnNMlYI6xaQPf/cSdXoBZ23DThX7VehiGJJXB76HTTalQKPC5CIHuFX8dlQ5yzt6baBQRJ4sDXhzpojRJA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> -->
    <!-- KaTeX Auto-render Function with custom delimiter rules -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Semi-Supervised Classification with Graph Convolutional Networks</h1><p class="page-description">Notes on GCNs by Kipf & Welling.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-24T00:00:00-05:00" itemprop="datePublished">
        Oct 24, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nathan Tsai</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#research-paper">research-paper</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#machine-learning">machine-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#graph-learning">graph-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#semi-supervised-classification-with-graph-convolutional-networks">Semi-Supervised Classification with Graph Convolutional Networks</a>
<ul>
<li class="toc-entry toc-h2"><a href="#abstract">Abstract</a></li>
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#fast-approximate-convolutions-on-graphs">Fast Approximate Convolutions on Graphs</a>
<ul>
<li class="toc-entry toc-h3"><a href="#spectral-graph-convolutions">Spectral Graph Convolutions</a></li>
<li class="toc-entry toc-h3"><a href="#layer-wise-linear-model">Layer-Wise Linear Model</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#semi-supervised-node-classification">Semi-Supervised Node Classification</a>
<ul>
<li class="toc-entry toc-h3"><a href="#example-cora-dataset">Example: Cora Dataset</a></li>
<li class="toc-entry toc-h3"><a href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#related-work">Related Work</a>
<ul>
<li class="toc-entry toc-h3"><a href="#graph-based-semi-supervised-learning">Graph-Based Semi-Supervised Learning</a></li>
<li class="toc-entry toc-h3"><a href="#neural-networks-on-graphs">Neural Networks on Graphs</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#experiments">Experiments</a>
<ul>
<li class="toc-entry toc-h3"><a href="#datasets">Datasets</a></li>
<li class="toc-entry toc-h3"><a href="#experimental-set-up">Experimental Set-Up</a></li>
<li class="toc-entry toc-h3"><a href="#baselines">Baselines</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#results">Results</a>
<ul>
<li class="toc-entry toc-h3"><a href="#semi-supervised-node-classification-1">Semi-Supervised Node Classification</a></li>
<li class="toc-entry toc-h3"><a href="#evaluation-of-propagation-model">Evaluation of Propagation Model</a></li>
<li class="toc-entry toc-h3"><a href="#training-time-per-epoch">Training Time per Epoch</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#discussion">Discussion</a>
<ul>
<li class="toc-entry toc-h3"><a href="#semi-supervised-model">Semi-Supervised Model</a></li>
<li class="toc-entry toc-h3"><a href="#limitations-and-future-work">Limitations and Future Work</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</li>
</ul><h1 id="semi-supervised-classification-with-graph-convolutional-networks">
<a class="anchor" href="#semi-supervised-classification-with-graph-convolutional-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Semi-Supervised Classification with Graph Convolutional Networks</h1>

<h2 id="abstract">
<a class="anchor" href="#abstract" aria-hidden="true"><span class="octicon octicon-link"></span></a>Abstract</h2>
<p>TODO</p>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>The authors first consider a problem: <em>How do we classify nodes in a graph, where labels are only available for a small subset of nodes?</em> Think of a citation network where some documents have labels, but most do not. The authors frame it as “graph-based semi-supervised learning, where label information is smoothed over the graph via some form explicit graph-based regularization term in the loss function.”</p>
<ul>
  <li>graph-based: working with a network</li>
  <li>semi-supervised: only some nodes have labels</li>
  <li>smoothed over the graph: labels are approximated across the graph</li>
  <li>explicit graph-based regularization:</li>
</ul>

<p>For example, using graph Laplacian regularization:</p>

<p>$\mathcal{L} = \mathcal{L}<em>0 + \lambda \mathcal{L}</em>{\text{reg}}$</p>

<p>where $\mathcal{L}<em>{\text{reg}} = \sum</em>{i,j} A_{i,j} \lVert f(X_{i}) - f(X_{j}) \rVert ^{2} = f(X)^{T} \Delta f(X)$</p>

<ul>
  <li>$\mathcal{L}_0$ : supervised loss with respect to labeled part of graph</li>
  <li>$\mathcal{L}_{\text{reg}}$ : graph Laplacian regularization term</li>
  <li>$f(\dots)$ : a neural network-like differentiable function</li>
  <li>$\lambda$ : regularization weight</li>
  <li>$X$ : matrix of node feature vectors $X_{i}$</li>
  <li>$\Delta = D - A$ : un-normalized <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">graph Laplacian</a> of an undirected graph $G=(V, E)$ , with $N$ nodes $v_{i} \in V$ , edges $(v_{i}, v_{j}) \in E$
    <ul>
      <li>$A \in \Reals^{N \times N}$ : adjacency matrix (binary or weighted)
        <ul>
          <li>1 if connected, 0 otherwise.</li>
          <li>Symmetric since graph is undirected, i.e. both sides of diagonal are same</li>
        </ul>
      </li>
      <li>$D_{ii} = \sum_{j} A_{ij}$ : degree matrix
        <ul>
          <li>The diagonal is the number of edges for that nodes</li>
        </ul>
      </li>
      <li>Basically, positive diagonal of each node’s number of edges and -1’s for each neighbor</li>
    </ul>
  </li>
  <li>Assumes that connected nodes in the graph share the same label
    <ul>
      <li>This may restrict modeling capacity because edges could contain more information than node similarity</li>
    </ul>
  </li>
</ul>

<p>The authors want to encode the graph structure in a neural network model $f(X, A)$ .
For all nodes with labels, train on a supervised target $\mathcal{L}<em>{0}$ and avoid graph regularization $\mathcal{L}</em>{\text{reg}}$ in the loss function.</p>

<p>By conditioning $f(\dots)$ on adjacency matrix $A$ , the model can distribute gradient information from supervised loss $\mathcal{L}_0$ to learn representations of nodes both with and without labels. Because the model knows each node’s neighbors, the labeled nodes can help inform the unlabeled nodes.</p>

<h2 id="fast-approximate-convolutions-on-graphs">
<a class="anchor" href="#fast-approximate-convolutions-on-graphs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fast Approximate Convolutions on Graphs</h2>

<p>The overall goal is to learn a function of graph features that uses:</p>
<ul>
  <li>node features, like a $N \times D$ feature matrix $X$ , with $N$ nodes and $D$ input features</li>
  <li>graph features, like an adjacency matrix $A$
and produces a node-level $N \times F$ output matrix $Z$ , with $N$ nodes and $F$ output features</li>
</ul>

<p>Every neural network layer $H$ can then be written as a non-linear function:</p>

<p>$H^{(l+1)} = f(H^{(l)}, A)$</p>

<ul>
  <li>There are $L$ layers in the neural network</li>
  <li>$H^{(0)} = X$ : input feature matrix is the first layer</li>
  <li>$H^{(L)} = Z$ : output feature matrix is the last layer</li>
</ul>

<p>In this case, the layer-wise propagation rule is:</p>

<p>$H^{(l+1)} = \sigma (\tilde{D}^{-\frac{1}{2}} \tilde{A}\tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})$</p>

<ul>
  <li>$\tilde{A} = A + I_{N}$ : adjacency matrix of undirected graph G plus self connections</li>
  <li>$I_{N}$ : identity matrix</li>
  <li>$\tilde{D}<em>{ii} = \sum</em>{j} \tilde{A}_{ij}$ :</li>
  <li>$W^{(l)}$ : trainable weight matrix of layer $l$</li>
  <li>$\sigma(\dots)$ : activation function, such as $\text{ReLU}(\dots) = \text{max}(0, \dots)$</li>
  <li>$H^{(l)} \in \Reals^{N \times D}$ : activation matrix of layer $l$</li>
  <li>This propagation rule comes from first-order approximation of local spectral filters on graphs</li>
</ul>

<h3 id="spectral-graph-convolutions">
<a class="anchor" href="#spectral-graph-convolutions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spectral Graph Convolutions</h3>

<p>A spectral graph convolution is the signal $x \in \Reals^{N}$ (scalar for every node) multiplied by a filter $g_{\theta} = \text{diag}(\theta)$ parameterized by $\theta \in \Reals^{N}$ in the Fourier domain.</p>

<p>$g_{\theta} \star x = U g_{\theta} U^{T} x$</p>

<ul>
  <li>$U$ : matrix of eigenvectors of normalized graph Laplacian $L_{\text{norm}}$</li>
  <li>$L_{\text{norm}} = I_{N} - D^{-\frac{1}{2}} A D^{-\frac{1}{2}} = U \Lambda U^{T}$ : <a href="https://en.wikipedia.org/wiki/Laplacian_matrix#Symmetric_normalized_Laplacian">symmetric normalized graph Laplacian</a>
</li>
  <li>$\Lambda$ : diagonal matrix of $L_{\text{norm}}$ eigenvalues</li>
  <li>$U^T x$ : graph Fourier transform of x</li>
  <li>$g_{\theta}$ : function of eigenvalues of $L_{\text{norm}}$ , i.e. $g_{\theta}(\Lambda)$</li>
</ul>

<p>Calculating graph spectral convolutions is expensive</p>
<ul>
  <li>multiplication with eigenvector matrix $U$ is $O(N^{2})$</li>
  <li>eigen-decomposition of $L_{\text{norm}}$ is expensive for large graphs</li>
</ul>

<p>We can approximate $g_{\theta}(\Lambda)$ using a truncated expansion in terms of Chebyshev polynomials $T_{k}(x)$ up to $K^{\text{th}}$ order:</p>

<p>$g_{\theta’}(\Lambda) \approx \sum_{k=0}^{K} \theta’<em>{k} T</em>{k}(\tilde{\Lambda})$</p>

<ul>
  <li>$\tilde{\Lambda} = \frac{2}{\lambda_{\text{max}}} \Lambda - I_{N}$ : rescaled diagonal matrix of eigenvalues</li>
  <li>$\lambda_{\text{max}}$ : largest eigenvalue of $L_{\text{norm}}$</li>
  <li>$\theta’ \in \Reals^{K}$ : vector of Chebyshev coefficients</li>
  <li>Chebyshev polynomials are recursively defined as $T_{k}(x) = 2x T_{k-1}(x) - T_{k-2}(x)$ , with $T_{0}(x) = 1$ and $T_{1}(x) = x$</li>
</ul>

<p>Now, we have an approximated spectral graph convolution of a signal $x$ with a filter $g_{\theta’}$ :</p>

<p>$g_{\theta’} \star x \approx \sum_{k=0}^{K} \theta’<em>{k} T</em>{k}(\tilde{L}_{\text{norm}})x$</p>

<ul>
  <li>$\tilde{L}<em>{\text{norm}} = \frac{2}{\lambda</em>{\text{max}}} L_{\text{norm}} - I_{N}$ : rescaled normalized graph Laplacian, verified by $(U \Lambda U^{T})^{k} = U \Lambda^{k} U^{T}$</li>
  <li>This expression is $K$-localized as a $K^{\text{th}}$-order polynomial in the Laplacian
    <ul>
      <li>It depends only on the $K^{\text{th}}$-order neighborhood, or nodes that are at maximum $K$ steps away from the central node</li>
    </ul>
  </li>
  <li>Complexity of calculating this is $\mathcal{O}(\lvert \mathcal{E} \rvert)$ , linear in the number of edges</li>
</ul>

<h3 id="layer-wise-linear-model">
<a class="anchor" href="#layer-wise-linear-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layer-Wise Linear Model</h3>

<p>We can stack multiple approximated spectral graph convolutions to build a neural network, with each layer followed by a point-wise non-linearity.
What if we limited the neighborhood to nodes at most 1 step away $(K=1)$ ?
This makes the approximated convolution a function that is linear with respect to $L_{\text{norm}}$ and therefore a linear function on the graph Laplacian spectrum</p>

<p>Layer-Wise linear formulation: We can stack multiple of these layers without being limited to explicit parameterization of Chebyshev polynomials.
Such model can alleviate problem of over-fitting on local neighborhood structures for graphs with very wide node degree distributions, i.e. some nodes are highly connected and some nodes are barely connected.
This is common in social networks, citation networks, knowledge graphs, etc.
This layer-wise linear formulation can build deeper models using a fixed computational budget, which is known to improve modeling capacity on a number of domains.</p>

<p>We further approximate $\lambda_{\text{max}} \approx 2$ because the neural network parameters will adapt to during training:</p>

<p>$g_{\theta’} \star x \approx \theta’<em>{0} + \theta’</em>{1} (L_{\text{norm}} - I_{N}) x = \theta’<em>{0} x - \theta’</em>{1} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x$</p>

<ul>
  <li>$\theta’<em>{0}$ and $\theta’</em>{1}$ : filter parameters</li>
</ul>

<p>Applying the filters successively effectively convolves the $k^{\text{th}}$-order neighborhood of a node, where $k$ is the number of successive filtering operations or convolutional layers in the neural network model.</p>

<p>Each layer takes into consideration 1-order neighborhood, each each successive layer considers more and more neighbors.</p>

<p>To address over-fitting and minimize operations per layer, we constrain the number of parameters:</p>

<p>$g_{\theta} \star x \approx \theta (I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) x$</p>

<ul>
  <li>$\theta = \theta’<em>{0} = -\theta’</em>{1}$ : single parameter</li>
  <li>$I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$ has eigenvalues in range [0, 2]</li>
</ul>

<p>Repeating this operation can lead to numerical instabilities and exploding/vanishing gradients in deep neural network models. We ca address this using a renormalization trick.</p>

<p>$I_{N} + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$</p>

<ul>
  <li>$\tilde{A} = A + I_{N}$ : adjusted adjacency matrix</li>
  <li>$\tilde{D}<em>{ii} = \sum</em>{j} \tilde{A}_{ij}$ : degree matrix</li>
</ul>

<p>We can generalize this definition to a signal $X \in \Reals^{N \times C}$ with $C$ input channel (features for every node) and $F$ filters for feature maps:</p>

<p>$Z = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta$</p>

<ul>
  <li>$\Theta \in \Reals^{C \times F}$ : matrix of filter parameters</li>
  <li>$Z \in \Reals^{N \times F}$ : convolved signal matrix</li>
  <li>Filtering operation has complexity $\mathcal{O}(\lvert \mathcal{E} \rvert F C)$</li>
  <li>$\tilde{A}X$ can be efficiently implemented as product of sparse matrix and dense matrix</li>
</ul>

<h2 id="semi-supervised-node-classification">
<a class="anchor" href="#semi-supervised-node-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Semi-Supervised Node Classification</h2>

<p>Now we have a flexible model $f(X,A)$ for efficient information propagation on graphs.
We can relax certain assumptions typically made in graph-based semi-supervised learning by condition our model $f(X,A)$ both on the data $X$ and adjacency matrix $A$ of underlying graph structure.</p>

<p>When the adjacency matrix $A$ contains information not present in the data $X$ , using both can be especially powerful.</p>
<ul>
  <li>E.g. Citation links between documents in citation network</li>
  <li>E.g. Relations in a knowledge graph</li>
</ul>

<h3 id="example-cora-dataset">
<a class="anchor" href="#example-cora-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example: Cora Dataset</h3>

<p><img src="/notes/images/gcn/gcn-model.png" alt="GCN Model Diagram" title="GCN Model Diagram"></p>

<p>Consider a two-layer GCN for semi-supervised node classification on a graph with symmetric adjacency matrix $A$ (binary or weighted).</p>

<p>Preprocessing step:</p>

<p>$\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$</p>

<p>Forward propagation step:</p>

<p>$Z = f(X,A) = \text{softmax}(\hat{A} \text{ ReLU}(\hat{A} X W^{(0)}) W^{(1)})$</p>

<ul>
  <li>$\hat{A}$ : adjacency matrix with symmetric normalization trick</li>
  <li>$W^{(0)} \in \Reals^{C \times H}$ : input-to-hidden weight matrix for hidden layer with $H$ feature maps</li>
  <li>$W^{(1)} \in \Reals^{H \times F}$ : hidden-to-output weight matrix</li>
  <li>$\text{softmax}(x_{i}) = \frac{1}{z} \exp{x_{i}}$ : softmax activation function
    <ul>
      <li>$z = \sum_{i} \exp{x_{i}}$ : applied row-wise</li>
    </ul>
  </li>
</ul>

<p>Evaluate cross-entropy error over all labeled examples:</p>

<p>$\mathcal{L} = - \sum_{l \in \mathcal{Y}<em>{L}} \sum</em>{f=1}^{F} Y_{lf} \ln{Z_{lf}}$</p>

<ul>
  <li>$\mathcal{Y}_{L}$ : set of node indices that have labels</li>
  <li>$F$ : number of output labels</li>
  <li>$Y_{lf}$ : correct value for node index $l$ and output label $f$
    <ul>
      <li>Should be 0 for incorrect output labels, 1 for correct output label</li>
    </ul>
  </li>
  <li>$Z_{lf}$ : predicted value for node index $l$ and output label $f$
    <ul>
      <li>Should be higher for the correct output label, lower for incorrect output labels</li>
    </ul>
  </li>
</ul>

<p>Neural network weights $W^{(0)}$ and $W^{(1)}$ are trained using batch gradient descent using full dataset for every training iteration.</p>

<p>Using sparse representation for $A$ , memory requirement is $\mathcal{O}(\lvert \mathcal{E} \rvert)$ , linear in the number of edges.
Use dropout to introduce stochasticity in the training process.
Future work: memory-efficient extensions with mini-batch stochastic gradient descent</p>

<h3 id="implementation">
<a class="anchor" href="#implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation</h3>
<p>Use efficient sparse-dense matrix multiplication to make the computation complexity of the forward propagation step $\mathcal{O}(\lvert \mathcal{E} \rvert C H F)$ , linear in the number of graph edges.</p>

<h2 id="related-work">
<a class="anchor" href="#related-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Related Work</h2>

<h3 id="graph-based-semi-supervised-learning">
<a class="anchor" href="#graph-based-semi-supervised-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Graph-Based Semi-Supervised Learning</h3>
<h3 id="neural-networks-on-graphs">
<a class="anchor" href="#neural-networks-on-graphs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Neural Networks on Graphs</h3>

<h2 id="experiments">
<a class="anchor" href="#experiments" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experiments</h2>

<h3 id="datasets">
<a class="anchor" href="#datasets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Datasets</h3>

<p><strong>Citation Networks</strong></p>

<p><strong>NELL</strong></p>

<p><strong>Random graphs</strong></p>

<h3 id="experimental-set-up">
<a class="anchor" href="#experimental-set-up" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Set-Up</h3>

<h3 id="baselines">
<a class="anchor" href="#baselines" aria-hidden="true"><span class="octicon octicon-link"></span></a>Baselines</h3>

<h2 id="results">
<a class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<h3 id="semi-supervised-node-classification-1">
<a class="anchor" href="#semi-supervised-node-classification-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Semi-Supervised Node Classification</h3>

<h3 id="evaluation-of-propagation-model">
<a class="anchor" href="#evaluation-of-propagation-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation of Propagation Model</h3>

<h3 id="training-time-per-epoch">
<a class="anchor" href="#training-time-per-epoch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Time per Epoch</h3>

<h2 id="discussion">
<a class="anchor" href="#discussion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Discussion</h2>

<h3 id="semi-supervised-model">
<a class="anchor" href="#semi-supervised-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Semi-Supervised Model</h3>

<h3 id="limitations-and-future-work">
<a class="anchor" href="#limitations-and-future-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>Limitations and Future Work</h3>

<p><strong>Memory requirement</strong></p>

<p><strong>Directed edges and edge features</strong></p>

<p><strong>Limiting Assumptions</strong></p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>
<ul>
  <li>http://tkipf.github.io/graph-convolutional-networks/</li>
</ul>

  </div><a class="u-url" href="/notes/gcn" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes and thoughts from a lifelong student.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nhtsai" target="_blank" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/nhtsai" target="_blank" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
