<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="robots" content="none, noindex, nofollow, noarchive, noimageindex, nosnippet" /><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Introduction to Probability | notes</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Introduction to Probability" />
<meta name="author" content="Nathan Tsai" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Course notes from Harvard STAT 110, Fall 2011." />
<meta property="og:description" content="Course notes from Harvard STAT 110, Fall 2011." />
<link rel="canonical" href="https://nhtsai.github.io/notes/harvard-stat110" />
<meta property="og:url" content="https://nhtsai.github.io/notes/harvard-stat110" />
<meta property="og:site_name" content="notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-23T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Introduction to Probability" />
<script type="application/ld+json">
{"headline":"Introduction to Probability","dateModified":"2021-09-23T00:00:00-05:00","url":"https://nhtsai.github.io/notes/harvard-stat110","datePublished":"2021-09-23T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nhtsai.github.io/notes/harvard-stat110"},"author":{"@type":"Person","name":"Nathan Tsai"},"description":"Course notes from Harvard STAT 110, Fall 2011.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nhtsai.github.io/notes/feed.xml" title="notes" /><link rel="shortcut icon" type="image/x-icon" href="/notes/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/katex.min.css" integrity="sha512-SgbQw5g+dYpW7CUSasc2ontF/7/JIHORpkzukPxbO2JIcyGy7p2WtlkfBGHvekdhJawrdp+p1bEzBH+nU/cbEg==" crossorigin="anonymous" />
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/katex.min.js" integrity="sha512-vhW78Uk043jhhQnaIpK3Io8CbTmcD3fcfZyhl7qz+Md1P78MI7Hlw39lEqdddt8nobv4sclKZU8CeVDzIfLUNA==" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.20/contrib/auto-render.min.js" integrity="sha512-ZA/RPrAo88DlwRnnoNVqKINnQNcWERzRK03PDaA4GIJiVZvGFIWQbdWCsUebMZfkWohnfngsDjXzU6PokO4jGw==" crossorigin="anonymous"></script>
    <!-- MathJax -->
    <!-- <script defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-chtml.min.js" integrity="sha512-93xLZnNMlYI6xaQPf/cSdXoBZ23DThX7VehiGJJXB76HTTalQKPC5CIHuFX8dlQ5yzt6baBQRJ4sDXhzpojRJA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> -->
    <!-- KaTeX Auto-render Function with custom delimiter rules -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/notes/">notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/notes/about/">About Me</a><a class="page-link" href="/notes/search/">Search</a><a class="page-link" href="/notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to Probability</h1><p class="page-description">Course notes from Harvard STAT 110, Fall 2011.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-23T00:00:00-05:00" itemprop="datePublished">
        Sep 23, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Nathan Tsai</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      34 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/notes/categories/#course-notes">course-notes</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#mathematics">mathematics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#probability">probability</a>
        &nbsp;
      
        <a class="category-tags-link" href="/notes/categories/#statistics">statistics</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#harvard-stat-110-introduction-to-probability-fall-2011">Harvard STAT 110: Introduction to Probability, Fall 2011</a>
<ul>
<li class="toc-entry toc-h2"><a href="#course-resources">Course Resources</a></li>
<li class="toc-entry toc-h2"><a href="#1-probability-counting">1. Probability, Counting</a></li>
<li class="toc-entry toc-h2"><a href="#2-story-proofs-axioms-of-probability">2. Story Proofs, Axioms of Probability</a></li>
<li class="toc-entry toc-h2"><a href="#3-birthday-problem-properties-of-probability">3. Birthday Problem, Properties of Probability</a></li>
<li class="toc-entry toc-h2"><a href="#4-conditional-probability-i">4. Conditional Probability I</a></li>
<li class="toc-entry toc-h2"><a href="#5-conditional-probability-ii-law-of-total-probability">5. Conditional Probability II, Law of Total Probability</a></li>
<li class="toc-entry toc-h2"><a href="#6-monty-hall-simpsons-paradox">6. Monty Hall, Simpson’s Paradox</a></li>
<li class="toc-entry toc-h2"><a href="#7-gamblers-ruin-random-variables">7. Gambler’s Ruin, Random Variables</a></li>
<li class="toc-entry toc-h2"><a href="#8-random-variables-and-their-distributions">8. Random Variables and Their Distributions</a></li>
<li class="toc-entry toc-h2"><a href="#9-expectation-i-indicator-random-variables-linearity">9. Expectation I, Indicator Random Variables, Linearity</a></li>
<li class="toc-entry toc-h2"><a href="#10-expectation-ii">10. Expectation II</a></li>
<li class="toc-entry toc-h2"><a href="#11-poisson-distribution">11. Poisson Distribution</a></li>
<li class="toc-entry toc-h2"><a href="#12-discrete-vs-continuous-uniform-distribution">12. Discrete vs. Continuous, Uniform Distribution</a></li>
<li class="toc-entry toc-h2"><a href="#13-normal-distribution">13. Normal Distribution</a></li>
<li class="toc-entry toc-h2"><a href="#14-location-scale-lotus">14. Location, Scale, LOTUS</a></li>
<li class="toc-entry toc-h2"><a href="#15-midterm-review">15. Midterm Review</a></li>
<li class="toc-entry toc-h2"><a href="#16-exponential-distribution">16. Exponential Distribution</a></li>
<li class="toc-entry toc-h2"><a href="#17-moment-generating-functions-i">17. Moment Generating Functions I</a></li>
<li class="toc-entry toc-h2"><a href="#18-moment-generating-functions-ii">18. Moment Generating Functions II</a></li>
<li class="toc-entry toc-h2"><a href="#19-joint-conditional-marginal-distributions">19. Joint, Conditional, Marginal Distributions</a></li>
<li class="toc-entry toc-h2"><a href="#20-multinomial-cauchy-distributions">20. Multinomial, Cauchy Distributions</a></li>
<li class="toc-entry toc-h2"><a href="#21-covariance-correlation">21. Covariance, Correlation</a></li>
<li class="toc-entry toc-h2"><a href="#22-transformations-convolutions">22. Transformations, Convolutions</a></li>
<li class="toc-entry toc-h2"><a href="#23-beta-distribution">23. Beta Distribution</a></li>
<li class="toc-entry toc-h2"><a href="#24-gamma-distribution-poisson-process">24. Gamma Distribution, Poisson Process</a></li>
<li class="toc-entry toc-h2"><a href="#25-order-statistics-conditional-expectation-i">25. Order Statistics, Conditional Expectation I</a></li>
<li class="toc-entry toc-h2"><a href="#26-conditional-expectation-ii">26. Conditional Expectation II</a></li>
<li class="toc-entry toc-h2"><a href="#27-conditional-expectation-given-rv">27. Conditional Expectation Given R.V.</a></li>
<li class="toc-entry toc-h2"><a href="#28-inequalities">28. Inequalities</a></li>
<li class="toc-entry toc-h2"><a href="#29-law-of-large-numbers-central-limit-theorem">29. Law of Large Numbers, Central Limit Theorem</a></li>
<li class="toc-entry toc-h2"><a href="#30-chi-square-student-t-multivariate-normal">30. Chi-Square, Student-T, Multivariate Normal</a></li>
<li class="toc-entry toc-h2"><a href="#31-markov-chains-i">31. Markov Chains I</a></li>
<li class="toc-entry toc-h2"><a href="#32-markov-chains-ii">32. Markov Chains II</a></li>
<li class="toc-entry toc-h2"><a href="#33-markov-chains-iii">33. Markov Chains III</a></li>
<li class="toc-entry toc-h2"><a href="#34-a-look-ahead">34. A Look Ahead</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</li>
</ul><h1 id="harvard-stat-110-introduction-to-probability-fall-2011">
<a class="anchor" href="#harvard-stat-110-introduction-to-probability-fall-2011" aria-hidden="true"><span class="octicon octicon-link"></span></a>Harvard STAT 110: Introduction to Probability, Fall 2011</h1>

<h2 id="course-resources">
<a class="anchor" href="#course-resources" aria-hidden="true"><span class="octicon octicon-link"></span></a>Course Resources</h2>
<ul>
  <li>Professor Joe Blitzstein</li>
  <li><a href="https://www.youtube.com/playlist?list=PL2SOU6wwxB0uwwH80KTQ6ht66KWxbzTIo">Course Lectures</a></li>
  <li><a href="https://projects.iq.harvard.edu/stat110">Course Website</a></li>
  <li><a href="https://projects.iq.harvard.edu/files/stat110/files/stat110harvarditunesusyllabus.pdf">Course Syllabus</a></li>
  <li><a href="https://learning.edx.org/course/course-v1:HarvardX+STAT110x+2T2021/home">Course EdX</a></li>
  <li>
<a href="http://probabilitybook.net/">Course Textbook</a>: Introduction to Probability, 2 ed. by Blitzstein &amp; Hwang</li>
  <li>Course focuses on using probability as a language and set of tools for understanding statistics, science, risk, and randomness.</li>
  <li>Topics: sample spaces/events, conditional probability, Bayes’ Theorem, univariate distributions, multivariate distributions, limit laws, Markov chains</li>
</ul>

<h2 id="1-probability-counting">
<a class="anchor" href="#1-probability-counting" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Probability, Counting</h2>

<ul>
  <li>
    <p>Probability has its roots in gambling and is an important part of many disciplines today</p>
  </li>
  <li>
    <p><strong>Sample Space</strong>: set of all possible outcomes of an experiment</p>
    <ul>
      <li>
<strong>Experiment</strong>: <em>anything</em> with certain possible outcomes that are unknown beforehand</li>
      <li>
<strong>Event</strong>: a <em>subset</em> of the sample space</li>
      <li>
<em>Example</em>: The <em>experiment</em> of rolling 2 dice has 36 total outcomes in the <em>sample space</em>, and an <em>event</em> could be the subset of all outcomes that sum up to a number</li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Naive Definition of Probability</strong>
    <ul>
      <li>$ P(A)=\frac{\text{favorable outcomes}}{\text{total possible outcomes}} $
        <ul>
          <li>The number of outcomes where Event A occurs over all possible outcomes</li>
        </ul>
      </li>
      <li>
<em>Example</em>: Flipping a fair coin twice, what is probability that both flips are heads?
        <ul>
          <li>1 favorable outcome: $(HH)$</li>
          <li>4 total outcomes: $(HH, HT, TH, TT)$</li>
          <li>Therefore, $P(\text{both heads}) = \frac{1}{4}$</li>
        </ul>
      </li>
      <li>Assumes that <em>all outcomes are equally likely</em> and a <em>finite sample space</em>
        <ul>
          <li>If the coin was not fair, the outcomes would not be equally likely</li>
          <li>If the sample space is infinite, the probability definition doesn’t make sense</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Multiplication Rule</strong>
    <ul>
      <li>If we have an experiment with $n_1$ possible outcomes, and for each outcome of $1\text{st}$ experiment, there are $n_2$ possible outcomes for the $2\text{nd}$ experiment, $\ldots$, and for each outcome of $(r-1)\text{th}$ experiment, there are $n_r$ possible outcomes for the $r\text{th}$ experiment, then there are $n_1 * n_2 * \ldots * n_r$ overall possible outcomes</li>
      <li>
        <p>The total number of possible outcomes of a series of experiments is the <em>product of each experiment’s outcomes</em></p>
      </li>
      <li>
<em>Example</em>: 3 flavors of ice cream and 2 types of cones
        <ul>
          <li>Ice cream flavor experiment has 3 outcomes (chocolate, vanilla, strawberry)</li>
          <li>Waffle cone experiment has 2 outcomes (cake, waffle)</li>
          <li>There are $6 \text{ possible outcomes}= 2 \times 3 = 3 \times 2$
            <ul>
              <li>Order of cone/ice cream choice does not matter</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Binomial Coefficient</strong>
    <ul>
      <li>$\binom{n}{k}=\frac{n!}{(n-k)!k!}, 0 \text{ if } k&gt;n$</li>
      <li>The number of possible subsets of size k given n elements, where <em>order does not matter</em>
        <ul>
          <li>Sets must contain distinct elements, so <em>without replacement</em>
</li>
          <li>Pronounced “n choose k”</li>
        </ul>
      </li>
      <li>
<em>Intuition</em>: From the multiplication rule
        <ul>
          <li>Choose k elements from n elements: $n(n-1)(n-2) \ldots (n-k+1)$</li>
          <li>Order does not matter: divide by $k!$ because we over-counted by that factor
            <ul>
              <li>Consider $\binom{5}{2}$ , we want to count $(1, 2)$ and $(2, 1)$ as equivalent, not separately</li>
              <li>There are $k!$ possible orderings of a size k subset, so we divide by $k!$ to reduce it down to 1</li>
            </ul>
          </li>
          <li>$\therefore \binom{n}{k} = \frac{n(n-1)(n-2) \ldots (n-k+1)}{k!}=\frac{n!}{(n-k)!k!}$</li>
        </ul>
      </li>
      <li>
<em>Example</em>: Find probability of full house in poker, 5 card hand
        <ul>
          <li>A <em>full house</em> is 3 cards of 1 rank and 2 cards of another rank, e.g. 3 sevens and 2 tens</li>
          <li>Assume deck is randomly shuffled so that all sets of 5 cards are equally likely
            <ul>
              <li>Because we assume cards are evenly shuffled, we can justify using naive definition</li>
            </ul>
          </li>
          <li>Number of possible hands: $\binom{52}{5}$ , “52 choose 5”</li>
          <li>Favorable hands (full house): $\binom{13}{1} \binom{4}{3} \times \binom{12}{1} \binom{4}{2}$
            <ul>
              <li>Of 13 ranks choose 1 and of 4 cards need 3, then of 12 remaining ranks choose 1 and of 4 cards we need 2</li>
            </ul>
          </li>
          <li>$\therefore \frac{\binom{13}{1} \binom{4}{3} \times \binom{12}{1} \binom{4}{2}}{\binom{52}{5}} = \frac{13 \binom{4}{3} \times 12 \binom{4}{2}}{\binom{52}{5}}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Sampling Table</strong>
    <ul>
      <li>From the multiplication rule, we can determine how many ways there are to choose $k$ elements out of $n$ total elements</li>
      <li>We can choose to sample <strong>with replacement</strong> (we can choose the same element again) or <strong>without replacement</strong> (we cannot choose the same element again)</li>
    </ul>

    <table>
      <thead>
        <tr>
          <th style="text-align: right"> </th>
          <th style="text-align: center">Order Matters</th>
          <th style="text-align: center">Order Doesn’t Matter</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: right"><strong>With Replacement</strong></td>
          <td style="text-align: center">$n^k$</td>
          <td style="text-align: center">$\binom{n+k-1}{k}$</td>
        </tr>
        <tr>
          <td style="text-align: right"><strong>Without Replacement</strong></td>
          <td style="text-align: center">$nPk = \frac{n!}{(n-k)!}$</td>
          <td style="text-align: center">$\binom{n}{k} = \frac{n!}{(n-k)!k!}$</td>
        </tr>
      </tbody>
    </table>

    <ul>
      <li>
<em>Sample Method</em>: With replacement, order matters
        <ul>
          <li>$n_1 \times n_2 \times \ldots \times n_k = n^k$</li>
          <li>Have $k$ experiments, each with $n$ outcomes and order matters, so ${1, 2} \neq {2, 1}$</li>
        </ul>
      </li>
      <li>
<em>Sample Method</em>: Without replacement, order matters
        <ul>
          <li>$n \times (n-1) \times \ldots \times (n-k+1)$</li>
          <li>Have $k$ experiments, with $n$ outcomes without replacement and order matters, so ${1, 2} \neq {2, 1}$</li>
        </ul>
      </li>
      <li>
<em>Sample Method</em>: Without replacement, order does not matter (<strong>Binomial Coefficient</strong>)
        <ul>
          <li>$\frac{n \times (n-1) \times \ldots \times (n-k+1)}{k \times (k-1) \times \ldots \times 1} = \frac{n!}{(n-k)! \times k!} = \binom{n}{k}$</li>
          <li>Have $k$ experiments, with $n$ outcomes without replacement and order does not matter, so ${1, 2} = {2, 1}$</li>
          <li>Want 1 combination of $k$ elements to represent all $k!$ permutations with the same $k$ elements, so need to divide by $k!$</li>
          <li>Think of grouping all possible permutations with the same $k$ elements into $k!$ groups
            <ul>
              <li>For the $k=3$ choice $(1,2,3)$, there are $3!$ orderings that we want to group into 1 representation ${1,2,3}$</li>
              <li>To do this grouping for all different choices, we divide by $k!$ so that we’re left with the number of unique combinations of $k=3$ elements</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
<em>Sample Method</em>: With replacement, order does not matter (<strong>Bose-Einstein</strong>)
        <ul>
          <li>$\frac{(n+k-1)!}{(n-1)! \times k!} = \binom{n+k-1}{n-1} = \binom{n+k-1}{k}$</li>
          <li>Hardest to understand, not obvious from multiplication rule</li>
          <li>Have $k$ experiments, with $n$ outcomes and order does not matter, so $(1, 2) = (2, 1)$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="2-story-proofs-axioms-of-probability">
<a class="anchor" href="#2-story-proofs-axioms-of-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Story Proofs, Axioms of Probability</h2>

<ul>
  <li>General Advice for Homework
    <ul>
      <li>Don’t lose common sense</li>
      <li>Do check answers, especially by doing simple and extreme cases</li>
      <li>Label people, objects, etc. (If we have n people, label them 1, 2, …, n)</li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Bose-Einstein</strong>
    <ul>
      <li>Bose said flipping a fair coin twice had 3 equally likely outcomes $(HH, TT, 1H+1T)$</li>
      <li>Bose thought of the two coin flips as indistinguishable $(HT = TH)$, rather than $(HT, TH)$</li>
      <li>Thinking of coins as indistinguishable led to the discovery Bose-Einstein condensate</li>
      <li>
<em>Sample Method</em>: Pick k times from a set of n objects, where order does not matter and with replacement
        <ul>
          <li>Prove the answer is $\binom{n+k-1}{k}$ ways</li>
        </ul>
      </li>
      <li>
<em>Case</em>: if $k=0, \binom{n-1}{0} = 1$
        <ul>
          <li>Only 1 way to choose 0 elements from $n-1$ elements, choose none!</li>
        </ul>
      </li>
      <li>
<em>Case</em>: if $k=1, \binom{n}{1} = n$
        <ul>
          <li>$n$ ways to choose 1 of $n$ elements</li>
          <li>When only picking 1 element, no difference if with/without replacement or if order matters or does not matter</li>
        </ul>
      </li>
      <li>
<em>Case</em>: if $n=2, \binom{k+1}{k} = \binom{k+1}{1} = k+1$
        <ul>
          <li>They are equal because: If we choose $k$ of $k+1$ elements, then the remaining is the 1. If we choose 1, then the remaining is the $k+1$</li>
          <li>If we get the first element $x$ times, we know we got the second element $k-x$ times</li>
          <li>Number of possible first elements we get is from ${0, \ldots, k}$ , so there are $k+1$ possibilities of choosing $k$ from $n=2$ elements</li>
          <li>Can think of this as flipping a fair coin twice</li>
        </ul>
      </li>
      <li>
<em>Equivalent</em>: How many ways are there to put k indistinguishable elements into n distinguishable groups?
        <ul>
          <li>
<em>indistinguishable</em> here means order does not matter and with replacement</li>
          <li>Say $n=4, k=6$, a possible way is $(<em><strong>),(),(</strong>),(</em>)$</li>
          <li>We can also represent groups using separators: $( *** \mid \mid ** \mid * )$</li>
          <li>There must be $k$ stars and $n-1$ separators, we just need to choose $k$ positions for stars in $n+k-1$ total positions, or $\binom{n+k-1}{k} = \binom{n+k-1}{n-1}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Story Proof</strong>
    <ul>
      <li>Proof by interpretation, rather than by algebra or calculus</li>
      <li>
<em>Example</em>: $\binom{n}{k} = \binom{n}{n-k}$
        <ul>
          <li>Think about what this means in terms of choosing k elements rather than calculating factorials</li>
        </ul>
      </li>
      <li>
<em>Example</em>: $n \binom{n-1}{k-1} = k \binom{n}{k}$
        <ul>
          <li>Imagine we pick k of n people, with one of them designated as the president</li>
          <li>How many ways can we do this?</li>
          <li>Approach 1: choose the president, then choose k-1 of n-1 people to form the rest of the group (left side)</li>
          <li>Approach 2: choose $k$ of $n$ people, then select $1$ of the $k$ people to be the president (right side)</li>
          <li>Both approaches count the same thing in different ways</li>
        </ul>
      </li>
      <li>
<em>Example</em>: $\binom{m+n}{k}=\sum_{j=0}^{k}\binom{m}{j}\binom{n}{k-j}$ (Vandermonde’s Identity)
        <ul>
          <li>Imagine a group of $m$ people and a group of $n$ people, and we want to select $k$ people total from both groups</li>
          <li>One way is we choose $j$ from group $m$ and $k-j$ from group $n$</li>
          <li>How many ways are there to choose $j$ from group $m$ and $k-j$ from group $n$?
            <ul>
              <li>$\binom{m}{j}\binom{n}{k-j}$ ways, using multiplication rule to get total number of ways for this $j$</li>
            </ul>
          </li>
          <li>Then, we add those disjoint cases up to get the total number of ways!</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Non-Naive Definition of Probability</strong>
    <ul>
      <li>
<strong>Probability Space</strong>: $S$ and $P$, where
        <ul>
          <li>S is sample space, or the set of all possible outcomes of an experiment</li>
          <li>P is a function that maps
            <ul>
              <li>Input (Domain): any event A, which is a subset $A \subseteq S$</li>
              <li>Output (Range): $P(A) \in [0,1]$ a number between 0 and 1</li>
            </ul>
          </li>
          <li>such that
            <ul>
              <li>$P(\emptyset)=0, P(S)=1$
                <ul>
                  <li>Probability of empty set is 0 and probability of full set is 1</li>
                  <li>What does it mean for the empty set to occur?
                    <ul>
                      <li>If we get an outcome $S_0$ , if $S_0 \in A$, then we say A occurred, else $A$ did not occur</li>
                      <li>If the empty set occurred, then $S_0 \in \emptyset$ , but nothing is in the empty set, so it cannot occur.</li>
                      <li>We want impossible events to be impossible $P(\emptyset) = 0$</li>
                    </ul>
                  </li>
                  <li>Why is an outcome in S a certainty?
                    <ul>
                      <li>$S_0$ must be in $S$ because $S$ represents the universe of all outcomes</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <li>$P(\cup_{n=1}^{\infty} A_n) = \sum_{n=1}^{\infty}P(A_n), \text{ if } A_1,A_2,\ldots \text{ are disjoint/non-overlapping}$
                <ul>
                  <li>Probability of the countably infinitely many union equals the sum of the probabilities if the events $A_1,A_2,\ldots$ are disjoint/non-overlapping</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Everything in probability basically derives from this definition with the two <strong>Axioms of Probability</strong>
</li>
    </ul>
  </li>
</ul>

<h2 id="3-birthday-problem-properties-of-probability">
<a class="anchor" href="#3-birthday-problem-properties-of-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Birthday Problem, Properties of Probability</h2>

<ul>
  <li>
<strong>Birthday Problem</strong>
    <ul>
      <li>In a group of people, how likely is it that at least one pair of people share the same birthday?</li>
      <li>How many people do you need for a 50% chance two people have the same birthday?
        <ul>
          <li>$K$ people, find probability that 2 have the same birthday</li>
          <li>Exclude Feb 29, assume other 365 days are equally likely (use naive definition of probability)</li>
          <li>Assume independence of births, one person’s birthday has no effect on another person’s birthday</li>
        </ul>
      </li>
      <li>
<strong>Pigeonhole Principle</strong>: If there are more people than days in a year, then the probability two people share the same birthday is certain
        <ul>
          <li>If $K &gt; 365$, then probability is $1$</li>
          <li>Imagine 365 boxes, if there are more than 365 dots, at least one box must have more than one dot</li>
        </ul>
      </li>
      <li>Intuition says if probability of 1 for &gt; 365 people, then to get probability of 0.5, we need around 180 people.
        <ul>
          <li>The answer is 23 people for 50.7% chance two people share the same birthday</li>
        </ul>
      </li>
      <li>Let $K &lt;= 365$</li>
      <li>Probability of no match (0 pairs share a birthday)
        <ul>
          <li>Each of K persons has $\frac{1}{365}$ chance of a particular birthday</li>
          <li>If no matches, every person must have a different birthday (no replacement)</li>
          <li>$P(\text{no match}) = \frac{365 \times 364 \times \ldots \times (365 - K + 1)}{365^{K}}$</li>
        </ul>
      </li>
      <li>Probability of match (at least 1 pair shares a birthday)
        <ul>
          <li>Complement: $P(\geq 1 \text{ match}) = 1 - P(\text{no match})$</li>
          <li>If $K=23$, $P(\text{match}) = 1 - \frac{365 \times 364 \times \ldots \times 343}{365^{23}} \approx 1 - 0.492703 \approx 0.507297 \approx 50.7\%$</li>
          <li>If $K=50$, $P(\text{match}) = 1 - \frac{365 \times 364 \times \ldots \times 316}{365^{50}} \approx 97\%$</li>
          <li>If $K=100$, $P(\text{match}) = 1 - \frac{365 \times 364 \times \ldots \times 343}{365^{100}} \geq 99.999\%$</li>
        </ul>
      </li>
      <li>How could we get a birthday match with such low number of people?
        <ul>
          <li>The more relevant quantity is the number of pairs in K people
            <ul>
              <li>$\binom{K}{2} = \frac{K(K-1)}{2}$</li>
              <li>For $K=23$, $\binom{23}{2} = \frac{23 * 22}{2} = 23 * 11 = 253 \text{ pairs}$</li>
              <li>253 is a lot of pairs, and any one of those pairs may share a birthday</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>If we then consider what K to have 50% chance of finding a pair with same birthday or off by 1 day, the answer is $K=14$
        <ul>
          <li>With 14 people, there is a better than 50% chance to find a pair of people with the same birthday or off by 1 day</li>
          <li>More complicated problem to calculate</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Axioms of Probability</strong>
    <ol>
      <li>$P(\emptyset) = 0, P(S) = 1$
        <ul>
          <li>Probability of empty set is 0 (impossible)</li>
          <li>Probability of full sample space is 1 (certain)</li>
        </ul>
      </li>
      <li>$P(\cup_{n=1}^{\infty} A_{n}) = \sum_{n=1}^{\infty} P(A_{n})$
        <ul>
          <li>If $A_1, A2, \ldots \text{ are disjoint events}$</li>
          <li>Probability of the union (infinite or finite) equals the sum of probabilities of all events if events are disjoint.</li>
          <li>Think of probabilities as disjoint sets, union of sets is same as adding the sets up
        * Assume $0 \leq P(A) \leq 1$ , probability is between 0 and 1
        * As long as a thing satisfies both axioms, we can apply any theorems of probability to the thing.
        * From these 2 axioms, we can derive every theorem of probability.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Properties of Probability</strong>
    <ol>
      <li>$P(A^C) = 1 - P(A)$
        <ul>
          <li>Probability of complement of A is 1 minus probability of A</li>
          <li>Proof:
            <ul>
              <li>$1 = P(S) = P(A \cup A^C)$</li>
              <li>$1 = P(A) + P(A^C)$ , since $A \cap A^C = \emptyset$
                <ul>
                  <li>A and its complement are disjoint, nothing can be in both by definition</li>
                </ul>
              </li>
              <li>Therefore, $P(A^C) = 1 - P(A)$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>If $A \in B$ , then $P(A) \leq P(B)$
        <ul>
          <li>A and B are events, A is <em>in</em> B, “if A occurs then B occurs”</li>
          <li>Proof:
            <ul>
              <li>$B = A \cup (B \cap A^C)$ , where $A, (B \cap A^C)$ are disjoint
                <ul>
                  <li>B is made of of A and everything in B that’s not in A</li>
                </ul>
              </li>
              <li>$P(B) = P(A) + P(B \cap A^C) \geq P(A)$
                <ul>
                  <li>From axiom #2, adding some non-negative probability so must be $\geq$</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        <ul>
          <li>To get a probability of A union B, we can sum probabilities if A and B are disjoint.</li>
          <li>Otherwise, if we add non-disjoint events A and B, we double count the overlapping intersection</li>
          <li>Simple case of <strong>inclusion-exclusion</strong> with 2 events</li>
          <li>Proof:
            <ul>
              <li>
<em>Technique</em>: “disjointification” of A and B so that we can apply axiom #2</li>
              <li>$P(A \cup B) = P(A \cup (B \cap A^C))$
                <ul>
                  <li>Union of A and B equals union of A and everything in B that’s not in A</li>
                </ul>
              </li>
              <li>$P(A \cup B) = P(A) + P(B \cap A^C)$
                <ul>
                  <li>Since $A, (B \cap A^C)$ are disjoint and we can apply axiom #2</li>
                </ul>
              </li>
              <li>
<em>Technique</em>: “wishful thinking” to see if two sides are equal</li>
              <li>$P(A) + P(B) - P(A \cap B) \eqsim P(A) + P(B \cap A^C)$</li>
              <li>$P(B) \eqsim P(A \cap B) + P(B \cap A^C)$
                <ul>
                  <li>$A \cap B, A^C \cap B$ are disjoint because no elements can be in both $A, A^C$</li>
                  <li>Using axiom #2, $P(B) = P((A \cap B) \cup (B \cap A^C))$</li>
                  <li>B is equal to the part of B in A and part of B not in A</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>General Inclusion-Exclusion</strong>
    <ul>
      <li>Finds the probability of a union</li>
      <li>Alternates inclusion and exclusion of portions</li>
      <li>
<em>Case</em>: Inclusion-Exclusion for 3 events
        <ul>
          <li>$P(A \cup B \cup C) = P(A) + P(B) + P(C) - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$</li>
          <li>Add all 3 events, subtract the intersections of pairs, then add the intersection of all</li>
        </ul>
      </li>
      <li>
<em>Case</em>: Inclusion-Exclusion for N events
        <ul>
          <li>$P(A_1 \cup A_2 \cup \ldots \cup A_N) = \sum^{N}<em>{j=1} P(A_j) - \sum^{N}</em>{i \lt j} P(A_i \cap A_j) + \sum^{N}_{i \lt j \lt k} P(A_i \cap A_j \cap A_k) - \ldots + (-1)^{N+1} \times P(A_1 \cap A_2 \cap \ldots \cap A_N)$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<em>Example</em>: <strong>Matching Problem</strong> / <strong>de Montmort’s Problem (1713)</strong>
    <ul>
      <li>Problem originated from gambling problems:
        <ul>
          <li>Imagine $N$ cards, labeled from 1 to N</li>
          <li>Shuffle the cards</li>
          <li>Flip over the cards while counting from 1 to N</li>
          <li>If the count <em>matches</em> the flipped card’s number, then win</li>
        </ul>
      </li>
      <li>What’s the probability that one card has the same number as its position in the deck?</li>
      <li>Let $A_j$ be the event “jth card matches”</li>
      <li>$P(\geq 1 \text{ match}) = P(A_1 \cup A_2 \cup \ldots \cup A_N)$</li>
      <li>$P(A_j) = \frac{1}{N}$
        <ul>
          <li>Probability that card’s number matches position is 1 over N since all positions are equally likely for the card labeled j</li>
          <li>Naive definition uses $\frac{(N-1)!}{N!} = \frac{1}{N}$
            <ul>
              <li>$N!$ possible permutations of deck of cards</li>
              <li>$(N-1)!$ possible permutations of the deck of cards, fixing card labeled j is at jth position</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>$P(A_1 \cap A_2) = \frac{(N-2)!}{N!} = \frac{1}{N(N-1)}$
        <ul>
          <li>$N!$ possible permutations of deck of cards</li>
          <li>$(N-2)!$ possible permutations of the deck of cards, fixing card labeled i at ith position and card labeled j at jth position</li>
        </ul>
      </li>
      <li>$P(A_1 \cap \ldots \cap A_K) = \frac{(N-K)!}{N!}$
        <ul>
          <li>$N!$ possible permutations of deck of cards</li>
          <li>$(N-K)!$ possible permutations of the deck of cards, fixing $K$ cards at their corresponding positions</li>
        </ul>
      </li>
      <li>
<em>Technique</em>: Apply Inclusion-Exclusion
        <ul>
          <li>$P(A_1 \cup A_2 \cup \ldots \cup A_N) = N (\frac{1}{N}) - (\frac{N(N-1)}{2!})(\frac{1}{N(N-1)}) + (\frac{N(N-1)(N-2)}{3!})(\frac{1}{N(N-1)(N-2)}) - \ldots$</li>
          <li>Include $N$ 1-matches, subtract $\binom{N}{2}$ 2-matches, add …, subtract …</li>
          <li>Everything cancels!</li>
          <li>$P(A_1 \cup A_2 \cup \ldots \cup A_N) = \frac{1}{1} - \frac{1}{2!} + \frac{1}{3!} - \frac{1}{4!} + \frac{1}{5!} - \ldots + (-1)^{N+1} \times (\frac{1}{N!})$</li>
          <li>This is the Taylor series for $e^x$ .</li>
          <li>$P(A_1 \cup A_2 \cup \ldots \cup A_N) \approx 1 - \frac{1}{e}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-conditional-probability-i">
<a class="anchor" href="#4-conditional-probability-i" aria-hidden="true"><span class="octicon octicon-link"></span></a>4. Conditional Probability I</h2>

<ul>
  <li>
<em>Example</em>: <strong>Matching Problem</strong> / <strong>de Montmort’s Problem (1713)</strong>
    <ul>
      <li>Most famous example of <em>inclusion-exclusion</em>
        <ul>
          <li>Deck of N cards, labeled from 1 to N</li>
          <li>Random shuffle and flip through deck</li>
          <li>Win if a card’s label matches its position</li>
        </ul>
      </li>
      <li>
<em>Case</em>: $A_j$ , the jth card in the deck is labeled j
        <ul>
          <li>$P(A_j) = \frac{1}{N}$ : for the jth card, there are N possible positions in the deck</li>
        </ul>
      </li>
      <li>
<em>Case</em> $P(\cup_{j=1}^{N} A_j)$ , the probability of a match on any card j
        <ul>
          <li>To apply inclusion-exclusion we need:
            <ul>
              <li>For any $K$ cards, $P(A_1 \cap A_2 \cap \ldots \cap A_K) = \frac{(N-K)!}{N!}$
                <ul>
                  <li>If K cards match their positions, then we fix those cards in place</li>
                  <li>The rest of the $N-K$ cards can be in any order, but the $K$ cards are constrained</li>
                </ul>
              </li>
              <li>There are $\binom{N}{K} = \frac{N!}{(N-K)!K!}$ such terms like this because the inclusion-exclusion does $P(\cap_{j=1}^{K} A_j)$ for any $K$</li>
              <li>These terms are all the same by <strong>symmetry</strong>.</li>
              <li>Therefore, $P(\text{match}) = P(\cup_{j=1}^{N} A_j) = 1 - \frac{1}{2!} + \frac{1}{3!} - \ldots (-1)^{N+1} \times \frac{1}{N!}$
                <ul>
                  <li>$\frac{1}{N!}$ is the case where the cards are perfectly ordered from 1 to N, all cards match, which is 1 of $N!$ possible orderings</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>$P(\text{no match}) = P(\cap_{j=1}^{N} A^{C}_{J}) = 1 - (1 - \frac{1}{2!} + \frac{1}{3!} - \ldots (-1)^{N+1} \times \frac{1}{N!}) \approx \frac{1}{e}$
            <ul>
              <li>Complement of union is the intersection of those complements</li>
              <li>The factorials in denominators should remind you of Taylor series of $e^x$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>If there were an infinite amount of cards, what is $P(\text{no match})$ ?
        <ul>
          <li>The chance a card is in the exact position is very unlikely, but there are so many chances in an infinite deck.</li>
          <li>The competing forces somehow reduce the answer to $\frac{1}{e}$ .</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Independent Events</strong>
    <ul>
      <li>
<em>Case</em>: Two Events A, B are <em>independent</em> if $P(A \cap B) = P(A) \times P(B)$
        <ul>
          <li>The probability that both A and B occurs is the product of probability of A and probability of B</li>
          <li>They’re independent, so we just multiply</li>
        </ul>
      </li>
      <li>
<em>Important</em>: Independence is completely different from <strong>disjointness</strong>
        <ul>
          <li>
<strong>Disjointnesss</strong> says if A occurs, B <em>cannot</em> occur</li>
        </ul>
      </li>
      <li>
<em>Case</em>: Three Events A, B, C are <em>independent</em> if $P(A \cap B) = P(A)P(B), P(A \cap C) = P(A)P(C), P(B \cap C) = P(B)P(C), P(A \cap B \cap C) = P(A)P(B)P(C)$
        <ul>
          <li>The pairwise independence does not imply independence of all 3 events, so we need the last equation too</li>
        </ul>
      </li>
      <li>
<em>Case</em>: N Events
        <ul>
          <li>Follows the same pattern</li>
          <li>Any 1, 2, 3, …, N events need to be <em>independent</em>
</li>
        </ul>
      </li>
      <li>
<em>Basic Rule</em>: “independent means multiply” from multiplication rule</li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<em>Example</em>: <strong>Newton-Pepys Problem (1693)</strong>
    <ul>
      <li>Samuel Pepys wanted to know a gambling problem solution, and Newton solved it for him.</li>
      <li>Problem
        <ul>
          <li>Have some fair dice, each with 6 equally-likely sides, independent from each other</li>
          <li>Which of these events is more likely?
            <ul>
              <li>Event A: at least 6 with 6 dice</li>
              <li>Event B: at least two 6’s with 12 dice</li>
              <li>Event C: at least three 6’s with 18 dice</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Pepys strongly believed Event C was more likely, but Newton solved the answer was Event A.</li>
      <li>Can use naive definition but we will use independence to solve.
        <ul>
          <li>Seeing “at least 1” should signal a union, but a complement of union is an intersection</li>
          <li>Intersection of independent events signals multiply!</li>
        </ul>
      </li>
      <li>Probability of Event A
        <ul>
          <li>Take complement: 1 minus probability that all dices are not 6</li>
          <li>$P(A) = 1 - (\frac{5}{6})^6 \approx 0.665$</li>
        </ul>
      </li>
      <li>Probability of Event B
        <ul>
          <li>Take complement: 1 minus probability of no 6’s minus probability of exactly one 6</li>
          <li>$P(B) = 1 - (\frac{5}{6})^6 - 12(\frac{1}{6})(\frac{5}{6})^11 \approx 0.619 $
            <ul>
              <li>Probability of exactly one 6: one dice is 6, the other 11 are not 6, and that one 6 dice can be any of the 12 total dice</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Probability of Event C
        <ul>
          <li>Take complement: 1 minus sum of probabilities of exactly zero to two 6’s</li>
          <li>$P(C) = 1 - \sum_{2}^{K=0} \binom{18}{K} \times (\frac{1}{6})^{K}(\frac{5}{6})^{18-K} \approx 0.597$
            <ul>
              <li>Choose position where the K dice will be 6</li>
              <li>Then set each K dice as 6</li>
              <li>Then the remaining dice can be anything not 6</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Therefore, Event A is the most likely, and Event C is the least likely.
        <ul>
          <li>Though Newton got the calculation right, his intuition was confusing and <em>wrong</em> because it was not dependent on <em>fair</em> dice.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Conditional Probability</strong>
    <ul>
      <li>How should you update probability/beliefs/uncertainty based on new evidence?
        <ul>
          <li>“Conditioning is the soul of statistics.” - Professor Blitzstein</li>
        </ul>
      </li>
      <li>$P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \text{ if } P(B) &gt; 0$
        <ul>
          <li>Probability that A occurs given that B occurs</li>
          <li>If A, B are independent, this doesn’t matter
            <ul>
              <li>$P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)P(B)}{P(B)} = P(A)$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
<em>Intuition 1</em>: <strong>Pebble World</strong>
        <ul>
          <li>Imagine a sample space S and not all outcomes are equally likely</li>
          <li>Imagine a finite number of outcomes, each represented by a pebble</li>
          <li>Consider in our sample space, there are 9 possible outcomes, or 9 pebbles, some larger than others, with a total mass of 1
            <ul>
              <li>Event is a subset of sample space</li>
              <li>Say Event B is a set of 4 pebbles</li>
              <li>Say Event A is a set of 1 pebble in B and 3 pebbles outside of B</li>
            </ul>
          </li>
          <li>$P(A \mid B)$
            <ul>
              <li>Since we know B occurred, the other 5 pebbles outside B are irrelevant</li>
              <li>Get rid of pebbles in $B^C$, our universe got restricted to B since we know Event B occurred</li>
              <li>In our new universe, find the pebbles that are also in Event A (numerator)</li>
              <li>But now total mass is not 1, so we <strong>renormalize</strong>, or multiply by a constant so new total mass is 1 again (denominator)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
<em>Intuition 2</em>: <strong>Frequentist World</strong>
        <ul>
          <li>If we flipped a coin 1000 times, and 612 of them are heads, then we can say $P(H) \approx 0.612$</li>
          <li>Interpret probability as the fraction of event occurring from <em>repeating the experiment many times</em>
</li>
          <li>$P(A \mid B)$
            <ul>
              <li>Repeat an experiment many times</li>
              <li>Circle the experiments where B occurred</li>
              <li>Of the circled experiments, what fraction of them did A also occur?</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Theorem 1
        <ul>
          <li>Suppose we wanted to find probability of A and B</li>
          <li>$P(A \cap B) = P(B) \times P(A \mid B) = P(B \cap A) = P(A) \times P(B \mid A)$</li>
          <li>If A and B are <em>independent</em>, then $P(A \mid B) = P(A), P(B \mid A) = P(B)$</li>
        </ul>
      </li>
      <li>Theorem 2
        <ul>
          <li>$P(A_1, \ldots, A_N) = P(A_1) \times P(A_2 \mid A_1) \times P(A_3 \mid A_2, A_1) \times \ldots \times P(A_N \mid A_1, \ldots, A_{N-1})$</li>
          <li>This is basically $N!$ theorems, repeatedly applying Theorem 1 multiple times</li>
        </ul>
      </li>
      <li>Theorem 3: <strong>Bayes’ Rule</strong>
        <ul>
          <li>We want $P(A \mid B)$ to relate to $P(B \mid A)$</li>
          <li>So we divide Theorem 1 by $P(B)$ on both sides</li>
          <li>$P(B) \times P(A \mid B) \div P(B) = P(A) \times P(B \mid A) \div P(B)$</li>
          <li>$P(A \mid B) = \frac{P(A)P(B \mid A)}{P(B)}$</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="5-conditional-probability-ii-law-of-total-probability">
<a class="anchor" href="#5-conditional-probability-ii-law-of-total-probability" aria-hidden="true"><span class="octicon octicon-link"></span></a>5. Conditional Probability II, Law of Total Probability</h2>

<ul>
  <li>Problem Solving
    <ul>
      <li>“Thinking conditionally is a condition for thinking.” - Professor Blitzstein</li>
      <li>How to Solve Problems
        <ol>
          <li>Try simple and extreme cases</li>
          <li>Break up the problem into simpler pieces/partitions</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<strong>Law of Total Probability</strong>
    <ul>
      <li>Let $A_1, \ldots, A_N$ be disjoint partitions of universe S</li>
      <li>Consider a sample space B within universe S</li>
      <li>$P(B) = P(B \cap A_1)<em>P(A_1) + \ldots + P(B \cap A_N)</em>P(A_N)$</li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<em>Example</em>: Get a random 2-card hand from standard deck of cards
    <ul>
      <li>Find $P(\text{both cards are aces} \mid \text{have an ace})$
        <ul>
          <li>$P(\text{both cards are aces} \mid \text{have an ace}) = \frac{P(\text{both cards are aces} \cap \text{have an ace})}{P(\text{have an ace})}$</li>
          <li>$P(\text{both cards are aces} \cap \text{have an ace}) = P(\text{both cards are aces})$
            <ul>
              <li>Having both cards aces is same as having one of the cards is an ace</li>
            </ul>
          </li>
          <li>$P(\text{both cards are aces} \mid \text{have an ace}) = \frac{P(\text{both cards are aces})}{P(\text{have an ace})}$</li>
          <li>$P(\text{both cards are aces} \mid \text{have an ace}) = \frac{\binom{4}{2} / \binom{52}{2}}{1 - \binom{48}{2}/\binom{52}{2}}$
            <ul>
              <li>Numerator: choose 2 of 4 Aces, out of all possible 2 card hands</li>
              <li>Denominator: 1 minus choose 2 of 48 non-Aces, out of all possible 2 card hands</li>
            </ul>
          </li>
          <li>$P(\text{both cards are aces} \mid \text{have an ace}) = \frac{1}{33}$</li>
        </ul>
      </li>
      <li>Find $P(\text{both cards are aces} \mid \text{have Ace of Spades})$
        <ul>
          <li>We have the Ace of Spades, second card can be any card in the deck other than the Ace of Spades, by symmetry</li>
          <li>$P(\text{both cards are aces} \mid \text{have Ace of Spades}) = \frac{3}{51} = \frac{1}{17}$</li>
        </ul>
      </li>
      <li>It’s twice as likely to have double Aces just by knowing the suit of the Ace we have!
        <ul>
          <li>What is going on? Think about least one Ace vs. a specific Ace…</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>
<em>Example</em>: A patient gets tested for a disease that affects 1% of the population and tests positive, using a test that is “95% accurate”.
    <ul>
      <li>Let $D = \text{ patient has disease}, T = \text{ patient tests positive}$</li>
      <li>$P(T \mid D) = 0.95 = P(T^C \mid D^C)$
        <ul>
          <li>If the patient has the disease, 95% of the time the test will be positive.</li>
          <li>If the patient does not have the disease, 95% of the time the test will be negative.</li>
        </ul>
      </li>
      <li>$P(D \mid T) = \frac{P(T \mid D) * P(D)}{P(T)}$
        <ul>
          <li>Patient wants to know: if the test is positive, how likely do they have the disease?</li>
          <li>Bayes’ Rule tells us how these are connected!</li>
        </ul>
      </li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$P(D \mid T) = \frac{P(T \mid D) * P(D)}{P(T \mid D)P(D) + P(T</td>
              <td>D^C)P(D^C)}$</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>We use the Law of Total Probability to find $P(T)$</li>
          <li>Probability patient tests positive is the sum of positive tests if patient tests positive and positive tests if patient tests negative</li>
        </ul>
      </li>
      <li>$P(D \mid T) = \frac{(0.95)(0.01)}{(0.95)*(0.01) + (1 - 0.95)(1 - 0.01)} \approx 0.16$</li>
      <li>Although the test is 95% accurate, there is only a 16% chance the patient has the disease if they have a positive test!
        <ul>
          <li>Why is this number so small and our intuition is so off?</li>
          <li>We need to consider that it’s both <em>rare that the test is wrong (5%)</em> and <em>rare that someone has the disease (1%)</em>!</li>
        </ul>
      </li>
      <li>
<em>Intuition</em>: If we had 1,000 patients, roughly 10 would have the disease (1%). If the 10 patients all test positive, but 5% of the 990 patients also test positive.
        <ul>
          <li>Roughly speaking, 50 patients test positive who don’t have the disease and 10 patients test positive who do have the disease</li>
          <li>This ratio of 50/10 is what leads to the $0.16$ .</li>
        </ul>
      </li>
      <li>
<em>Important</em>: Bayes’ Rule is coherent
        <ul>
          <li>Updating probabilities using the intersection of two new evidence is equal to updating probabilities using the first new evidence then updating again using the second new evidence, in any order</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br></p>

<ul>
  <li>Common Mistakes with Conditional Probability
    <ol>
      <li>Confusing $P(A \mid B)$ with $P(B \mid A)$ , “Prosecutor’s Fallacy”
        <ul>
          <li>
<em>Example</em>: Sally Clark Case
            <ul>
              <li>Clark’s two babies died suddenly and was charged with murder</li>
              <li>The only evidence that probability of baby dies not by murder is $\frac{1}{8500}$</li>
              <li>But since Clark lost two babies, the probability of not murder was $\frac{1}{8500} * \frac{1}{8500}$</li>
              <li>However, this assumes independence between baby deaths</li>
              <li>Want $P(\text{innocence} \mid \text{evidence})$ , rather than $P(\text{evidence} \mid \text{innocence})$</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Confusing $P(A)$ , the <strong>prior</strong>, with $P(A \mid B)$ , the <strong>posterior</strong>
        <ul>
          <li>If Event A occurs, $P(A \mid A) = 1$ , but $P(A) \neq 1$</li>
        </ul>
      </li>
      <li>Confusing independence with conditional independence
        <ul>
          <li>
<strong>Conditional Independence</strong>: Events A and B are conditionally independent given Event C if $P(A \cap B \mid C) = P(A \mid C) * P(B \mid C)$</li>
          <li>Conditional independence given Event C <em>does not imply</em> independence.
            <ul>
              <li>
<em>Example</em>: Chess opponent of unknown strength
                <ul>
                  <li>If you play a series of chess games with the opponent, conditioning on the opponent’s strength, all games are independent (conditional independence)
                    <ul>
                      <li>However, this does not imply the games are unconditionally independent!</li>
                    </ul>
                  </li>
                  <li>If you win the first 5 games, then you think the opponent is weaker than you.
                    <ul>
                      <li>The earlier games give you evidence of the opponent’s strength, even though the games are seemingly independent</li>
                    </ul>
                  </li>
                  <li>Independence says earlier games give no indication of the opponent’s strength</li>
                  <li>It may happen that the game outcomes are <em>conditionally independent</em> given strength of opponent but <em>not unconditionally independent</em>
</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Unconditional independence <em>does not imply</em> conditional independence given Event C
            <ul>
              <li>
<em>Example</em>: If the alarm goes off (Event A), which can be caused by two causes: fire (Event F) or popcorn (Event C)
                <ul>
                  <li>Suppose F and C are independent.</li>
                  <li>But what’s the probability that there’s a fire given the alarm goes off and nobody is making popcorn?
                    <ul>
                      <li>$P(F \mid A, C^C) = 1$</li>
                      <li>Must be 1 if we eliminate the only other explanation of making popcorn</li>
                    </ul>
                  </li>
                  <li>F and C are initially <em>independent</em>, but <em>not conditionally independent</em> given A</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h2 id="6-monty-hall-simpsons-paradox">
<a class="anchor" href="#6-monty-hall-simpsons-paradox" aria-hidden="true"><span class="octicon octicon-link"></span></a>6. Monty Hall, Simpson’s Paradox</h2>

<ul>
  <li>
<em>Example</em>: <strong>Monty Hall Problem</strong>
    <ul>
      <li>Problem
        <ul>
          <li>Monty Hall was a game show host on “Let’s Make a Deal”. There are 3 doors (Door 1, Door 2, Door 3): one door has a car, the other two have goats.</li>
          <li>Monty Hall asks you to choose a door. When you choose a door (Door 1), Monty Hall will open up either of the other doors (Door 2, Door 3), revealing a goat behind a door (Door 2)</li>
          <li>You know the car is either the door you initially chose (Door 1) or the remaining unopened door (Door 3). Should you switch you initial door choice?</li>
        </ul>
      </li>
      <li>Assumptions
        <ul>
          <li>The doors are equally likely to have the car</li>
          <li>Monty Hall knows which door has the car behind it</li>
          <li>You want the car, not the goats</li>
          <li>Monty always opens a goat door after you choose an initial door</li>
          <li>If Monty has a choice of which door to open (you initially picked the car), he will open one of the goat doors with equal probabilities</li>
        </ul>
      </li>
      <li>The answer is that you should switch, which gives a winning chance of $2/3$.
        <ul>
          <li>Once the door is opened, the probabilities of each door is no longer $1/3$.</li>
          <li>Monty opens Door 2 and reveals a goat, so it would seem the probabilities of winning is now $1/2$ each for Doors 1 and 3.</li>
          <li>There is a subtle flaw in this thinking. We need to condition on all the evidence, which includes the fact Monty Hall opened Door 2.</li>
        </ul>
      </li>
      <li>
        <p><em>Intuition</em>: Use a tree diagram based on initial door choice, the car door, and the door Monty opens.</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Initial Door</th>
              <th style="text-align: center">Car Door</th>
              <th style="text-align: center">Monty Door</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center">1</td>
              <td style="text-align: center">1 $(1/3)$</td>
              <td style="text-align: center">2 $(1/2)$</td>
            </tr>
            <tr>
              <td style="text-align: center">1</td>
              <td style="text-align: center">1 $(1/3)$</td>
              <td style="text-align: center">3 $(1/2)$</td>
            </tr>
            <tr>
              <td style="text-align: center">1</td>
              <td style="text-align: center">2 $(1/3)$</td>
              <td style="text-align: center">3 $(1)$</td>
            </tr>
            <tr>
              <td style="text-align: center">1</td>
              <td style="text-align: center">3 $(1/3)$</td>
              <td style="text-align: center">2 $(1)$</td>
            </tr>
          </tbody>
        </table>

        <ul>
          <li>If we condition on the fact Monty opened Door 2, we either took the top path (car is behind Door 1) or the bottom path (car is behind Door 3). The probability of the top path is $1/3 * 1/2 = 1/6$, and the probability of the bottom path is $1/3 * 1 = 1/3$. We renormalize the two options (make total sum = 1), to find the car has a $1/3$ chance of being behind Door 1 and a $2/3$ chance of being behind Door 3.</li>
          <li>This means $1/3$ of the time your initial guess is correct, and the other $2/3$ of the time you should switch</li>
        </ul>
      </li>
      <li>
<em>Intuition</em>: Law of Total Probability
        <ul>
          <li>What do we wish we knew beforehand? In this case, we wish we know which door has the car. Now we condition on that.</li>
          <li>Let $S$ = succeed (assuming we always switch) and $D_{j}$ = Door j has the car where $j \in {1, 2, 3}$</li>
          <li>From the law of Total Probabilities, $P(S) = P(S \vert D_{1}) \frac{1}{3} + P(S \vert D_{2}) \frac{1}{3} + P(S \vert D_{3}) \frac{1}{3}$
            <ul>
              <li>In the first case: we pick Door 1, car is behind Door 1, Monty opens either Door 2 or Door 3, and we switch our choice, so $P(S \vert D_{1}) = 0$</li>
              <li>In the second case: we pick Door 1, car is behind Door 2, Monty must open Door 3, and we switch our choice, so $P(S \vert D_{2}) = 1$</li>
              <li>In the third case: we pick Door 1, car is behind Door 3, Monty must open Door 2, and we switch our choice, so $P(S \vert D_{3}) = 1$</li>
            </ul>
          </li>
          <li>Therefore, $P(S) = 0 + 1 (\frac{1}{3}) + 1 (\frac{1}{3}) = \frac{2}{3}$</li>
          <li>By symmetry, we also know that $P(S \vert \text{ Monty opens Door 2}) = \frac{2}{3}$</li>
        </ul>
      </li>
      <li>
<em>Intuition</em>: Consider if there were 1,000,000 doors and Monty opens 999,9988 doors. In this case it is clear you should switch since your initial guess is most likely incorrect.</li>
    </ul>
  </li>
  <li>
<strong>Simpson’s Paradox</strong>
    <ul>
      <li>Is it possible to have 2 doctors, where the first doctor has a higher success rate at every single type of surgery than the second doctor, yet the second doctor overall has a higher success rate?
        <ul>
          <li>Simpson’s Paradox says this is possible, but this is very counterintuitive. How can one thing better in all individual cases be worse after aggregating the cases?</li>
        </ul>
      </li>
      <li>
        <p>Consider first doctor is Dr. Hibbert and second doctor is Dr. Nick, and they both do 100 cases total of two types of surgeries: heart surgery and band-aid removal</p>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Hibbert</th>
              <th style="text-align: center">Heart</th>
              <th style="text-align: center">Band-Aid</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center">Success</td>
              <td style="text-align: center">70</td>
              <td style="text-align: center">10</td>
            </tr>
            <tr>
              <td style="text-align: center">Failure</td>
              <td style="text-align: center">20</td>
              <td style="text-align: center">0</td>
            </tr>
          </tbody>
        </table>

        <table>
          <thead>
            <tr>
              <th style="text-align: center">Nick</th>
              <th style="text-align: center">Heart</th>
              <th style="text-align: center">Band-Aid</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align: center">Success</td>
              <td style="text-align: center">2</td>
              <td style="text-align: center">81</td>
            </tr>
            <tr>
              <td style="text-align: center">Failure</td>
              <td style="text-align: center">8</td>
              <td style="text-align: center">9</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li>If you need surgery, who do you choose? For each individual operation (conditioning on a type of surgery), Dr. Hibbert has a higher success rate. But overall (unconditionally), Dr. Hibbert has a 80% success rate, and Dr. Nick has a 83% success rate.</li>
      <li>Simpson’s Paradox in terms of Conditional Probability
        <ul>
          <li>Let A = surgery is successful</li>
          <li>Let B = treated by Dr. Nick</li>
          <li>Let C = heart surgery
            <ul>
              <li>Called the <em>confounder</em> or <em>control</em>, the type of surgery you want to condition on</li>
              <li>If we fail to condition on C, then Simpson’s Paradox shows us the inequality can flip.</li>
              <li>Knowing we got treated by Dr. Nick gives us information about what type of surgery we had, which then affects the probability of success.</li>
            </ul>
          </li>
          <li>$ P(A \vert B, C) &lt; P(A \vert B^{C}, C)$
            <ul>
              <li>The probability of success of heart surgery from Dr. Nick is less than probability of success of heart surgery from Dr. Hibbert.</li>
            </ul>
          </li>
          <li>$ P(A \vert B, C^{C}) &lt; P(A \vert B^{C}, C^{C})$
            <ul>
              <li>The probability of success of band-aid removal from Dr. Nick is less than probability of success of band-aid removal from Dr. Hibbert.</li>
            </ul>
          </li>
          <li>But overall, $P(A \vert B)$ &gt; P(A \vert B^{C})$
            <ul>
              <li>The probability of successful surgery is higher with Dr. Nick than with Dr. Hibbert.</li>
              <li>Notice how the inequality flips</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>How is Simpson’s Paradox possible? How can the sign flip from the conditioning on individual events compared to overall?
        <ul>
          <li>From the Law of Total Probability using conditional probability, $P(A \vert B) = P(A \vert B, C) P(C \vert B) + P(A \vert B, C^{C}) P(C^{C} \vert B)$</li>
          <li>We know that $P(A \vert B, C) &lt; P(A \vert B^{C}, C)$ and that $P(A \vert B, C^{C}) &lt; P(A \vert B^{C}, C^{C})$</li>
          <li>However, we are unable to reduce it further to $P(A \vert B)$ due to the terms $P(C \vert B), P(C^{C} \vert B)$, which are weights conditional on B.</li>
          <li>
<em>Intuition</em>: $P(C \vert B)$ is the probability of performing heart surgery for Dr. Nick, which is very different than $P(C \vert B^{C})$ probability of performing heart surgery for Dr. Hibbert. The weights $P(C \vert B), P(C^{C} \vert B)$ change, and that is what enables Simpson’s Paradox to happen.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><em>Example</em>: There was a lawsuit against UC Berkeley claiming sex discriminations in graduate admission rates. Looking at the overall admission rates, it looked like it was easier for men to get into UC Berkeley for graduate school. However, looking at the data of each individual apartment, the admission rates were generally more fair. The reason was that certain departments are more popular for women applicants and certain departments are harder to get into than others.</p>
  </li>
  <li>
<em>Example</em>: There are four jars that each contain two flavors of jellybeans. Suppose you like one flavor over the other. Suppose jars are “better” if they have a higher percentage of the jellybean you like. If we combine the two “better” jars into one large jar and likewise with the two “worse” jars, Simpson’s Paradox says the large jar created from the two “worse” jars may now have a higher percentage of the favorable jellybean and be “better”.</li>
</ul>

<h2 id="7-gamblers-ruin-random-variables">
<a class="anchor" href="#7-gamblers-ruin-random-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>7. Gambler’s Ruin, Random Variables</h2>

<h2 id="8-random-variables-and-their-distributions">
<a class="anchor" href="#8-random-variables-and-their-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>8. Random Variables and Their Distributions</h2>

<h2 id="9-expectation-i-indicator-random-variables-linearity">
<a class="anchor" href="#9-expectation-i-indicator-random-variables-linearity" aria-hidden="true"><span class="octicon octicon-link"></span></a>9. Expectation I, Indicator Random Variables, Linearity</h2>

<h2 id="10-expectation-ii">
<a class="anchor" href="#10-expectation-ii" aria-hidden="true"><span class="octicon octicon-link"></span></a>10. Expectation II</h2>

<h2 id="11-poisson-distribution">
<a class="anchor" href="#11-poisson-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>11. Poisson Distribution</h2>

<h2 id="12-discrete-vs-continuous-uniform-distribution">
<a class="anchor" href="#12-discrete-vs-continuous-uniform-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>12. Discrete vs. Continuous, Uniform Distribution</h2>

<h2 id="13-normal-distribution">
<a class="anchor" href="#13-normal-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>13. Normal Distribution</h2>

<h2 id="14-location-scale-lotus">
<a class="anchor" href="#14-location-scale-lotus" aria-hidden="true"><span class="octicon octicon-link"></span></a>14. Location, Scale, LOTUS</h2>

<h2 id="15-midterm-review">
<a class="anchor" href="#15-midterm-review" aria-hidden="true"><span class="octicon octicon-link"></span></a>15. Midterm Review</h2>

<h2 id="16-exponential-distribution">
<a class="anchor" href="#16-exponential-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>16. Exponential Distribution</h2>

<h2 id="17-moment-generating-functions-i">
<a class="anchor" href="#17-moment-generating-functions-i" aria-hidden="true"><span class="octicon octicon-link"></span></a>17. Moment Generating Functions I</h2>

<h2 id="18-moment-generating-functions-ii">
<a class="anchor" href="#18-moment-generating-functions-ii" aria-hidden="true"><span class="octicon octicon-link"></span></a>18. Moment Generating Functions II</h2>

<h2 id="19-joint-conditional-marginal-distributions">
<a class="anchor" href="#19-joint-conditional-marginal-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>19. Joint, Conditional, Marginal Distributions</h2>

<h2 id="20-multinomial-cauchy-distributions">
<a class="anchor" href="#20-multinomial-cauchy-distributions" aria-hidden="true"><span class="octicon octicon-link"></span></a>20. Multinomial, Cauchy Distributions</h2>

<h2 id="21-covariance-correlation">
<a class="anchor" href="#21-covariance-correlation" aria-hidden="true"><span class="octicon octicon-link"></span></a>21. Covariance, Correlation</h2>

<h2 id="22-transformations-convolutions">
<a class="anchor" href="#22-transformations-convolutions" aria-hidden="true"><span class="octicon octicon-link"></span></a>22. Transformations, Convolutions</h2>

<h2 id="23-beta-distribution">
<a class="anchor" href="#23-beta-distribution" aria-hidden="true"><span class="octicon octicon-link"></span></a>23. Beta Distribution</h2>

<h2 id="24-gamma-distribution-poisson-process">
<a class="anchor" href="#24-gamma-distribution-poisson-process" aria-hidden="true"><span class="octicon octicon-link"></span></a>24. Gamma Distribution, Poisson Process</h2>

<h2 id="25-order-statistics-conditional-expectation-i">
<a class="anchor" href="#25-order-statistics-conditional-expectation-i" aria-hidden="true"><span class="octicon octicon-link"></span></a>25. Order Statistics, Conditional Expectation I</h2>

<h2 id="26-conditional-expectation-ii">
<a class="anchor" href="#26-conditional-expectation-ii" aria-hidden="true"><span class="octicon octicon-link"></span></a>26. Conditional Expectation II</h2>

<h2 id="27-conditional-expectation-given-rv">
<a class="anchor" href="#27-conditional-expectation-given-rv" aria-hidden="true"><span class="octicon octicon-link"></span></a>27. Conditional Expectation Given R.V.</h2>

<h2 id="28-inequalities">
<a class="anchor" href="#28-inequalities" aria-hidden="true"><span class="octicon octicon-link"></span></a>28. Inequalities</h2>

<h2 id="29-law-of-large-numbers-central-limit-theorem">
<a class="anchor" href="#29-law-of-large-numbers-central-limit-theorem" aria-hidden="true"><span class="octicon octicon-link"></span></a>29. Law of Large Numbers, Central Limit Theorem</h2>

<h2 id="30-chi-square-student-t-multivariate-normal">
<a class="anchor" href="#30-chi-square-student-t-multivariate-normal" aria-hidden="true"><span class="octicon octicon-link"></span></a>30. Chi-Square, Student-T, Multivariate Normal</h2>

<h2 id="31-markov-chains-i">
<a class="anchor" href="#31-markov-chains-i" aria-hidden="true"><span class="octicon octicon-link"></span></a>31. Markov Chains I</h2>

<h2 id="32-markov-chains-ii">
<a class="anchor" href="#32-markov-chains-ii" aria-hidden="true"><span class="octicon octicon-link"></span></a>32. Markov Chains II</h2>

<h2 id="33-markov-chains-iii">
<a class="anchor" href="#33-markov-chains-iii" aria-hidden="true"><span class="octicon octicon-link"></span></a>33. Markov Chains III</h2>

<h2 id="34-a-look-ahead">
<a class="anchor" href="#34-a-look-ahead" aria-hidden="true"><span class="octicon octicon-link"></span></a>34. A Look Ahead</h2>

<h2 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

  </div><a class="u-url" href="/notes/harvard-stat110" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Notes and thoughts from a lifelong student.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/nhtsai" target="_blank" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/nhtsai" target="_blank" title="nhtsai"><svg class="svg-icon grey"><use xlink:href="/notes/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
